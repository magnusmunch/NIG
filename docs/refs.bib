
@article{van_dyk_nesting_2000,
	title = {{NESTING} {EM} {ALGORITHMS} {FOR} {COMPUTATIONAL} {EFFICIENCY}},
	volume = {10},
	issn = {1017-0405},
	url = {http://www.jstor.org/stable/24306713},
	abstract = {Computing posterior modes (e.g., maximum likelihood estimates) for models involving latent variables or missing data often involves complicated optimization procedures. By splitting this task into two simpler parts, however, EM-type algorithms often offer a simple solution. Although this approach has proven useful, in some settings even these simpler tasks are challenging. In particular, computations involving latent variables are typically difficult to simplify. Thus, in models such as hierarchical models with complicated latent variable structures, computationally intensive methods may be required for the expectation step of EM. This paper describes how nesting two or more EM algorithms can take advantage of closed form conditional expectations and lead to algorithms which converge faster, are straightforward to implement, and enjoy stable convergence properties. Methodology to monitor convergence of nested EM algorithms is developed using importance and bridge sampling. The strategy is applied to hierarchical probit and t regression models to derive algorithms which incorporate aspects of Monte-Carlo EM, PX-EM, and nesting in order to combine computational efficiency with easy implementation.},
	number = {1},
	urldate = {2016-09-13},
	journal = {Statistica Sinica},
	author = {van Dyk, David A.},
	year = {2000},
	pages = {203--225},
	file = {NestingEMAlgorithmsComputationalEfficiency.pdf:/Users/magnusmunch/Zotero/storage/SCE23QDX/NestingEMAlgorithmsComputationalEfficiency.pdf:application/pdf}
}

@article{devroye_exact_2009,
	title = {On exact simulation algorithms for some distributions related to {Jacobi} theta functions},
	volume = {79},
	issn = {0167-7152},
	url = {http://www.sciencedirect.com/science/article/pii/S0167715209002867},
	doi = {10.1016/j.spl.2009.07.028},
	abstract = {We develop exact random variate generators for several distributions related to the Jacobi theta function. These include the distributions of the maximum of a Brownian bridge, a Brownian meander and a Brownian excursion, and distributions of certain first passage times of Bessel processes. The algorithms are based on the alternating series method. Furthermore, we survey various distributional identities and point out ways of dealing with generalizations of these basic distributions.},
	number = {21},
	urldate = {2016-11-08},
	journal = {Statistics \& Probability Letters},
	author = {Devroye, Luc},
	month = nov,
	year = {2009},
	pages = {2251--2259},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/T4X4QQDG/Devroye - 2009 - On exact simulation algorithms for some distributi.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/G4UAF8JT/S0167715209002867.html:text/html}
}

@article{schmoldt_digitoxin_1975,
	title = {Digitoxin metabolism by rat liver microsomes},
	volume = {24},
	issn = {0006-2952},
	language = {eng},
	number = {17},
	journal = {Biochemical Pharmacology},
	author = {Schmoldt, A. and Benthe, H. F. and Haberland, G.},
	month = sep,
	year = {1975},
	pmid = {10},
	keywords = {Animals, Chromatography, Thin Layer, Digitoxigenin, Digitoxin, Hydroxylation, In Vitro Techniques, Male, Microsomes, Liver, NADP, Rats, Time Factors},
	pages = {1639--1641}
}

@article{choi_polya-gamma_2013,
	title = {The {Polya}-{Gamma} {Gibbs} sampler for {Bayesian} logistic regression is uniformly ergodic},
	volume = {7},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1377005819},
	doi = {10.1214/13-EJS837},
	abstract = {One of the most widely used data augmentation algorithms is Albert and Chib’s (1993) algorithm for Bayesian probit regression. Polson, Scott, and Windle (2013) recently introduced an analogous algorithm for Bayesian logistic regression. The main difference between the two is that Albert and Chib’s (1993) truncated normals are replaced by so-called Polya-Gamma random variables. In this note, we establish that the Markov chain underlying Polson, Scott, and Windle’s (2013) algorithm is uniformly ergodic. This theoretical result has important practical benefits. In particular, it guarantees the existence of central limit theorems that can be used to make an informed decision about how long the simulation should be run.},
	language = {EN},
	urldate = {2016-10-19},
	journal = {Electronic Journal of Statistics},
	author = {Choi, Hee Min and Hobert, James P.},
	year = {2013},
	mrnumber = {MR3091616},
	keywords = {Markov chain, Monte Carlo, Polya-Gamma distribution, data augmentation algorithm, minorization condition},
	pages = {2054--2064},
	file = {euclid.ejs.1377005819.pdf:/Users/magnusmunch/Zotero/storage/GXHX58IR/euclid.ejs.1377005819.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8MI6UI5B/1377005819.html:text/html}
}

@article{meijer_efficient_2013,
	title = {Efficient approximate \textit{k} -fold and leave-one-out cross-validation for ridge regression: {Efficient} approximate \textit{k} -fold and leave-one-out cross-validation},
	volume = {55},
	issn = {03233847},
	shorttitle = {Efficient approximate \textit{k} -fold and leave-one-out cross-validation for ridge regression},
	url = {http://doi.wiley.com/10.1002/bimj.201200088},
	doi = {10.1002/bimj.201200088},
	language = {en},
	number = {2},
	urldate = {2016-09-13},
	journal = {Biometrical Journal},
	author = {Meijer, Rosa J. and Goeman, Jelle J.},
	month = mar,
	year = {2013},
	pages = {141--155},
	file = {EfficientApproximateKfoldLOOCVRidgeRegression.pdf:/Users/magnusmunch/Zotero/storage/FN4VMDGS/EfficientApproximateKfoldLOOCVRidgeRegression.pdf:application/pdf}
}

@article{park_practical_2015,
	title = {Practical {Issues} in {Screening} and {Variable} {Selection} in {Genome}-{Wide} {Association} {Analysis}},
	issn = {1176-9351},
	url = {http://www.la-press.com/practical-issues-in-screening-and-variable-selection-in-genome-wide-as-article-a4614},
	doi = {10.4137/CIN.S16350},
	language = {en},
	urldate = {2015-09-14},
	journal = {Cancer Informatics},
	author = {Park, Taesung and Hong, Sungyeon and Kim, Yongkang},
	month = jan,
	year = {2015},
	pages = {55},
	file = {PracticalIssuesScreeningVarSelectionGWAssociation.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\PracticalIssuesScreeningVarSelectionGWAssociation.pdf:application/pdf;PracticalIssuesScreeningVarSelectionGWAssociation.pdf:/Users/magnusmunch/Zotero/storage/R44ZW866/PracticalIssuesScreeningVarSelectionGWAssociation.pdf:application/pdf}
}

@article{druilhet_starvation_1976,
	title = {Starvation survival of {Salmonella} enteritidis},
	volume = {125},
	issn = {0021-9193},
	abstract = {Washed cells of Salmonella enteritidis harvested from a defined medium during logarithmic growth were subjected to starvation in pH 7 phosphate buffer at 37 C. Viability was measured by slide cultures and plate counts. The survival of cell suspensions equivalent to 1 to 10 mg (dry wt)/ml was influenced by cryptic growth. The rate of cryptic growth, assessed by plate counts, increased with cell density and could not be alleviated by starvation with dialysis. Dialysis of the starving culture did retard the onset of cryptic growth but did not eliminate it, indicating that the major substrates for regrowth were relatively large cellular components. In phosphate buffer, 6.7 homologous heat-killed cells allowed for the doubling of one S. enteritidis cell. Cryptic growth was not observed when cells were starved on the surface of membrane filters or in suspensions equivalent to 20 mug (dry wt)/ml (105 cells/ml). Similar half-life survival times were calculated for both these populations, but the shape of their survival curves differed significantly. These differences were attributed to stress factors encountered during cell preparation and during starvation. The half-life survival time of S. enteritidis starved at 20 mug (dry wt)/ml was 140 h in phosphate buffer, 82 h in 3,6-endomethylene-1,2,3,-6-tetrahydrophthalic acid buffer, and 77 h in tris(hydroxymethyl)aminomethane buffer.},
	language = {eng},
	number = {1},
	journal = {Journal of Bacteriology},
	author = {Druilhet, R. E. and Sobek, J. M.},
	month = jan,
	year = {1976},
	pmid = {1369},
	pmcid = {PMC233342},
	keywords = {Buffers, Hot Temperature, Hydrogen-Ion Concentration, Micropore Filters, Phosphates, Phthalic Acids, Salmonella enteritidis, Tromethamine},
	pages = {119--124}
}

@techreport{mackay_ensemble_1997,
	title = {Ensemble learning for hidden {Markov} models},
	author = {MacKay, David J. C.},
	year = {1997},
	file = {10.1.1.52.9627.pdf:/Users/magnusmunch/Zotero/storage/8XXF9PW5/10.1.1.52.9627.pdf:application/pdf}
}

@article{chib_marginal_1995,
	title = {Marginal {Likelihood} from the {Gibbs} {Output}},
	volume = {90},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2291521},
	doi = {10.2307/2291521},
	abstract = {In the context of Bayes estimation via Gibbs sampling, with or without data augmentation, a simple approach is developed for computing the marginal density of the sample data (marginal likelihood) given parameter draws from the posterior distribution. Consequently, Bayes factors for model comparisons can be routinely computed as a by-product of the simulation. Hitherto, this calculation has proved extremely challenging. Our approach exploits the fact that the marginal density can be expressed as the prior times the likelihood function over the posterior density. This simple identity holds for any parameter value. An estimate of the posterior density is shown to be available if all complete conditional densities used in the Gibbs sampler have closed-form expressions. To improve accuracy, the posterior density is estimated at a high density point, and the numerical standard error of resulting estimate is derived. The ideas are applied to probit regression and finite mixture models.},
	number = {432},
	urldate = {2016-09-13},
	journal = {Journal of the American Statistical Association},
	author = {Chib, Siddhartha},
	year = {1995},
	pages = {1313--1321},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/FVFJICM7/Chib - 1995 - Marginal Likelihood from the Gibbs Output.pdf:application/pdf}
}

@article{polson_bayesian_2013,
	title = {Bayesian {Inference} for {Logistic} {Models} {Using} {Pólya}–{Gamma} {Latent} {Variables}},
	volume = {108},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2013.829001},
	doi = {10.1080/01621459.2013.829001},
	abstract = {We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Pólya–Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effect models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that (1) circumvent the need for analytic approximations, numerical integration, or Metropolis–Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Pólya–Gamma distribution, are implemented in the R package BayesLogit. Supplementary materials for this article are available online.},
	number = {504},
	urldate = {2016-09-15},
	journal = {Journal of the American Statistical Association},
	author = {Polson, Nicholas G. and Scott, James G. and Windle, Jesse},
	month = dec,
	year = {2013},
	pages = {1339--1349},
	file = {BayesianInferenceLogRegModelsPolyaGammaLatentVars.pdf:/Users/magnusmunch/Zotero/storage/VDK8S2H4/BayesianInferenceLogRegModelsPolyaGammaLatentVars.pdf:application/pdf;Full Text PDF:/Users/magnusmunch/Zotero/storage/PDKP9JFH/Polson et al. - 2013 - Bayesian Inference for Logistic Models Using Pólya.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/HWBHFG72/01621459.2013.html:text/html;techsuppR1.pdf:/Users/magnusmunch/Zotero/storage/SVUQ2BMZ/techsuppR1.pdf:application/pdf}
}

@article{hernandez-lobato_expectation_2015,
	title = {Expectation propagation in linear regression models with spike-and-slab priors},
	volume = {99},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1007/s10994-014-5475-7},
	doi = {10.1007/s10994-014-5475-7},
	abstract = {An expectation propagation (EP) algorithm is proposed for approximate inference in linear regression models with spike-and-slab priors. This EP method is applied to regression tasks in which the number of training instances is small and the number of dimensions of the feature space is large. The problems analyzed include the reconstruction of genetic networks, the recovery of sparse signals, the prediction of user sentiment from customer-written reviews and the analysis of biscuit dough constituents from NIR spectra. The proposed EP method outperforms in most of these tasks another EP method that ignores correlations in the posterior and a variational Bayes technique for approximate inference. Additionally, the solutions generated by EP are very close to those given by Gibbs sampling, which can be taken as the gold standard but can be much more computationally expensive. In the tasks analyzed, spike-and-slab priors generally outperform other sparsifying priors, such as Laplace, Student’s tt and horseshoe priors. The key to the improved predictions with respect to Laplace and Student’s tt priors is the superior selective shrinkage capacity of the spike-and-slab prior distribution.},
	language = {en},
	number = {3},
	urldate = {2016-11-15},
	journal = {Machine Learning},
	author = {Hernández-Lobato, José Miguel and Hernández-Lobato, Daniel and Suárez, Alberto},
	month = jun,
	year = {2015},
	pages = {437--487},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/9FVBP39F/Hernández-Lobato et al. - 2015 - Expectation propagation in linear regression model.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/QBU5GZR9/s10994-014-5475-7.html:text/html}
}

@article{ogutu_genomic_2012,
	title = {Genomic selection using regularized linear regression models: ridge regression, lasso, elastic net and their extensions},
	volume = {6},
	issn = {1753-6561},
	shorttitle = {Genomic selection using regularized linear regression models},
	url = {http://www.biomedcentral.com/1753-6561/6/S2/S10},
	doi = {10.1186/1753-6561-6-S2-S10},
	language = {en},
	number = {Suppl 2},
	urldate = {2015-09-14},
	journal = {BMC Proceedings},
	author = {Ogutu, Joseph O and Schulz-Streeck, Torben and Piepho, Hans-Peter},
	year = {2012},
	pages = {S10},
	file = {GenomicSelectionRegularizedRegressionAndExtensions.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\GenomicSelectionRegularizedRegressionAndExtensions.pdf:application/pdf}
}

@article{rajaratnam_lasso_2016,
	title = {Lasso regression: estimation and shrinkage via the limit of {Gibbs} sampling},
	volume = {78},
	issn = {1467-9868},
	shorttitle = {Lasso regression},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/rssb.12106/abstract},
	doi = {10.1111/rssb.12106},
	abstract = {The application of the lasso is espoused in high dimensional settings where only a small number of the regression coefficients are believed to be non-zero (i.e. the solution is sparse). Moreover, statistical properties of high dimensional lasso estimators are often proved under the assumption that the correlation between the predictors is bounded. In this vein, co-ordinatewise methods, which are the most common means of computing the lasso solution, naturally work well in the presence of low-to-moderate multicollinearity. The computational speed of co-ordinatewise algorithms, although excellent for sparse and low-to-moderate multicollinearity settings, degrades as sparsity decreases and multicollinearity increases. Though lack of sparsity and high multicollinearity can be quite common in contemporary applications, model selection is still a necessity in such settings. Motivated by the limitations of co-ordinatewise algorithms in such ‘non-sparse’ and ‘high multicollinearity’ settings, we propose the novel ‘deterministic Bayesian lasso’ algorithm for computing the lasso solution. This algorithm is developed by considering a limiting version of the Bayesian lasso. In contrast with co-ordinatewise algorithms, the performance of the deterministic Bayesian lasso improves as sparsity decreases and multicollinearity increases. Importantly, in non-sparse and high multicollinearity settings the algorithm proposed can offer substantial increases in computational speed over co-ordinatewise algorithms. A rigorous theoretical analysis demonstrates that the deterministic Bayesian lasso algorithm converges to the lasso solution and it leads to a representation of the lasso estimator which shows how it achieves both l1- and l2-types of shrinkage simultaneously. Connections between the deterministic Bayesian lasso and other algorithms are also provided. The benefits of the deterministic Bayesian lasso algorithm are then illustrated on simulated and real data.},
	language = {en},
	number = {1},
	urldate = {2016-09-13},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Rajaratnam, Bala and Roberts, Steven and Sparks, Doug and Dalal, Onkar},
	month = jan,
	year = {2016},
	keywords = {Bayesian lasso, Lasso regression, Limit of Gibbs sampler, Multicollinearity},
	pages = {153--174},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/BATTWFT2/Rajaratnam et al. - 2016 - Lasso regression estimation and shrinkage via the.pdf:application/pdf;Rajaratnam_et_al-2016_Supplements.pdf:/Users/magnusmunch/Zotero/storage/4T8NK9D6/Rajaratnam_et_al-2016_Supplements.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/4N9FZQGW/abstract.html:text/html}
}

@article{friedman_pathwise_2007,
	title = {Pathwise coordinate optimization},
	volume = {1},
	issn = {1932-6157},
	url = {http://projecteuclid.org/euclid.aoas/1196438020},
	doi = {10.1214/07-AOAS131},
	language = {en},
	number = {2},
	urldate = {2015-09-14},
	journal = {The Annals of Applied Statistics},
	author = {Friedman, Jerome and Hastie, Trevor and Höfling, Holger and Tibshirani, Robert},
	month = dec,
	year = {2007},
	pages = {302--332},
	file = {PathwiseCoordinateOptimization.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\PathwiseCoordinateOptimization.pdf:application/pdf}
}

@article{wei_monte_1990,
	title = {A {Monte} {Carlo} {Implementation} of the {EM} {Algorithm} and the {Poor} {Man}'s {Data} {Augmentation} {Algorithms}},
	volume = {85},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2290005},
	doi = {10.2307/2290005},
	abstract = {The first part of this article presents the Monte Carlo implementation of the E step of the EM algorithm. Given the current guess to the maximizer of the posterior distribution, latent data patterns are generated from the conditional predictive distribution. The expected value of the augmented log-posterior is then updated as a mixture of augmented log-posteriors, mixed over the generated latent data patterns (multiple imputations). In the M step of the algorithm, this mixture is maximized to obtain the update to the maximizer of the observed posterior. The gradient and Hessian of the observed log posterior are also expressed as mixtures, mixed over the multiple imputations. The relation between the Monte Carlo EM (MCEM) algorithm and the data augmentation algorithm is noted. Two modifications to the MCEM algorithm (the poor man's data augmentation algorithms), which allow for the calculation of the entire posterior, are then presented. These approximations serve as diagnostics for the validity of the normal approximation to the posterior, as well as starting points for the full data augmentation analysis. The methodology is illustrated with two examples.},
	number = {411},
	urldate = {2016-09-13},
	journal = {Journal of the American Statistical Association},
	author = {Wei, Greg C. G. and Tanner, Martin A.},
	year = {1990},
	pages = {699--704},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/2F4TDWXS/Wei and Tanner - 1990 - A Monte Carlo Implementation of the EM Algorithm a.pdf:application/pdf}
}

@article{bose_delineation_1975,
	title = {Delineation of the intimate details of the backbone conformation of pyridine nucleotide coenzymes in aqueous solution},
	volume = {66},
	issn = {0006-291X},
	language = {eng},
	number = {4},
	journal = {Biochemical and Biophysical Research Communications},
	author = {Bose, K. S. and Sarma, R. H.},
	month = oct,
	year = {1975},
	pmid = {2},
	keywords = {Fourier Analysis, Magnetic Resonance Spectroscopy, Models, Molecular, Molecular Conformation, NAD, NADP, Structure-Activity Relationship, Temperature},
	pages = {1173--1179}
}

@article{palmer_type_nodate,
	title = {Type {II} variational methods in {Bayesian} estimation},
	url = {http://www.academia.edu/download/33835434/varuai.pdf},
	urldate = {2016-09-13},
	author = {Palmer, J. A. and Wipf, D. P. and Kreutz-Delgado, K.},
	file = {Type2VariationalBayesMethodsBayesianEstimation.pdf:/Users/magnusmunch/Zotero/storage/6TE9PH79/Type2VariationalBayesMethodsBayesianEstimation.pdf:application/pdf}
}

@article{botte_observation_1976,
	title = {Observation on the ultrastructure of the nucleolus of {Paracentrotus} lividus in relation to morphological events during development},
	volume = {69},
	issn = {0035-6050},
	language = {eng, ita},
	number = {3-4},
	journal = {Rivista Di Biologia},
	author = {Botte, L. and Scippa, S.},
	month = dec,
	year = {1976},
	pmid = {70053},
	keywords = {Animals, Cell Differentiation, Cell Nucleolus, Cell Nucleus, Dactinomycin, Sea Urchins},
	pages = {169--194}
}

@article{fan_variable_2001,
	title = {Variable {Selection} via {Nonconcave} {Penalized} {Likelihood} and its {Oracle} {Properties}},
	volume = {96},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753382273},
	doi = {10.1198/016214501753382273},
	language = {en},
	number = {456},
	urldate = {2015-09-14},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Li, Runze},
	month = dec,
	year = {2001},
	pages = {1348--1360},
	file = {VariableSelectionNonconcavePenalizedLLOracleProp.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\VariableSelectionNonconcavePenalizedLLOracleProp.pdf:application/pdf;VariableSelectionNonconcavePenalizedLLOracleProp.pdf:/Users/magnusmunch/Zotero/storage/UUAENWSN/VariableSelectionNonconcavePenalizedLLOracleProp.pdf:application/pdf}
}

@article{madigan_priors_2008,
	title = {Priors on the {Variance} in {Sparse} {Bayesian} {Learning}; the demi-{Bayesian} {Lasso}},
	url = {http://academiccommons.columbia.edu/catalog/ac:44891},
	abstract = {We explore the use of proper priors for variance parameters of certain sparse Bayesian regression models. This leads to a connection between sparse Bayesian learning (SBL) models (Tipping, 2001) and the recently proposed Bayesian Lasso (Park and Casella, 2008). We outline simple modifications of existing algorithms to solve this new variant which essentially uses type-II maximum likelihood to fit the Bayesian Lasso model. We also propose an Elastic-net (Zou and Hastie, 2005) heuristic to help with modeling correlated inputs. Experimental results show the proposals to compare favorably to both the Lasso and traditional and more recent sparse Bayesian algorithms.},
	urldate = {2016-09-13},
	author = {Madigan, David},
	year = {2008},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/4A7QE8J4/Madigan - 2008 - Priors on the Variance in Sparse Bayesian Learning.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/RKBKENCG/ac44891.html:text/html}
}

@incollection{turner_online_2013,
	title = {Online {Variational} {Approximations} to non-{Exponential} {Family} {Change} {Point} {Models}: {With} {Application} to {Radar} {Tracking}},
	shorttitle = {Online {Variational} {Approximations} to non-{Exponential} {Family} {Change} {Point} {Models}},
	url = {http://papers.nips.cc/paper/5124-online-variational-approximations-to-non-exponential-family-change-point-models-with-application-to-radar-tracking.pdf},
	urldate = {2016-09-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Turner, Ryan D and Bottone, Steven and Stanek, Clay J},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {306--314},
	file = {NIPS Full Text PDF:/Users/magnusmunch/Zotero/storage/852DAFWI/Turner et al. - 2013 - Online Variational Approximations to non-Exponenti.pdf:application/pdf;NIPS Snapshort:/Users/magnusmunch/Zotero/storage/JK4B6R4U/5124-online-variational-approximations-to-non-exponential-family-change-point-models-with-appli.html:text/html}
}

@phdthesis{beal_variational_2003,
	address = {London},
	title = {Variational algorithms for approximate {Bayesian} inference},
	url = {https://www.researchgate.net/publication/34000771_Variational_algorithms_for_approximate_Bayesian_inference},
	abstract = {Thesis (Ph.D.)--University of London, 2003.},
	urldate = {2016-09-26},
	school = {University College},
	author = {Beal, Matthew James},
	month = jan,
	year = {2003},
	file = {Snapshot:/Users/magnusmunch/Zotero/storage/CAIHJJFB/34000771_Variational_algorithms_for_approximate_Bayesian_inference.html:text/html;VariationalAlgorithmsApproximateBayesianInference.pdf:/Users/magnusmunch/Zotero/storage/VQC6GFJG/VariationalAlgorithmsApproximateBayesianInference.pdf:application/pdf}
}

@article{sachs_lp_1976,
	title = {{LP} in the {ER}?},
	volume = {149},
	language = {eng},
	number = {4},
	journal = {IMJ. Illinois medical journal},
	author = {Sachs, H. K.},
	month = apr,
	year = {1976},
	pmid = {5364},
	keywords = {Child, Preschool, Emergencies, Female, Humans, Lead, Lead Poisoning, Male},
	pages = {365--367}
}

@article{chatterjee_bootstrapping_2011,
	title = {Bootstrapping {Lasso} {Estimators}},
	volume = {106},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10159},
	doi = {10.1198/jasa.2011.tm10159},
	language = {en},
	number = {494},
	urldate = {2016-09-13},
	journal = {Journal of the American Statistical Association},
	author = {Chatterjee, A. and Lahiri, S. N.},
	month = jun,
	year = {2011},
	pages = {608--625},
	file = {1-s2.0-S0167947310003671-main.pdf:/Users/magnusmunch/Zotero/storage/P5RPKWFC/1-s2.0-S0167947310003671-main.pdf:application/pdf;BootstrapLasso.pdf:/Users/magnusmunch/Zotero/storage/UGJ6UGXD/BootstrapLasso.pdf:application/pdf}
}

@article{osborne_lasso_2000,
	title = {On the lasso and its dual},
	volume = {9},
	number = {2},
	journal = {Journal of computational and graphical statistics},
	author = {Osborne, Michael R. and Presnell, Brett and Turlach, Berwin A.},
	year = {2000},
	pages = {319--337},
	file = {LassoDual.pdf:/Users/magnusmunch/Zotero/storage/S6F8VBW7/LassoDual.pdf:application/pdf}
}

@inproceedings{fan_statistical_2007,
	title = {Statistical challenges with high dimensionality: feature selection in knowledge discovery},
	volume = {III},
	shorttitle = {Statistical challenges with high dimensionality},
	booktitle = {Proceedings of the {International} {Congress} of {Mathematicians} {Madrid}, {August} 22-30, 2006},
	author = {Fan, J and Li, R},
	editor = {Sanz Solé, Marta and Soria, Javier and Varona, Juan Luis and Verdera, Joan},
	year = {2007},
	keywords = {dimensionality, high, learning, machine},
	pages = {595--622},
	file = {StatisticalChallengesHighDimensionalityFeatureSelectionKnowledgeDiscovery.pdf:/Users/magnusmunch/Zotero/storage/E92B4WS6/StatisticalChallengesHighDimensionalityFeatureSelectionKnowledgeDiscovery.pdf:application/pdf}
}

@inproceedings{minka_expectation_2001,
	address = {San Francisco, CA, USA},
	series = {{UAI} '01},
	title = {Expectation {Propagation} for {Approximate} {Bayesian} {Inference}},
	isbn = {978-1-55860-800-9},
	url = {http://dl.acm.org/citation.cfm?id=647235.720257},
	urldate = {2016-11-18},
	booktitle = {Proceedings of the 17th {Conference} in {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Minka, Thomas P.},
	year = {2001},
	pages = {362--369},
	file = {minka-ep-uai.pdf:/Users/magnusmunch/Zotero/storage/ZWXA8F6C/minka-ep-uai.pdf:application/pdf}
}

@unpublished{polson_improved_2013,
	title = {Improved {Pólya}-{Gamma} {Sampling}.pdf},
	url = {http://www2.stat.duke.edu/~jbw44/Papers/NewPGSamplerWriteUp.pdf},
	author = {Polson, Nicholas G.},
	month = mar,
	year = {2013},
	file = {ImprovedPolyaGammaSampling.pdf:/Users/magnusmunch/Zotero/storage/F6AVGU45/ImprovedPolyaGammaSampling.pdf:application/pdf}
}

@article{zhao_diagnostics_2013,
	title = {Diagnostics for {Variational} {Bayes} approximations},
	url = {http://arxiv.org/abs/1309.5117},
	abstract = {Variational Bayes (VB) has shown itself to be a powerful approximation method in many application areas. This paper describes some diagnostics methods which can assess how well the VB approximates the true posterior, particularly with regards to its covariance structure. The methods proposed also allow us to generate simple corrections when the approximation error is large. It looks at joint, marginal and conditional aspects of the approximate posterior and shows how to apply these techniques in both simulated and real data examples.},
	urldate = {2016-11-01},
	journal = {arXiv:1309.5117 [stat]},
	author = {Zhao, Hui and Marriott, Paul},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.5117},
	keywords = {Statistics - Computation},
	file = {arXiv\:1309.5117 PDF:/Users/magnusmunch/Zotero/storage/FE3BJZ2P/Zhao and Marriott - 2013 - Diagnostics for Variational Bayes approximations.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/H54KB5JD/1309.html:text/html}
}

@article{riddle_respiratory_1976,
	title = {Respiratory metabolism of the centipede {Nadabius} coloradensis ({Cockerell}): influence of temperature, season and starvation},
	volume = {55},
	issn = {0300-9629},
	shorttitle = {Respiratory metabolism of the centipede {Nadabius} coloradensis ({Cockerell})},
	language = {eng},
	number = {2a},
	journal = {Comparative Biochemistry and Physiology. A, Comparative Physiology},
	author = {Riddle, W. A.},
	year = {1976},
	pmid = {7412},
	keywords = {Acclimatization, Animals, Arthropods, Female, Male, Oxygen Consumption, Seasons, Sex Factors, Starvation, Temperature},
	pages = {147--151}
}

@article{belitser_needles_2015,
	title = {Needles and straw in a haystack: robust empirical {Bayes} confidence for possibly sparse sequences},
	shorttitle = {Needles and straw in a haystack},
	url = {http://arxiv.org/abs/1511.01803},
	abstract = {In the many normal means model we construct an empirical Bayes posterior which we then use for uncertainty quantification for the unknown (possibly sparse) parameter by constructing an estimator and a confidence set around it as empirical Bayes credible ball. We allow the model to be misspecified (the normality assumption can be dropped, with some moment conditions instead), leading to the robust empirical Bayes inference. An important step in assessing the uncertainty is the derivation of the fact that the empirical Bayes posterior contracts to the parameter with a local (i.e., depending on the parameter) rate which is the best over certain family of local rates; therefore called oracle rate. We introduce the so called excessive bias restriction under which we establish the local (oracle) confidence optimality of the empirical Bayes credible ball. Adaptive minimax results (for the estimation and posterior contraction problems) over sparsity classes follow from our local results. An extra (square root of) log factor appears in the radial rate of the confidence ball; it is not known whether this is an artifact or not.},
	urldate = {2016-09-19},
	journal = {arXiv:1511.01803 [math, stat]},
	author = {Belitser, Eduard and Nurushev, Nurzhan},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.01803},
	keywords = {Mathematics - Statistics Theory},
	file = {arXiv\:1511.01803 PDF:/Users/magnusmunch/Zotero/storage/4ZT5Q7U7/Belitser and Nurushev - 2015 - Needles and straw in a haystack robust empirical .pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/9HAM5Q2T/1511.html:text/html}
}

@article{carbonetto_scalable_2012,
	title = {Scalable {Variational} {Inference} for {Bayesian} {Variable} {Selection} in {Regression}, and {Its} {Accuracy} in {Genetic} {Association} {Studies}},
	volume = {7},
	issn = {1936-0975, 1931-6690},
	url = {http://projecteuclid.org/euclid.ba/1339616726},
	doi = {10.1214/12-BA703},
	abstract = {The Bayesian approach to variable selection in regression is a powerful tool for tackling many scientific problems. Inference for variable selection models is usually implemented using Markov chain Monte Carlo (MCMC). Because MCMC can impose a high computational cost in studies with a large number of variables, we assess an alternative to MCMC based on a simple variational approximation. Our aim is to retain useful features of Bayesian variable selection at a reduced cost. Using simulations designed to mimic genetic association studies, we show that this simple variational approximation yields posterior inferences in some settings that closely match exact values. In less restrictive (and more realistic) conditions, we show that posterior probabilities of inclusion for individual variables are often incorrect, but variational estimates of other useful quantities{\textbar}including posterior distributions of the hyperparameters{\textbar}are remarkably accurate. We illustrate how these results guide the use of variational inference for a genome-wide association study with thousands of samples and hundreds of thousands of variables.},
	language = {EN},
	number = {1},
	urldate = {2016-09-27},
	journal = {Bayesian Analysis},
	author = {Carbonetto, Peter and Stephens, Matthew},
	month = mar,
	year = {2012},
	mrnumber = {MR2896713},
	keywords = {Monte Carlo, Variable selection, genetic association studies, variational inference},
	pages = {73--108},
	file = {euclid.ba.1339616726.pdf:/Users/magnusmunch/Zotero/storage/NWEDS26P/euclid.ba.1339616726.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/JAQG6PR2/1339616726.html:text/html}
}

@article{tibshirani_least_2004,
	title = {Least angle regression},
	volume = {32},
	issn = {0090-5364},
	url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1083178935/},
	doi = {10.1214/009053604000000067},
	language = {en},
	number = {2},
	urldate = {2015-09-14},
	journal = {The Annals of Statistics},
	author = {Tibshirani, Robert and Johnstone, Iain and Hastie, Trevor and Efron, Bradley},
	month = apr,
	year = {2004},
	pages = {407--499},
	file = {LARS.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\LARS.pdf:application/pdf}
}

@article{bishop_bayesian_1999,
	title = {Bayesian pca},
	url = {http://books.google.com/books?hl=en&lr=&id=bMuzXPzlkG0C&oi=fnd&pg=PA382&dq=%22a+data+set+D+of+observed+d-dimensional+vectors+D+%3D+%7Bt+n+%7D+where+n%22+%22retained+variance+is+a+maximum,+or+equivalently+the+linear+projection+for+which%22+&ots=MwsdzBAGMb&sig=8y5ikc7XyeaXMyg7uCg1VHTmndI},
	urldate = {2016-09-13},
	journal = {Advances in neural information processing systems},
	author = {Bishop, Christopher M.},
	year = {1999},
	pages = {382--388},
	file = {BayesianPCA.pdf:/Users/magnusmunch/Zotero/storage/BNECKZVZ/BayesianPCA.pdf:application/pdf}
}

@article{waldmann_evaluation_2013,
	title = {Evaluation of the lasso and the elastic net in genome-wide association studies},
	volume = {4},
	issn = {1664-8021},
	url = {http://journal.frontiersin.org/article/10.3389/fgene.2013.00270/abstract},
	doi = {10.3389/fgene.2013.00270},
	urldate = {2015-09-14},
	journal = {Frontiers in Genetics},
	author = {Waldmann, Patrik and Mészáros, Gábor and Gredler, Birgit and Fuerst, Christian and Sölkner, Johann},
	year = {2013},
	file = {EvaluationLassoENGenomewideAssociation.pdf:/Users/magnusmunch/Zotero/storage/42E37MPA/EvaluationLassoENGenomewideAssociation.pdf:application/pdf;EvaluationLassoENGenomewideAssociation.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\EvaluationLassoENGenomewideAssociation.pdf:application/pdf}
}

@article{meier_group_2008,
	title = {The group lasso for logistic regression: {Group} {Lasso} for {Logistic} {Regression}},
	volume = {70},
	issn = {13697412},
	shorttitle = {The group lasso for logistic regression},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00627.x},
	doi = {10.1111/j.1467-9868.2007.00627.x},
	language = {en},
	number = {1},
	urldate = {2015-09-14},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Meier, Lukas and Van De Geer, Sara and Bühlmann, Peter},
	month = jan,
	year = {2008},
	pages = {53--71},
	file = {GroupLassoLogisticRegression.pdf:/Users/magnusmunch/Zotero/storage/IUPASA9T/GroupLassoLogisticRegression.pdf:application/pdf;GroupLassoLogisticRegression.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\GroupLassoLogisticRegression.pdf:application/pdf}
}

@article{blei_variational_2016,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior distribution. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	urldate = {2016-09-27},
	journal = {arXiv:1601.00670 [cs, stat]},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1601.00670 PDF:/Users/magnusmunch/Zotero/storage/JPZ2BADG/Blei et al. - 2016 - Variational Inference A Review for Statisticians.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/Z9PF9AIP/1601.html:text/html}
}

@article{kimelberg_alterations_1975,
	title = {Alterations in phospholipid-dependent ({Na}+ +{K}+)-{ATPase} activity due to lipid fluidity. {Effects} of cholesterol and {Mg}2+},
	volume = {413},
	issn = {0006-3002},
	abstract = {The (Na+ +K+)-activated, Mg2+-dependent ATPase from rabbit kidney outer medulla was prepared in a partially inactivated, soluble form depleted of endogenous phospholipids, using deoxycholate. This preparation was reactivated 10 to 50-fold by sonicated liposomes of phosphatidylserine, but not by non-sonicated phosphatidylserine liposomes or sonicated phosphatidylcholine liposomes. The reconstituted enzyme resembled native membrane preparations of (Na+ +K+)-ATPase in its pH optimum being around 7.0, showing optimal activity at Mg2+:ATP mol ratios of approximately 1 and a Km value for ATP of 0.4 mM. Arrhenius plots of this reactivated activity at a constant pH of 7.0 and an Mg2+: ATP mol ratio of 1:1 showed a discontinuity (sharp change of slope) at 17 degrees C, with activation energy (Ea) values of 13-15 kcal/mol above this temperature and 30-35 kcal below it. A further discontinuity was also found at 8.0 degrees C and the Ea below this was very high (greater than 100 kcal/mol). Increased Mg2+ concentrations at Mg2+:ATP ratios in excess of 1:1 inhibited the (Na+ +K+)-ATPase activity and also abolished the discontinuities in the Arrhenius plots. The addition of cholesterol to phosphatidylserine at a 1:1 mol ratio partially inhibited (Na+ +K+)-ATPase reactivation. Arrhenius plots under these conditions showed a single discontinuity at 20 degrees C and Ea values of 22 and 68 kcal/mol above and below this temperature respectively. The ouabain-insensitive Mg2+-ATPase normally showed a linear Arrhenius plot with an Ea of 8 kcal/mol. The cholesterol-phosphatidylserine mixed liposomes stimulated the Mg2+-ATPase activity, which now also showed a discontinuity at 20 degrees C with, however, an increased value of 14 kcal/mol above this temperature and 6 kcal/mol below. Kinetic studies showed that cholesterol had no significant effect on the Km values for ATP. Since both cholesterol and Mg2+ are known to alter the effects of temperature on the fluidity of phospholipids, the above results are discussed in this context.},
	language = {eng},
	number = {1},
	journal = {Biochimica Et Biophysica Acta},
	author = {Kimelberg, H. K.},
	month = nov,
	year = {1975},
	pmid = {90},
	keywords = {Adenosine Triphosphatases, Adenosine Triphosphate, Animals, Cholesterol, Hydrogen-Ion Concentration, Kidney Medulla, Kinetics, Liposomes, Magnesium, Microsomes, Phosphatidylethanolamines, Phosphatidylserines, Potassium, Rabbits, Sodium, Sonication, Temperature, Thermodynamics},
	pages = {143--156}
}

@article{curnutte_manganese-dependent_1976,
	title = {Manganese-dependent {NADPH} oxidation by granulocyte particles. {The} role of superoxide and the nonphysiological nature of the manganese requirement},
	volume = {57},
	issn = {0021-9738},
	doi = {10.1172/JCI108348},
	abstract = {Recent work has indicated that superoxide is involved in the manganese-stimulated oxidation of NADPH by crude granule preparations of guinea pig neutrophils. The characteristics of a model manganese-requiring NADPH-oxidizing system that employs a defined O2-generator have now been compared to the original neutrophil-granule system. With respect to pH dependence, cyanide sensitivity, and reduced pyridine nucleotide specificity, the properties of the two systems are very similar. Additional information has been obtained concerning cation specificity and the kinetics of the metal-catalyzed NADPH oxidation. From the similarities between the properties of the model and neutrophil particle systems, we postulate that the manganese-dependent NADPH oxidation observed in the presence of neutrophil granules represents in large part of nonenzymatic free radical chain involving the oxidation of NADPH to NADP, with O2- as both the chain initiator and one of the propagating species. In this reaction, the neutrophil particles serve only as a source of O2-. Further, the same changes in kinetics (decrease in apparent Km for NADPH) observed previously when granules from phagocytizing rather than resting cells were employed could be mimicked by varying the rate of O2-generation by the model system. We conclude from these results that it is unnecessary to invoke a manganese-requiring enzyme as a component of the phagocytically stimulated respiratory system of the neutrophil.},
	language = {eng},
	number = {4},
	journal = {The Journal of Clinical Investigation},
	author = {Curnutte, J. T. and Karnovsky, M. L. and Babior, B. M.},
	month = apr,
	year = {1976},
	pmid = {7574},
	pmcid = {PMC436750},
	keywords = {Animals, Granulocytes, Guinea Pigs, Iron, Kinetics, Leukocytes, Manganese, NADP, Oxidation-Reduction, Oxygen, Phagocytosis, Superoxides},
	pages = {1059--1067}
}

@article{fan_selective_2010,
	title = {A selective overview of variable selection in high dimensional feature space},
	volume = {20},
	journal = {Statistica Sinica},
	author = {Fan, Jianqing and Lv, Jinchi},
	month = jan,
	year = {2010},
	note = {invited review article},
	pages = {101--148},
	file = {SelectiveOverviewVarSelectionHighDimSpace.pdf:/Users/magnusmunch/Zotero/storage/JPDS74VV/SelectiveOverviewVarSelectionHighDimSpace.pdf:application/pdf;SelectiveOverviewVarSelectionHighDimSpace.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\SelectiveOverviewVarSelectionHighDimSpace.pdf:application/pdf}
}

@article{worathumrong_effect_1975,
	title = {The effect of o-salicylate upon pentose phosphate pathway activity in normal and {G}6PD-deficient red cells},
	volume = {30},
	issn = {0007-1048},
	abstract = {The effect of the major metabolite of aspirin, namely salicylic acid, upon the pentose phosphate pathway (PPP) of normal and G6PD-deficient red cells has been studied. Salicylic acid was shown to inhibit this pathway in proportion to the amount present. At any concentration of this substance there was greater inhibition of the PPP in G6PD-deficient than in normal red cells.},
	language = {eng},
	number = {2},
	journal = {British Journal of Haematology},
	author = {Worathumrong, N. and Grimes, A. J.},
	month = jun,
	year = {1975},
	pmid = {35},
	keywords = {Blood Glucose, Erythrocytes, Glucosephosphate Dehydrogenase Deficiency, Humans, Hydrogen-Ion Concentration, Methylene Blue, Pentosephosphates, Sodium Salicylate},
	pages = {225--231}
}

@book{hastie_elements_2009,
	address = {New York, NY},
	edition = {2nd ed},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	shorttitle = {The elements of statistical learning},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
	year = {2009},
	keywords = {Bioinformatics, Computational intelligence, Data mining, Forecasting, Inference, Machine learning, Methodology, Statistics},
	file = {ESLII_print10.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Statistical learning\\ESLII_print10.pdf:application/pdf}
}

@article{hurd_advantages_2009,
	title = {Advantages of next-generation sequencing versus the microarray in epigenetic research},
	volume = {8},
	issn = {1473-9550, 1477-4062},
	url = {http://bfgp.oxfordjournals.org/cgi/doi/10.1093/bfgp/elp013},
	doi = {10.1093/bfgp/elp013},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Briefings in Functional Genomics and Proteomics},
	author = {Hurd, P. J. and Nelson, C. J.},
	month = may,
	year = {2009},
	pages = {174--183},
	file = {AdvantagesNextGenSeqVersusMicroarrayEpiGenResearch.pdf:/Users/magnusmunch/Zotero/storage/2EKBKPSD/AdvantagesNextGenSeqVersusMicroarrayEpiGenResearch.pdf:application/pdf}
}

@book{jorgensen_statistical_1982,
	address = {New York ; Heidelberg ; Berlin},
	series = {Lecture notes in statistics ; 9. 830800018},
	title = {Statistical properties of the generalized inverse {Gaussian} distribution},
	isbn = {978-0-387-90665-2},
	abstract = {\$\$C note\$\$V Originally presented as the author's thesis (M.Sc.), Aarhus University.},
	language = {eng},
	publisher = {Springer},
	author = {Jørgensen, Bent},
	year = {1982},
	keywords = {31.73 mathematical statistics, QA276.7, Statistiek, Verallgemeinerte inverse Gaussverteilung.},
	file = {bok%3A978-1-4612-5698-4.pdf:/Users/magnusmunch/Zotero/storage/XU6WJNTH/bok%3A978-1-4612-5698-4.pdf:application/pdf}
}

@article{zhao_comparison_2014,
	title = {Comparison of {RNA}-{Seq} and {Microarray} in {Transcriptome} {Profiling} of {Activated} {T} {Cells}},
	volume = {9},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0078644},
	doi = {10.1371/journal.pone.0078644},
	language = {en},
	number = {1},
	urldate = {2016-09-13},
	journal = {PLoS ONE},
	author = {Zhao, Shanrong and Fung-Leung, Wai-Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
	editor = {Zhang, Shu-Dong},
	month = jan,
	year = {2014},
	pages = {e78644},
	file = {ComparisonRNASeqMicroarrayTranscriptomeProfilingActivatedTCells.pdf:/Users/magnusmunch/Zotero/storage/QJF5BP6T/ComparisonRNASeqMicroarrayTranscriptomeProfilingActivatedTCells.pdf:application/pdf}
}

@article{van_de_wiel_better_2016,
	title = {Better prediction by use of co-data: adaptive group-regularized ridge regression},
	volume = {35},
	issn = {02776715},
	shorttitle = {Better prediction by use of co-data},
	url = {http://doi.wiley.com/10.1002/sim.6732},
	doi = {10.1002/sim.6732},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Statistics in Medicine},
	author = {van de Wiel, Mark A. and Lien, Tonje G. and Verlaat, Wina and van Wieringen, Wessel N. and Wilting, Saskia M.},
	month = feb,
	year = {2016},
	pages = {368--381},
	file = {article.pdf:/Users/magnusmunch/Zotero/storage/N7W75WGI/van de Wiel et al. - 2016 - Better prediction by use of co-data adaptive grou.pdf:application/pdf;sim6732-sup-0001-Supplementary1.pdf:/Users/magnusmunch/Zotero/storage/CNPFHP5A/sim6732-sup-0001-Supplementary1.pdf:application/pdf;van de Wiel et al. - 2016 - Better prediction by use of co-data adaptive grou.pdf:/Users/magnusmunch/Zotero/storage/WQFH375F/van de Wiel et al. - 2016 - Better prediction by use of co-data adaptive grou.pdf:application/pdf}
}

@article{huang_empirical_2015,
	title = {Empirical {Bayesian} elastic net for multiple quantitative trait locus mapping},
	volume = {114},
	url = {http://www.nature.com/hdy/journal/v114/n1/abs/hdy201479a.html},
	number = {1},
	urldate = {2016-09-13},
	journal = {Heredity},
	author = {Huang, Anhui and Xu, Shizhong and Cai, Xiaodong},
	year = {2015},
	pages = {107--115},
	file = {EmpiricalBayesENMultipleQTLMapping.pdf:/Users/magnusmunch/Zotero/storage/PA5CB76N/EmpiricalBayesENMultipleQTLMapping.pdf:application/pdf}
}

@article{kpogbezan_empirical_2016,
	title = {An empirical {Bayes} approach to network recovery using external knowledge},
	url = {http://arxiv.org/abs/1605.07514},
	abstract = {Reconstruction of a high-dimensional network may benefit substantially from the inclusion of prior knowledge on the network topology. In the case of gene interaction networks such knowledge may come for instance from pathway repositories like KEGG, or be inferred from data of a pilot study. The Bayesian framework provides a natural means of including such prior knowledge. Based on a Bayesian Simultaneous Equation Model, we develop an appealing empirical Bayes procedure which automatically assesses the relevance of the used prior knowledge. We use variational Bayes method for posterior densities approximation and compare its accuracy with that of Gibbs sampling strategy. Our method is computationally fast, and can outperform known competitors. In a simulation study we show that accurate prior data can greatly improve the reconstruction of the network, but need not harm the reconstruction if wrong. We demonstrate the benefits of the method in an analysis of gene expression data from GEO. In particular, the edges of the recovered network have superior reproducibility (compared to that of competitors) over resampled versions of the data.},
	urldate = {2016-11-08},
	journal = {arXiv:1605.07514 [stat]},
	author = {Kpogbezan, Gino B. and van der Vaart, Aad W. and van Wieringen, Wessel N. and Leday, Gwenaël G. R. and van de Wiel, Mark A.},
	month = may,
	year = {2016},
	note = {arXiv: 1605.07514},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1605.07514 PDF:/Users/magnusmunch/Zotero/storage/T6G8XUUJ/Kpogbezan et al. - 2016 - An empirical Bayes approach to network recovery us.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/CN9NRWDP/1605.html:text/html}
}

@article{beal_variational_2003-1,
	title = {The {Variational} {Bayesian} {EM} {Algorithm} for {Incomplete} {Data}: with {Application} to {Scoring} {Graphical} {Model} {Structures}},
	volume = {7},
	shorttitle = {The {Variational} {Bayesian} {EM} {Algorithm} for {Incomplete} {Data}},
	journal = {Bayesian Statistics},
	author = {Beal, MJ and Ghahramani, Z},
	year = {2003},
	file = {Snapshot:/Users/magnusmunch/Zotero/storage/CUH5QCHC/224881759_The_Variational_Bayesian_EM_Algorithm_for_Incomplete_Data_with_Application_to_Scoring.html:text/html;valencia02.pdf:/Users/magnusmunch/Zotero/storage/74JX9H8E/valencia02.pdf:application/pdf}
}

@article{makar_formate_1975,
	title = {Formate assay in body fluids: application in methanol poisoning},
	volume = {13},
	issn = {0006-2944},
	shorttitle = {Formate assay in body fluids},
	language = {eng},
	number = {2},
	journal = {Biochemical Medicine},
	author = {Makar, A. B. and McMartin, K. E. and Palese, M. and Tephly, T. R.},
	month = jun,
	year = {1975},
	pmid = {1},
	keywords = {Aldehyde Oxidoreductases, Animals, Body Fluids, Carbon Dioxide, Formates, Haplorhini, Humans, Hydrogen-Ion Concentration, Kinetics, Methanol, Methods, Pseudomonas},
	pages = {117--126}
}

@article{park_l1-regularization_2007,
	title = {L1-regularization path algorithm for generalized linear models},
	volume = {69},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com.ezproxy.leidenuniv.nl:2048/doi/10.1111/j.1467-9868.2007.00607.x/abstract},
	doi = {10.1111/j.1467-9868.2007.00607.x},
	abstract = {Summary.  We introduce a path following algorithm for L1-regularized generalized linear models. The L1-regularization procedure is useful especially because it, in effect, selects variables according to the amount of penalization on the L1-norm of the coefficients, in a manner that is less greedy than forward selection–backward deletion. The generalized linear model path algorithm efficiently computes solutions along the entire regularization path by using the predictor–corrector method of convex optimization. Selecting the step length of the regularization parameter is critical in controlling the overall accuracy of the paths; we suggest intuitive and flexible strategies for choosing appropriate values. We demonstrate the implementation with several simulated and real data sets.},
	language = {en},
	number = {4},
	urldate = {2016-09-13},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Park, Mee Young and Hastie, Trevor},
	month = sep,
	year = {2007},
	keywords = {Generalized linear model, Lasso, Path algorithm, Predictor–corrector method, Regularization, Variable selection},
	pages = {659--677},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/BU5XWCBX/Park and Hastie - 2007 - L1-regularization path algorithm for generalized l.pdf:application/pdf;Full Text PDF:/Users/magnusmunch/Zotero/storage/I7S6J7J9/Park and Hastie - 2007 - L1-regularization path algorithm for generalized l.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/48B9222F/abstract.html:text/html;Snapshot:/Users/magnusmunch/Zotero/storage/JPZGKIFZ/abstract.html:text/html}
}

@article{george_calibration_2000,
	title = {Calibration and empirical {Bayes} variable selection},
	volume = {87},
	number = {4},
	journal = {Biometrika},
	author = {george, edward and foster, dean},
	year = {2000},
	pages = {731--747},
	file = {CalibrationEBSelection.pdf:/Users/magnusmunch/Zotero/storage/49QGNVZI/CalibrationEBSelection.pdf:application/pdf}
}

@article{goeman_l1_2009,
	title = {L1 {Penalized} {Estimation} in the {Cox} {Proportional} {Hazards} {Model}},
	issn = {03233847, 15214036},
	url = {http://doi.wiley.com/10.1002/bimj.200900028},
	doi = {10.1002/bimj.200900028},
	language = {en},
	urldate = {2016-09-13},
	journal = {Biometrical Journal},
	author = {Goeman, Jelle J.},
	month = nov,
	year = {2009},
	pages = {NA--NA},
	file = {L1PenalizedEstimationCoxProportionalHazardsModel.pdf:/Users/magnusmunch/Zotero/storage/FX9FCQJN/L1PenalizedEstimationCoxProportionalHazardsModel.pdf:application/pdf}
}

@article{tibshirani_regression_1996,
	title = {Regression shrinkage and selection via the lasso},
	volume = {58},
	number = {1},
	journal = {Journal of the Royal Statistical Society, Series B},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288},
	file = {RegressionShrinkageSelectionLasso.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\RegressionShrinkageSelectionLasso.pdf:application/pdf}
}

@article{polson_local_2012,
	title = {Local shrinkage rules, {Lévy} processes and regularized regression},
	volume = {74},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2011.01015.x/abstract},
	doi = {10.1111/j.1467-9868.2011.01015.x},
	abstract = {Summary.  We use Lévy processes to generate joint prior distributions, and therefore penalty functions, for a location parameter  as p grows large. This generalizes the class of local–global shrinkage rules based on scale mixtures of normals, illuminates new connections between disparate methods and leads to new results for computing posterior means and modes under a wide class of priors. We extend this framework to large-scale regularized regression problems where p{\textgreater}n, and we provide comparisons with other methodologies.},
	language = {en},
	number = {2},
	urldate = {2016-10-25},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Polson, Nicholas G. and Scott, James G.},
	month = mar,
	year = {2012},
	keywords = {Lévy processes, Normal scale mixtures, Partial least squares, Principal components regression, Shrinkage, Sparsity},
	pages = {287--311},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/TGZI96T5/Polson and Scott - 2012 - Local shrinkage rules, Lévy processes and regulari.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8HTX7V9K/abstract.html:text/html}
}

@inproceedings{azevedo-filho_laplaces_1994,
	address = {San Francisco, CA, USA},
	series = {{UAI}'94},
	title = {Laplace's {Method} {Approximations} for {Probabilistic} {Inferencein} {Belief} {Networks} with {Continuous} {Variables}},
	isbn = {978-1-55860-332-5},
	url = {http://dl.acm.org/citation.cfm?id=2074394.2074399},
	abstract = {Laplace's method, a family of asymptotic methods used to approximate integrals, is presented as a potential candidate for the tool box of techniques used for knowledge acquisition and probabilistic inference in belief networks with continuous variables. This technique approximates posterior moments and marginal posterior distributions with reasonable accuracy [errors are O(n-2) for posterior means] in many interesting cases. The method also seems promising for computing approximations for Bayes factors for use in the context of model selection, model uncertainty and mixtures of pdfs. The limitations, regularity conditions and computational difficulties for the implementation of Laplace's method are comparable to those associated with the methods of maximum likelihood and posterior mode analysis.},
	urldate = {2016-09-13},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Azevedo-Filho, Adriano and Shachter, Ross D.},
	year = {1994},
	pages = {28--36},
	file = {ACM Full Text PDF:/Users/magnusmunch/Zotero/storage/T8D93ZEN/Azevedo-Filho and Shachter - 1994 - Laplace's Method Approximations for Probabilistic .pdf:application/pdf}
}

@article{cho_elastic-net_2009,
	title = {Elastic-net regularization approaches for genome-wide association studies of rheumatoid arthritis},
	volume = {3},
	issn = {1753-6561},
	url = {http://www.biomedcentral.com/1753-6561/3/S7/S25},
	doi = {10.1186/1753-6561-3-s7-s25},
	language = {en},
	number = {Suppl 7},
	urldate = {2015-09-14},
	journal = {BMC Proceedings},
	author = {Cho, Seoae and Kim, Haseong and Oh, Sohee and Kim, Kyunga and Park, Taesung},
	year = {2009},
	pages = {S25},
	file = {ENRegularizationGWAssociationRheumatoidArthritis.pdf:/Users/magnusmunch/Zotero/storage/JW4NRJGC/ENRegularizationGWAssociationRheumatoidArthritis.pdf:application/pdf;ENRegularizationGWAssociationRheumatoidArthritis.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\ENRegularizationGWAssociationRheumatoidArthritis.pdf:application/pdf}
}

@article{tipping_probabilistic_1999,
	title = {Probabilistic {Principal} {Component} {Analysis}},
	volume = {61},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00196/abstract},
	doi = {10.1111/1467-9868.00196},
	abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Tipping, Michael E. and Bishop, Christopher M.},
	month = jan,
	year = {1999},
	keywords = {Density estimation, EM algorithm, Gaussian mixtures, Maximum likelihood, Principal component analysis, Probability model},
	pages = {611--622},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/DXK5FWUJ/Tipping and Bishop - 1999 - Probabilistic Principal Component Analysis.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8E9QIIZT/abstract.html:text/html}
}

@article{biane_probability_2001,
	title = {Probability laws related to the {Jacobi} theta and {Riemann} zeta functions, and {Brownian} excursions},
	volume = {38},
	issn = {0273-0979, 1088-9485},
	url = {http://www.ams.org/bull/2001-38-04/S0273-0979-01-00912-0/},
	doi = {10.1090/S0273-0979-01-00912-0},
	abstract = {Abstract:},
	number = {4},
	urldate = {2016-10-07},
	journal = {Bulletin of the American Mathematical Society},
	author = {Biane, Philippe and Pitman, Jim and Yor, Marc},
	year = {2001},
	keywords = {Bessel process, Infinitely divisible laws, functional equation, sums of independent exponential variables},
	pages = {435--465},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/E3DHE52G/Biane et al. - 2001 - Probability laws related to the Jacobi theta and R.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/IQPCE2QJ/S0273-0979-01-00912-0.html:text/html}
}

@article{tai_incorporating_2007,
	title = {Incorporating prior knowledge of predictors into penalized classifiers with multiple penalty terms},
	volume = {23},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/btm234},
	doi = {10.1093/bioinformatics/btm234},
	language = {en},
	number = {14},
	urldate = {2015-09-14},
	journal = {Bioinformatics},
	author = {Tai, F. and Pan, W.},
	month = jul,
	year = {2007},
	pages = {1775--1782},
	file = {IncorpPriorKnowledgePredsPenClassifiersMultPenaltyTerms.pdf:/Users/magnusmunch/Zotero/storage/C9M9HWTG/IncorpPriorKnowledgePredsPenClassifiersMultPenaltyTerms.pdf:application/pdf;IncorpPriorKnowledgePredsPenClassifiersMultPenaltyTerms.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\IncorpPriorKnowledgePredsPenClassifiersMultPenaltyTerms.pdf:application/pdf}
}

@article{zou_adaptive_2006,
	title = {The {Adaptive} {Lasso} and {Its} {Oracle} {Properties}},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000000735},
	doi = {10.1198/016214506000000735},
	language = {en},
	number = {476},
	urldate = {2016-09-13},
	journal = {Journal of the American Statistical Association},
	author = {Zou, Hui},
	month = dec,
	year = {2006},
	pages = {1418--1429},
	file = {AdaptiveLassoOracleProperties.pdf:/Users/magnusmunch/Zotero/storage/Z6CT3PMT/AdaptiveLassoOracleProperties.pdf:application/pdf}
}

@article{waldron_optimized_2011,
	title = {Optimized application of penalized regression methods to diverse genomic data},
	volume = {27},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/content/27/24/3399},
	doi = {10.1093/bioinformatics/btr591},
	abstract = {Motivation: Penalized regression methods have been adopted widely for high-dimensional feature selection and prediction in many bioinformatic and biostatistical contexts. While their theoretical properties are well-understood, specific methodology for their optimal application to genomic data has not been determined.
Results: Through simulation of contrasting scenarios of correlated high-dimensional survival data, we compared the LASSO, Ridge and Elastic Net penalties for prediction and variable selection. We found that a 2D tuning of the Elastic Net penalties was necessary to avoid mimicking the performance of LASSO or Ridge regression. Furthermore, we found that in a simulated scenario favoring the LASSO penalty, a univariate pre-filter made the Elastic Net behave more like Ridge regression, which was detrimental to prediction performance. We demonstrate the real-life application of these methods to predicting the survival of cancer patients from microarray data, and to classification of obese and lean individuals from metagenomic data. Based on these results, we provide an optimized set of guidelines for the application of penalized regression for reproducible class comparison and prediction with genomic data.
Availability and Implementation: A parallelized implementation of the methods presented for regression and for simulation of synthetic data is provided as the pensim R package, available at http://cran.r-project.org/web/packages/pensim/index.html.
Contact: chuttenh@hsph.harvard.edu; juris@ai.utoronto.ca
Supplementary Information: Supplementary data are available at Bioinformatics online.},
	language = {en},
	number = {24},
	urldate = {2016-09-13},
	journal = {Bioinformatics},
	author = {Waldron, Levi and Pintilie, Melania and Tsao, Ming-Sound and Shepherd, Frances A. and Huttenhower, Curtis and Jurisica, Igor},
	month = dec,
	year = {2011},
	pmid = {22156367},
	pages = {3399--3406},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/CBSRGTTF/Waldron et al. - 2011 - Optimized application of penalized regression meth.pdf:application/pdf;Full Text PDF:/Users/magnusmunch/Zotero/storage/ET8MSI3A/Waldron et al. - 2011 - Optimized application of penalized regression meth.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/2JR92KZ9/3399.html:text/html;Snapshot:/Users/magnusmunch/Zotero/storage/P6Z6GZWC/3399.html:text/html}
}

@article{aravkin_estimation_2012,
	title = {On the estimation of hyperparameters for {Empirical} {Bayes} estimators: {Maximum} {Marginal} {Likelihood} vs {Minimum} {MSE}},
	volume = {45},
	issn = {14746670},
	shorttitle = {On the estimation of hyperparameters for {Empirical} {Bayes} estimators},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1474667015379398},
	doi = {10.3182/20120711-3-BE-2027.00353},
	language = {en},
	number = {16},
	urldate = {2016-09-13},
	journal = {IFAC Proceedings Volumes},
	author = {Aravkin, A. and Burke, J.V. and Chiuso, A. and Pillonetto, G.},
	month = jul,
	year = {2012},
	pages = {125--130},
	file = {EstimationHyperparametersEBEstimatorsMMLvsMMSE.pdf:/Users/magnusmunch/Zotero/storage/7ZDAJW86/EstimationHyperparametersEBEstimatorsMMLvsMMSE.pdf:application/pdf}
}

@article{moroi_comparison_1975,
	title = {Comparison between procaine and isocarboxazid metabolism in vitro by a liver microsomal amidase-esterase},
	volume = {24},
	issn = {0006-2952},
	language = {eng},
	number = {16},
	journal = {Biochemical Pharmacology},
	author = {Moroi, K. and Sato, T.},
	month = aug,
	year = {1975},
	pmid = {8},
	keywords = {Amidohydrolases, Animals, Esterases, Hydrogen-Ion Concentration, In Vitro Techniques, Isocarboxazid, Kinetics, Male, Metals, Microsomes, Liver, Phospholipids, Procaine, Proteins, Rats, Subcellular Fractions, Temperature},
	pages = {1517--1521}
}

@article{weissman_global_2013,
	title = {Global convergence of the {EM} algorithm for unconstrained latent variable models with categorical indicators},
	volume = {78},
	number = {1},
	journal = {Psychometrika},
	author = {Weissman, Alexander},
	year = {2013},
	pages = {134--153},
	file = {GlobalConvergenceEMAlgUnconstrLatentVarModelsCatIndicators.pdf:/Users/magnusmunch/Zotero/storage/5S7GWJ7P/GlobalConvergenceEMAlgUnconstrLatentVarModelsCatIndicators.pdf:application/pdf}
}

@article{bovelstad_predicting_2007,
	title = {Predicting survival from microarray data a comparative study},
	volume = {23},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/btm305},
	doi = {10.1093/bioinformatics/btm305},
	language = {en},
	number = {16},
	urldate = {2015-09-14},
	journal = {Bioinformatics},
	author = {Bovelstad, H.M. and Nygard, S. and Storvold, H.L. and Aldrin, M. and Borgan, O. and Frigessi, A. and Lingjaerde, O.C.},
	month = aug,
	year = {2007},
	pages = {2080--2087},
	file = {PredictingSurvivalMicroarrayData.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\PredictingSurvivalMicroarrayData.pdf:application/pdf;PredictingSurvivalMicroarrayData.pdf:/Users/magnusmunch/Zotero/storage/BMGQZJ4A/PredictingSurvivalMicroarrayData.pdf:application/pdf}
}

@article{van_de_geer_adaptive_2011,
	title = {The adaptive and the thresholded {Lasso} for potentially misspecified models (and a lower bound for the {Lasso})},
	volume = {5},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1311600467},
	doi = {10.1214/11-EJS624},
	language = {en},
	urldate = {2016-09-13},
	journal = {Electronic Journal of Statistics},
	author = {van de Geer, Sara and Bühlmann, Peter and Zhou, Shuheng},
	year = {2011},
	pages = {688--749},
	file = {AdaptiveThresholdedLassoPotentiallyMisspecifiedModels.pdf:/Users/magnusmunch/Zotero/storage/GZP99W7R/AdaptiveThresholdedLassoPotentiallyMisspecifiedModels.pdf:application/pdf}
}

@article{hcine_highly_2014,
	title = {Highly {Accurate} {Log} {Skew} {Normal} {Approximation} to the {Sum} of {Correlated} {Lognormals}},
	url = {http://arxiv.org/abs/1501.02347},
	doi = {10.5121/csit.2014.41304},
	abstract = {Several methods have been proposed to approximate the sum of correlated lognormal RVs. However the accuracy of each method relies highly on the region of the resulting distribution being examined, and the individual lognormal parameters, i.e., mean and variance. There is no such method which can provide the needed accuracy for all cases. This paper propose a universal yet very simple approximation method for the sum of correlated lognormals based on log skew normal approximation. The main contribution on this work is to propose an analytical method for log skew normal parameters estimation. The proposed method provides highly accurate approximation to the sum of correlated lognormal distributions over the whole range of dB spreads for any correlation coefficient. Simulation results show that our method outperforms all previously proposed methods and provides an accuracy within 0.01 dB for all cases.},
	urldate = {2016-09-29},
	journal = {arXiv:1501.02347 [cs, math]},
	author = {Hcine, Marwane Ben and Bouallegue, Ridha},
	month = dec,
	year = {2014},
	note = {arXiv: 1501.02347},
	pages = {41--52},
	file = {arXiv\:1501.02347 PDF:/Users/magnusmunch/Zotero/storage/K7Z6AZU5/Hcine and Bouallegue - 2014 - Highly Accurate Log Skew Normal Approximation to t.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/95CMQG8U/1501.html:text/html}
}

@article{jeffreys_invariant_1946,
	title = {An {Invariant} {Form} for the {Prior} {Probability} in {Estimation} {Problems}},
	volume = {186},
	issn = {1364-5021, 1471-2946},
	url = {http://rspa.royalsocietypublishing.org/content/186/1007/453},
	doi = {10.1098/rspa.1946.0056},
	abstract = {It is shown that a certain differential form depending on the values of the parameters in a law of chance is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters. For laws containing a location and a scale parameter a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters. This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems.},
	language = {en},
	number = {1007},
	urldate = {2016-09-13},
	journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
	author = {Jeffreys, Harold},
	month = sep,
	year = {1946},
	pages = {453--461},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/P2MKEW54/Jeffreys - 1946 - An Invariant Form for the Prior Probability in Est.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/45SXDDNE/453.html:text/html}
}

@article{tibshirani_degrees_2012,
	title = {Degrees of freedom in lasso problems},
	volume = {40},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1342625466},
	doi = {10.1214/12-AOS1003},
	language = {en},
	number = {2},
	urldate = {2016-09-13},
	journal = {The Annals of Statistics},
	author = {Tibshirani, Ryan J. and Taylor, Jonathan},
	month = apr,
	year = {2012},
	pages = {1198--1232},
	file = {DegreesFreedomLassoProblems.pdf:/Users/magnusmunch/Zotero/storage/V5DZ7QX9/DegreesFreedomLassoProblems.pdf:application/pdf}
}

@incollection{mackay_hyperparameters:_1996,
	address = {Dordrecht},
	title = {Hyperparameters: {Optimize}, or integrate out?},
	isbn = {978-94-015-8729-7},
	shorttitle = {Hyperparameters},
	url = {http://link.springer.com/chapter/10.1007/978-94-015-8729-7_2},
	urldate = {2016-09-13},
	booktitle = {Maximum entropy and {Bayesian} methods: {Santa} {Barbara}, {California}, {U}.{S}.{A}., 1993},
	publisher = {Springer Netherlands},
	author = {MacKay, David JC},
	editor = {Heidbreder, Glenn R.},
	year = {1996},
	pages = {43--59},
	file = {HyperparametersOptimizeIntegrateOut.pdf:/Users/magnusmunch/Zotero/storage/8K6W266F/HyperparametersOptimizeIntegrateOut.pdf:application/pdf}
}

@incollection{watanabe_application_2003,
	title = {Application of {Variational} {Bayesian} {Approach} to {Speech} {Recognition}},
	url = {http://papers.nips.cc/paper/2174-application-of-variational-bayesian-approach-to-speech-recognition.pdf},
	urldate = {2016-09-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 15},
	publisher = {MIT Press},
	author = {Watanabe, Shinji and Minami, Yasuhiro and Nakamura, Atsushi and Ueda, Naonori},
	editor = {Becker, S. and Thrun, S. and Obermayer, K.},
	year = {2003},
	pages = {1261--1268},
	file = {NIPS Full Text PDF:/Users/magnusmunch/Zotero/storage/4R8RXBJG/Watanabe et al. - 2003 - Application of Variational Bayesian Approach to Sp.pdf:application/pdf;NIPS Snapshort:/Users/magnusmunch/Zotero/storage/J339DID9/2174-application-of-variational-bayesian-approach-to-speech-recognition.html:text/html}
}

@article{brawn_closed-form_2007,
	title = {Closed-form parameter estimates for a truncated gamma distribution},
	volume = {18},
	issn = {11804009, 1099095X},
	url = {http://doi.wiley.com/10.1002/env.841},
	doi = {10.1002/env.841},
	language = {en},
	number = {6},
	urldate = {2016-09-13},
	journal = {Environmetrics},
	author = {Brawn, Dan and Upton, Graham},
	month = sep,
	year = {2007},
	pages = {633--645},
	file = {ClosedFormParameterEstimatesTruncatedGammaDistribution.pdf:/Users/magnusmunch/Zotero/storage/7TXTEJ28/ClosedFormParameterEstimatesTruncatedGammaDistribution.pdf:application/pdf}
}

@article{yuan_model_2006,
	title = {Model {Selection} and {Estimation} in {Regression} with {Grouped} {Variables}},
	volume = {68},
	issn = {1369-7412},
	url = {http://www.jstor.org/stable/3647556},
	abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multi-factor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
	number = {1},
	urldate = {2016-09-13},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Yuan, Ming and Lin, Yi},
	year = {2006},
	pages = {49--67},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/CCXX233Q/Yuan and Lin - 2006 - Model Selection and Estimation in Regression with .pdf:application/pdf}
}

@article{zhou_lognormal_2012,
	title = {Lognormal and {Gamma} {Mixed} {Negative} {Binomial} {Regression}},
	volume = {2012},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4180062/},
	abstract = {In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of random effects, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples.},
	urldate = {2016-09-13},
	journal = {Proceedings of the ... International Conference on Machine Learning. International Conference on Machine Learning},
	author = {Zhou, Mingyuan and Li, Lingbo and Dunson, David and Carin, Lawrence},
	year = {2012},
	pmid = {25279391},
	pmcid = {PMC4180062},
	pages = {1343--1350},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/KV8Z49TR/Zhou et al. - 2012 - Lognormal and Gamma Mixed Negative Binomial Regres.pdf:application/pdf}
}

@article{hans_bayesian_2009,
	title = {Bayesian lasso regression},
	volume = {96},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asp047},
	doi = {10.1093/biomet/asp047},
	language = {en},
	number = {4},
	urldate = {2015-09-14},
	journal = {Biometrika},
	author = {Hans, C.},
	month = dec,
	year = {2009},
	pages = {835--845},
	file = {BayesianLassoRegression.pdf:/Users/magnusmunch/Zotero/storage/6CXRW5NK/BayesianLassoRegression.pdf:application/pdf}
}

@article{scott_expectation-maximization_2013,
	title = {Expectation-maximization for logistic regression},
	url = {http://arxiv.org/abs/1306.0040},
	urldate = {2016-09-13},
	journal = {arXiv preprint arXiv:1306.0040},
	author = {Scott, James G. and Sun, Liang},
	year = {2013},
	file = {EMLogisticRegression.pdf:/Users/magnusmunch/Zotero/storage/ZXVP32KX/EMLogisticRegression.pdf:application/pdf}
}

@misc{r_development_core_team_r:_2008,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	isbn = {3-900051-07-0},
	url = {http://www.R-project.org},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Development Core Team}},
	year = {2008},
	file = {euclid.aos.1091626180.pdf:/Users/magnusmunch/Zotero/storage/J5KWDNE7/euclid.aos.1091626180.pdf:application/pdf}
}

@article{aravkin_mse_2012,
	title = {On the {MSE} {Properties} of {Empirical} {Bayes} {Methods} for {Sparse} {Estimation}},
	volume = {45},
	issn = {14746670},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1474667015380782},
	doi = {10.3182/20120711-3-BE-2027.00301},
	language = {en},
	number = {16},
	urldate = {2016-09-13},
	journal = {IFAC Proceedings Volumes},
	author = {Aravkin, A. and Burke, J.V. and Chiuso, A. and Pillonetto, G.},
	month = jul,
	year = {2012},
	pages = {965--970},
	file = {MSEPropertiesEBMethodsSparseEstimation.pdf:/Users/magnusmunch/Zotero/storage/VVMEWPJZ/MSEPropertiesEBMethodsSparseEstimation.pdf:application/pdf}
}

@article{heisterkamp_choice_nodate,
	title = {On the {Choice} of {Elastic} {Net} {Parameters} for {Generalized} {Lin}-ear {Models}},
	author = {Heisterkamp, Simon H.},
	file = {ChoiceElasticNetParametersGLM.pdf:/Users/magnusmunch/Zotero/storage/EESW8QPN/ChoiceElasticNetParametersGLM.pdf:application/pdf}
}

@article{austin_penalized_2013,
	title = {Penalized regression and risk prediction in genome-wide association studies},
	volume = {6},
	issn = {19321864},
	url = {http://doi.wiley.com/10.1002/sam.11183},
	doi = {10.1002/sam.11183},
	language = {en},
	number = {4},
	urldate = {2015-09-14},
	journal = {Statistical Analysis and Data Mining},
	author = {Austin, Erin and Pan, Wei and Shen, Xiaotong},
	month = aug,
	year = {2013},
	pages = {315--328},
	file = {PenalizedRegressionRiskPredGWAssociation.pdf:/Users/magnusmunch/Zotero/storage/EK52MVNC/PenalizedRegressionRiskPredGWAssociation.pdf:application/pdf;PenalizedRegressionRiskPredGWAssociation.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\PenalizedRegressionRiskPredGWAssociation.pdf:application/pdf}
}

@article{aravkin_convex_2014,
	title = {Convex vs non-convex estimators for regression and sparse estimation: the mean squared error properties of {ARD} and {GLasso}.},
	volume = {15},
	shorttitle = {Convex vs non-convex estimators for regression and sparse estimation},
	url = {http://www.jmlr.org/papers/volume15/aravkin14a/aravkin14a.pdf},
	number = {1},
	urldate = {2016-09-13},
	journal = {Journal of Machine Learning Research},
	author = {Aravkin, Aleksandr Y. and Burke, James V. and Chiuso, Alessandro and Pillonetto, Gianluigi},
	year = {2014},
	pages = {217--252},
	file = {ConvexvsNonconvexApproachesSparseEstimation.pdf:/Users/magnusmunch/Zotero/storage/M54FRRJP/ConvexvsNonconvexApproachesSparseEstimation.pdf:application/pdf}
}

@article{chow_studies_1975,
	title = {Studies of oxygen binding energy to hemoglobin molecule},
	volume = {66},
	issn = {0006-291X},
	language = {eng},
	number = {4},
	journal = {Biochemical and Biophysical Research Communications},
	author = {Chow, Y. W. and Pietranico, R. and Mukerji, A.},
	month = oct,
	year = {1975},
	pmid = {6},
	keywords = {Binding Sites, Cobalt, Hemoglobins, Humans, Hydrogen-Ion Concentration, Iron, Ligands, Mathematics, Oxygen, Oxyhemoglobins, Protein Binding, Spectrum Analysis},
	pages = {1424--1431}
}

@article{holmes_bayesian_2006,
	title = {Bayesian auxiliary variable models for binary and multinomial regression},
	volume = {1},
	url = {http://projecteuclid.org/euclid.ba/1340371078},
	number = {1},
	urldate = {2016-09-13},
	journal = {Bayesian analysis},
	author = {Holmes, Chris C. and Held, Leonhard and {others}},
	year = {2006},
	pages = {145--168},
	file = {BayesianAuxiliaryVariableModelsBinaryMultinomialRegression.pdf:/Users/magnusmunch/Zotero/storage/5BEFI3RP/BayesianAuxiliaryVariableModelsBinaryMultinomialRegression.pdf:application/pdf}
}

@article{michael_generating_1976,
	title = {Generating {Random} {Variates} {Using} {Transformations} with {Multiple} {Roots}},
	volume = {30},
	issn = {0003-1305},
	url = {http://www.jstor.org/stable/2683801},
	doi = {10.2307/2683801},
	abstract = {The general approach to generating random variates through transformations with multiple roots is discussed. Multinomial probabilities are determined for the selection of the different roots. An application of the general result yields a new and simple technique for the generation of variates from the inverse Gaussian distribution.},
	number = {2},
	urldate = {2016-11-07},
	journal = {The American Statistician},
	author = {Michael, John R. and Schucany, William R. and Haas, Roy W.},
	year = {1976},
	pages = {88--90},
	file = {2683801.pdf:/Users/magnusmunch/Zotero/storage/UJU6WK7Q/2683801.pdf:application/pdf}
}

@article{van_houwelingen_cross-validated_2006,
	title = {Cross-validated {Cox} regression on microarray gene expression data},
	volume = {25},
	issn = {02776715, 10970258},
	url = {http://doi.wiley.com/10.1002/sim.2353},
	doi = {10.1002/sim.2353},
	language = {en},
	number = {18},
	urldate = {2016-09-13},
	journal = {Statistics in Medicine},
	author = {van Houwelingen, Hans C. and Bruinsma, Tako and Hart, Augustinus A. M. and van't Veer, Laura J. and Wessels, Lodewyk F. A.},
	month = sep,
	year = {2006},
	pages = {3201--3216},
	file = {CVCoxRegressionMicroarrayGeneExpressionData.pdf:/Users/magnusmunch/Zotero/storage/VT8MCDDC/CVCoxRegressionMicroarrayGeneExpressionData.pdf:application/pdf}
}

@article{hoerl_ridge_1970,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {12},
	issn = {00401706},
	shorttitle = {Ridge {Regression}},
	url = {http://www.jstor.org/stable/1267351?origin=crossref},
	doi = {10.2307/1267351},
	number = {1},
	urldate = {2015-09-14},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	month = feb,
	year = {1970},
	pages = {55},
	file = {RidgeRegressionBiasedEstimationNonorthogonalProblems.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\RidgeRegressionBiasedEstimationNonorthogonalProblems.pdf:application/pdf}
}

@article{huang_adaptive_2008,
	title = {Adaptive {Lasso} for sparse high-dimensional regression models},
	url = {http://www.jstor.org/stable/24308572},
	urldate = {2016-09-13},
	journal = {Statistica Sinica},
	author = {Huang, Jian and Ma, Shuangge and Zhang, Cun-Hui},
	year = {2008},
	pages = {1603--1618},
	file = {AdaptiveLassoSparseHighDimensionalRegressionModels.pdf:/Users/magnusmunch/Zotero/storage/SWA3V5BR/AdaptiveLassoSparseHighDimensionalRegressionModels.pdf:application/pdf}
}

@article{dempster_maximum_1977,
	title = {Maximum {Likelihood} from {Incomplete} {Data} via the {EM} {Algorithm}},
	volume = {39},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2984875},
	abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	number = {1},
	urldate = {2016-09-13},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	year = {1977},
	pages = {1--38},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/2NBCF56N/Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via the EM.pdf:application/pdf}
}

@article{harris_copper-induced_1976,
	title = {Copper-induced activation of aortic lysyl oxidase in vivo},
	volume = {73},
	issn = {0027-8424},
	abstract = {Raising day-old chicks on diets lacking copper severely depressed the activity of lysyl oxidase, a copper metalloenzyme in connective tissue. Administration of CuSO4 either through the diet or through intraperitoneal injections restored the lysyl oxidase activity in aortic tissue. Two hours after the chicks received CuSO4 (1 mg/kg) the activity of lysyl oxidase rose rapidly to attain, within 4-6 hr, a new steady-state level which was five to 20 times higher than the basal (saline-injected) activity. Twenty hours after copper administration, activity was still higher, in some experiments double that achieved at 6 hr. Very low amounts of cycloheximide injected intraperitoneally 45 min before and 3 hr after copper suppressed the activation response by two-thirds. Cycloheximide given 2 or 4 hr after the copper was only one-half as effective. Actinomycin D caused only a 10-15\% inhibition of the copper-induced activation. The data suggest that copper is a key regulator of lysyl oxidase activity in aorta and may in fact be a major determinant of the steady-state levels of the enzyme in that tissue.},
	language = {eng},
	number = {2},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Harris, E. D.},
	month = feb,
	year = {1976},
	pmid = {1753},
	pmcid = {PMC335910},
	keywords = {Amino Acid Oxidoreductases, Animals, Aorta, Chickens, Copper, Cycloheximide, Dactinomycin, Enzyme Activation, Male, Protein-Lysine 6-Oxidase},
	pages = {371--374},
	file = {2673607.pdf:/Users/magnusmunch/Zotero/storage/MFZXZFPM/2673607.pdf:application/pdf}
}

@article{donoho_ideal_1994,
	title = {Ideal {Spatial} {Adaptation} by {Wavelet} {Shrinkage}},
	volume = {81},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2337118?origin=crossref},
	doi = {10.2307/2337118},
	number = {3},
	urldate = {2015-09-14},
	journal = {Biometrika},
	author = {Donoho, David L. and Johnstone, Iain M.},
	month = aug,
	year = {1994},
	pages = {425},
	file = {IdealSpatialAdaptionWaveletShrinkage.pdf:/Users/magnusmunch/Zotero/storage/36IIDIAD/IdealSpatialAdaptionWaveletShrinkage.pdf:application/pdf;IdealSpatialAdaptionWaveletShrinkage.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\IdealSpatialAdaptionWaveletShrinkage.pdf:application/pdf}
}

@article{jaakkola_bayesian_2000,
	title = {Bayesian parameter estimation via variational methods},
	volume = {10},
	url = {http://link.springer.com/article/10.1023/A:1008932416310},
	number = {1},
	urldate = {2016-09-13},
	journal = {Statistics and Computing},
	author = {Jaakkola, Tommi S. and Jordan, Michael I.},
	year = {2000},
	pages = {25--37},
	file = {art%3A10.1023%2FA%3A1008932416310.pdf:/Users/magnusmunch/Zotero/storage/UZFCTMZR/art%3A10.1023%2FA%3A1008932416310.pdf:application/pdf;art%3A10.1023%2FA%3A1008932416310.pdf:/Users/magnusmunch/Zotero/storage/VIBNVWR5/art%3A10.1023%2FA%3A1008932416310.pdf:application/pdf}
}

@inproceedings{scott_fully_2012,
	title = {Fully {Bayesian} {Inference} for {Neural} {Models} with {Negative}-{Binomial} {Spiking}.pdf},
	url = {http://papers.nips.cc/paper/4567-fully-bayesian-inference-for-neural-models-with-negative-binomial-spiking.pdf},
	booktitle = {Advances in neural information processing systems 25},
	publisher = {Curran Associates, Inc.},
	author = {Scott, James G. and Pillow, Jonathan W.},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1898--1906},
	file = {FullyBayesianInferenceNeuralModelsNBSpiking.pdf:/Users/magnusmunch/Zotero/storage/B267GCEG/FullyBayesianInferenceNeuralModelsNBSpiking.pdf:application/pdf}
}

@incollection{bernardo_shrink_2011,
	title = {Shrink {Globally}, {Act} {Locally}: {Sparse} {Bayesian} {Regularization} and {Prediction}},
	isbn = {978-0-19-969458-7},
	shorttitle = {Shrink {Globally}, {Act} {Locally}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199694587.001.0001/acprof-9780199694587-chapter-17},
	urldate = {2016-10-25},
	booktitle = {Bayesian {Statistics} 9},
	publisher = {Oxford University Press},
	author = {Polson, Nicholas G. and Scott, James G.},
	editor = {Bernardo, José M. and Bayarri, M. J. and Berger, James O. and Dawid, A. P. and Heckerman, David and Smith, Adrian F. M. and West, Mike},
	month = oct,
	year = {2011},
	pages = {501--538},
	file = {Bayes1.pdf:/Users/magnusmunch/Zotero/storage/HQPRWEJJ/Bayes1.pdf:application/pdf}
}

@inproceedings{hinton_keeping_1993,
	address = {New York, NY, USA},
	series = {{COLT} '93},
	title = {Keeping the {Neural} {Networks} {Simple} by {Minimizing} the {Description} {Length} of the {Weights}},
	isbn = {978-0-89791-611-0},
	url = {http://doi.acm.org/10.1145/168304.168306},
	doi = {10.1145/168304.168306},
	urldate = {2016-09-27},
	booktitle = {Proceedings of the {Sixth} {Annual} {Conference} on {Computational} {Learning} {Theory}},
	publisher = {ACM},
	author = {Hinton, Geoffrey E. and van Camp, Drew},
	year = {1993},
	pages = {5--13},
	file = {ACM Full Text PDF:/Users/magnusmunch/Zotero/storage/JZ2AQK6D/Hinton and van Camp - 1993 - Keeping the Neural Networks Simple by Minimizing t.pdf:application/pdf}
}

@article{heisterkamp_empirical_1999,
	title = {Empirical {Bayesian} estimator for a {Poisson} process propagated in time},
	volume = {41},
	number = {4},
	journal = {Biometrical Journal},
	author = {Heisterkamp, Simon H. and van Houwelingen, Hans C. and downs, a. m.},
	year = {1999},
	pages = {385--400},
	file = {EmpiricalBayesianEstimatorsPoissonProcess.pdf:/Users/magnusmunch/Zotero/storage/23FD2PGN/EmpiricalBayesianEstimatorsPoissonProcess.pdf:application/pdf}
}

@article{hahn_decoupling_2015,
	title = {Decoupling {Shrinkage} and {Selection} in {Bayesian} {Linear} {Models}: {A} {Posterior} {Summary} {Perspective}},
	volume = {110},
	issn = {0162-1459},
	shorttitle = {Decoupling {Shrinkage} and {Selection} in {Bayesian} {Linear} {Models}},
	url = {http://dx.doi.org/10.1080/01621459.2014.993077},
	doi = {10.1080/01621459.2014.993077},
	abstract = {Selecting a subset of variables for linear models remains an active area of research. This article reviews many of the recent contributions to the Bayesian model selection and shrinkage prior literature. A posterior variable selection summary is proposed, which distills a full posterior distribution over regression coefficients into a sequence of sparse linear predictors.},
	number = {509},
	urldate = {2016-09-15},
	journal = {Journal of the American Statistical Association},
	author = {Hahn, P. Richard and Carvalho, Carlos M.},
	month = jan,
	year = {2015},
	pages = {435--448},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/EFEEDR5I/Hahn and Carvalho - 2015 - Decoupling Shrinkage and Selection in Bayesian Lin.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/Q6JSS4F4/01621459.2014.html:text/html}
}

@article{bergersen_weighted_2011,
	title = {Weighted {Lasso} with {Data} {Integration}},
	volume = {10},
	issn = {1544-6115},
	url = {http://www.degruyter.com/view/j/sagmb.2011.10.issue-1/sagmb.2011.10.1.1703/sagmb.2011.10.1.1703.xml},
	doi = {10.2202/1544-6115.1703},
	number = {1},
	urldate = {2015-09-14},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Bergersen, Linn Cecilie and Glad, Ingrid K. and Lyng, Heidi},
	month = jan,
	year = {2011},
	pages = {1--29},
	file = {WeightedLassoDataIntegration.pdf:/Users/magnusmunch/Zotero/storage/MBI5ZXCI/WeightedLassoDataIntegration.pdf:application/pdf;WeightedLassoDataIntegration.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\WeightedLassoDataIntegration.pdf:application/pdf;WeightedLassoDataIntegration.pdf:/Users/magnusmunch/Zotero/storage/THXSADP8/WeightedLassoDataIntegration.pdf:application/pdf}
}

@misc{wetterstrand_dna_2014,
	title = {{DNA} sequencing costs: data from the {NHGRI} genome sequencing program ({GSP})},
	url = {http://www.genome.gov/sequencingcosts/},
	urldate = {2015-04-29},
	author = {Wetterstrand, Kris},
	year = {2014}
}

@article{friedman_regularization_2010,
	title = {Regularization paths for generalized linear models via coordinate descent},
	volume = {33},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = jan,
	year = {2010},
	pages = {1--22},
	file = {v33i01.pdf:/Users/magnusmunch/Zotero/storage/IEIJNSEI/v33i01.pdf:application/pdf}
}

@article{hesterberg_least_2008,
	title = {Least angle and ℓ 1 penalized regression: {A} review},
	volume = {2},
	issn = {1935-7516},
	shorttitle = {Least angle and ℓ 1 penalized regression},
	url = {http://projecteuclid.org/euclid.ssu/1211317636},
	doi = {10.1214/08-SS035},
	language = {en},
	number = {0},
	urldate = {2015-09-14},
	journal = {Statistics Surveys},
	author = {Hesterberg, Tim and Choi, Nam Hee and Meier, Lukas and Fraley, Chris},
	year = {2008},
	pages = {61--93},
	file = {LeastAngleL1PenalizedRegressionReview.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\LeastAngleL1PenalizedRegressionReview.pdf:application/pdf;LeastAngleL1PenalizedRegressionReview.pdf:/Users/magnusmunch/Zotero/storage/UU323SII/LeastAngleL1PenalizedRegressionReview.pdf:application/pdf}
}

@article{ishwaran_spike_2005,
	title = {Spike and {Slab} {Variable} {Selection}: {Frequentist} and {Bayesian} {Strategies}},
	volume = {33},
	issn = {0090-5364},
	shorttitle = {Spike and {Slab} {Variable} {Selection}},
	url = {http://www.jstor.org/stable/3448605},
	abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.},
	number = {2},
	urldate = {2016-11-02},
	journal = {The Annals of Statistics},
	author = {Ishwaran, Hemant and Rao, J. Sunil},
	year = {2005},
	pages = {730--773},
	file = {3448605.pdf:/Users/magnusmunch/Zotero/storage/I62XR967/3448605.pdf:application/pdf}
}

@article{chakraborty_bayesian_2011,
	title = {A {Bayesian} hybrid {Huberized} support vector machine and its applications in high-dimensional medical data},
	volume = {55},
	issn = {01679473},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947310003671},
	doi = {10.1016/j.csda.2010.09.024},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Computational Statistics \& Data Analysis},
	author = {Chakraborty, Sounak and Guo, Ruixin},
	month = mar,
	year = {2011},
	pages = {1342--1356},
	file = {1-s2.0-S0167947310003671-main.pdf:/Users/magnusmunch/Zotero/storage/Z6HZTH9Z/1-s2.0-S0167947310003671-main.pdf:application/pdf}
}

@book{lariccia_maximum_2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Maximum {Penalized} {Likelihood} {Estimation}},
	volume = {2},
	isbn = {978-0-387-40267-3 978-0-387-68902-9},
	url = {http://link.springer.com/10.1007/b12285},
	urldate = {2016-09-23},
	publisher = {Springer New York},
	author = {LaRiccia, Vincent N. and Eggermont, Paul P.},
	year = {2009},
	file = {MaximumPenalizedLikelihoodEstimation_Vol1_DensityEstimation.pdf:/Users/magnusmunch/Zotero/storage/KQRWIMSN/MaximumPenalizedLikelihoodEstimation_Vol1_DensityEstimation.pdf:application/pdf}
}

@article{jamil_selection_2012,
	title = {Selection properties of type {II} maximum likelihood (empirical {Bayes}) in linear models with individual variance components for predictors},
	volume = {33},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865512000074},
	doi = {10.1016/j.patrec.2012.01.004},
	abstract = {Maximum likelihood (ML) in the linear model overfits when the number of predictors (M) exceeds the number of objects (N). One of the possible solution is the relevance vector machine (RVM) which is a form of automatic relevance detection and has gained popularity in the pattern recognition machine learning community by the famous textbook of Bishop (2006). RVM assigns individual precisions to weights of predictors which are then estimated by maximizing the marginal likelihood (type II ML or empirical Bayes). We investigated the selection properties of RVM both analytically and by experiments in a regression setting.

We show analytically that RVM selects predictors when the absolute z-ratio ({\textbar}least squares estimate{\textbar}/standard error) exceeds 1 in the case of orthogonal predictors and, for M = 2, that this still holds true for correlated predictors when the other z-ratio is large. RVM selects the stronger of two highly correlated predictors. In experiments with real and simulated data, RVM is outcompeted by other popular regularization methods (LASSO and/or PLS) in terms of the prediction performance. We conclude that type II ML is not the general answer in high dimensional prediction problems.

In extensions of RVM to obtain stronger selection, improper priors (based on the inverse gamma family) have been assigned to the inverse precisions (variances) with parameters estimated by penalized marginal likelihood. We critically assess this approach and suggest a proper variance prior related to the Beta distribution which gives similar selection and shrinkage properties and allows a fully Bayesian treatment.},
	number = {9},
	urldate = {2016-10-31},
	journal = {Pattern Recognition Letters},
	author = {Jamil, Tahira and ter Braak, Cajo J. F.},
	month = jul,
	year = {2012},
	keywords = {Automatic relevance detection, Empirical Bayes, Lasso, Relevance vector machine, Sparse model, Type II maximum likelihood},
	pages = {1205--1212},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/G2HA73U8/Jamil and ter Braak - 2012 - Selection properties of type II maximum likelihood.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/K25ICA63/S0167865512000074.html:text/html}
}

@article{jaccard_lois_1902,
	title = {lois de distribution florale dans la zone alpine},
	volume = {38},
	number = {144},
	journal = {Bulletin de la société vaudoise des sciences naturelles},
	author = {Jaccard, Paul},
	year = {1902},
	file = {JaccardIndex.pdf:/Users/magnusmunch/Zotero/storage/7N3WD4RM/JaccardIndex.pdf:application/pdf}
}

@article{csiszar_information_1984,
	title = {Information {Geometry} and {Alternating} {Minimization} {Procedures}.pdf},
	number = {1},
	journal = {Statistics and decisions},
	author = {Csiszár, I. and Tusnády, Gábor},
	year = {1984},
	pages = {205--237},
	file = {InformationGeometryAlternatingMinimizationProcedures.pdf:/Users/magnusmunch/Zotero/storage/RU42J42Q/InformationGeometryAlternatingMinimizationProcedures.pdf:application/pdf}
}

@incollection{barber_laplace_2016,
	series = {Abel {Symposia}},
	title = {Laplace {Approximation} in {High}-{Dimensional} {Bayesian} {Regression}},
	copyright = {©2016 Springer International Publishing Switzerland},
	isbn = {978-3-319-27097-5 978-3-319-27099-9},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-27099-9_2},
	abstract = {We consider Bayesian variable selection in sparse high-dimensional regression, where the number of covariates p may be large relative to the sample size n, but at most a moderate number q of covariates are active. Specifically, we treat generalized linear models. For a single fixed sparse model with well-behaved prior distribution, classical theory proves that the Laplace approximation to the marginal likelihood of the model is accurate for sufficiently large sample size n. We extend this theory by giving results on uniform accuracy of the Laplace approximation across all models in a high-dimensional scenario in which p and q, and thus also the number of considered models, may increase with n. Moreover, we show how this connection between marginal likelihood and Laplace approximation can be used to obtain consistency results for Bayesian approaches to variable selection in high-dimensional regression.},
	language = {en},
	number = {11},
	urldate = {2016-09-21},
	booktitle = {Statistical {Analysis} for {High}-{Dimensional} {Data}},
	publisher = {Springer International Publishing},
	author = {Barber, Rina Foygel and Drton, Mathias and Tan, Kean Ming},
	editor = {Frigessi, Arnoldo and Bühlmann, Peter and Glad, Ingrid K. and Langaas, Mette and Richardson, Sylvia and Vannucci, Marina},
	year = {2016},
	doi = {10.1007/978-3-319-27099-9_2},
	keywords = {Bioinformatics, Computational Mathematics and Numerical Analysis, Statistical Theory and Methods, Statistics and Computing/Statistics Programs, Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences, Statistics for Life Sciences, Medicine, Health Sciences},
	pages = {15--36},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/C6GG92CW/Barber et al. - 2016 - Laplace Approximation in High-Dimensional Bayesian.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/HTIQ5UTK/Barber et al. - 2016 - Laplace Approximation in High-Dimensional Bayesian.html:text/html}
}

@article{efron_empirical_2001,
	title = {Empirical {Bayes} analysis of a microarray experiment},
	volume = {96},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753382129},
	number = {456},
	urldate = {2016-09-13},
	journal = {Journal of the American statistical association},
	author = {Efron, Bradley and Tibshirani, Robert and Storey, John D. and Tusher, Virginia},
	year = {2001},
	pages = {1151--1160},
	file = {1-s2.0-S0167947310003671-main.pdf:/Users/magnusmunch/Zotero/storage/442ZZJ4U/1-s2.0-S0167947310003671-main.pdf:application/pdf;EmpiricalBayesAnalysisMicroarrayExperiment.pdf:/Users/magnusmunch/Zotero/storage/QD6H8VXR/EmpiricalBayesAnalysisMicroarrayExperiment.pdf:application/pdf}
}

@article{zou_one-step_2008,
	title = {One-step sparse estimates in nonconcave penalized likelihood models},
	volume = {36},
	issn = {0090-5364, 2168-8966},
	url = {http://projecteuclid.org/euclid.aos/1216237287},
	doi = {10.1214/009053607000000802},
	abstract = {Fan and Li propose a family of variable selection methods via penalized likelihood using concave penalty functions. The nonconcave penalized likelihood estimators enjoy the oracle properties, but maximizing the penalized likelihood function is computationally challenging, because the objective function is nondifferentiable and nonconcave. In this article, we propose a new unified algorithm based on the local linear approximation (LLA) for maximizing the penalized likelihood for a broad class of concave penalty functions. Convergence and other theoretical properties of the LLA algorithm are established. A distinguished feature of the LLA algorithm is that at each LLA step, the LLA estimator can naturally adopt a sparse representation. Thus, we suggest using the one-step LLA estimator from the LLA algorithm as the final estimates. Statistically, we show that if the regularization parameter is appropriately chosen, the one-step LLA estimates enjoy the oracle properties with good initial estimators. Computationally, the one-step LLA estimation methods dramatically reduce the computational cost in maximizing the nonconcave penalized likelihood. We conduct some Monte Carlo simulation to assess the finite sample performance of the one-step sparse estimation methods. The results are very encouraging.},
	language = {EN},
	number = {4},
	urldate = {2016-10-26},
	journal = {The Annals of Statistics},
	author = {Zou, Hui and Li, Runze},
	month = aug,
	year = {2008},
	mrnumber = {MR2435443},
	keywords = {AIC, BIC, Lasso, SCAD, one-step estimator, oracle properties},
	pages = {1509--1533},
	file = {euclid.aos.1216237287.pdf:/Users/magnusmunch/Zotero/storage/5S5RUSCI/euclid.aos.1216237287.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/2HU6UHKD/1216237287.html:text/html}
}

@article{tapp_evaluation_2012,
	title = {Evaluation of multiple variate selection methods from a biological perspective: a nutrigenomics case study},
	volume = {7},
	issn = {1555-8932, 1865-3499},
	shorttitle = {Evaluation of multiple variate selection methods from a biological perspective},
	url = {http://link.springer.com/10.1007/s12263-012-0288-4},
	doi = {10.1007/s12263-012-0288-4},
	language = {en},
	number = {3},
	urldate = {2015-09-14},
	journal = {Genes \& Nutrition},
	author = {Tapp, Henri S. and Radonjic, Marijana and Kate Kemsley, E. and Thissen, Uwe},
	month = jul,
	year = {2012},
	pages = {387--397},
	file = {EvaluationVariateSelectionMethodsNutrigenomcs.pdf:/Users/magnusmunch/Zotero/storage/IMD26BR5/EvaluationVariateSelectionMethodsNutrigenomcs.pdf:application/pdf;EvaluationVariateSelectionMethodsNutrigenomcs.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\EvaluationVariateSelectionMethodsNutrigenomcs.pdf:application/pdf}
}

@article{casella_empirical_2001,
	title = {Empirical {Bayes} {Gibbs} sampling},
	volume = {2},
	number = {4},
	journal = {Biostatistics},
	author = {Casella, George},
	year = {2001},
	pages = {485--500},
	file = {Biostat-2001-Casella-485-500.pdf:/Users/magnusmunch/Zotero/storage/38IRWHCP/Biostat-2001-Casella-485-500.pdf:application/pdf}
}

@article{futreal_census_2004,
	title = {A census of human cancer genes},
	volume = {4},
	issn = {1474-175X, 1474-1768},
	url = {http://www.nature.com/doifinder/10.1038/nrc1299},
	doi = {10.1038/nrc1299},
	number = {3},
	urldate = {2016-09-13},
	journal = {Nature Reviews Cancer},
	author = {Futreal, P. Andrew and Coin, Lachlan and Marshall, Mhairi and Down, Thomas and Hubbard, Timothy and Wooster, Richard and Rahman, Nazneen and Stratton, Michael R.},
	month = mar,
	year = {2004},
	pages = {177--183},
	file = {CensusHumanCancerGenes.pdf:/Users/magnusmunch/Zotero/storage/G5TCUQJ2/CensusHumanCancerGenes.pdf:application/pdf;CensusHumanCancerGenes.pdf:/Users/magnusmunch/Zotero/storage/RDX5MCHD/CensusHumanCancerGenes.pdf:application/pdf}
}

@incollection{neal_view_1998,
	series = {{NATO} {ASI} {Series}},
	title = {A {View} of the {Em} {Algorithm} that {Justifies} {Incremental}, {Sparse}, and other {Variants}},
	copyright = {©1998 Springer Science+Business Media Dordrecht},
	isbn = {978-94-010-6104-9 978-94-011-5014-9},
	url = {http://link.springer.com/chapter/10.1007/978-94-011-5014-9_12},
	abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
	language = {en},
	number = {89},
	urldate = {2016-09-27},
	booktitle = {Learning in {Graphical} {Models}},
	publisher = {Springer Netherlands},
	author = {Neal, Radford M. and Hinton, Geoffrey E.},
	editor = {Jordan, Michael I.},
	year = {1998},
	doi = {10.1007/978-94-011-5014-9_12},
	keywords = {Artificial Intelligence (incl. Robotics), Statistical Physics, Dynamical Systems and Complexity, Statistics, general},
	pages = {355--368},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/R7ZTS7EZ/Neal and Hinton - 1998 - A View of the Em Algorithm that Justifies Incremen.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/RWXGTBBN/10.html:text/html}
}

@article{li_bayesian_2010,
	title = {The {Bayesian} elastic net},
	volume = {5},
	issn = {1936-0975},
	url = {http://projecteuclid.org/euclid.ba/1340369796},
	doi = {10.1214/10-BA506},
	language = {en},
	number = {1},
	urldate = {2015-09-14},
	journal = {Bayesian Analysis},
	author = {Li, Qing and Lin, Nan},
	month = mar,
	year = {2010},
	pages = {151--170},
	file = {BayesianElasticNet.pdf:/Users/magnusmunch/Zotero/storage/X6AEU9NC/BayesianElasticNet.pdf:application/pdf}
}

@incollection{mackay_developments_1995,
	title = {Developments in {Probabilistic} {Modelling} with {Neural} {Networks} — {Ensemble} {Learning}},
	copyright = {©1995 Springer-Verlag London Limited},
	isbn = {978-3-540-19992-2 978-1-4471-3087-1},
	url = {http://link.springer.com/chapter/10.1007/978-1-4471-3087-1_37},
	abstract = {Ensemble learning by variational free energy minimization is a framework for statistical inference in which an ensemble of parameter vectors is optimized rather than a single parameter vector. The ensemble approximates the posterior probability distribution of the parameters. In this paper I give a review of ensemble learning using a simple example.},
	language = {en},
	urldate = {2016-09-26},
	booktitle = {Neural {Networks}: {Artificial} {Intelligence} and {Industrial} {Applications}},
	publisher = {Springer London},
	author = {MacKay, David J. C.},
	editor = {Kappen, Bert and Gielen, Stan},
	year = {1995},
	doi = {10.1007/978-1-4471-3087-1_37},
	keywords = {Artificial Intelligence (incl. Robotics), Neurosciences, Pattern Recognition},
	pages = {191--198},
	file = {10.1.1.49.3128.pdf:/Users/magnusmunch/Zotero/storage/DN6UKEP3/10.1.1.49.3128.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8TJFWCUW/10.html:text/html;Snapshot:/Users/magnusmunch/Zotero/storage/99ENFQE3/10.html:text/html}
}

@article{karabatsos_fast_2014,
	title = {Fast {Marginal} {Likelihood} {Estimation} of the {Ridge} {Parameter} (s) in {Ridge} {Regression} and {Generalized} {Ridge} {Regression} for {Big} {Data}},
	url = {http://arxiv.org/abs/1409.2437},
	urldate = {2016-09-13},
	journal = {arXiv preprint arXiv:1409.2437},
	author = {Karabatsos, George},
	year = {2014},
	file = {FastMLLEstimRidgeParsRidgeRegrGeneralizedRidgeRegBigData.pdf:/Users/magnusmunch/Zotero/storage/Q5HHUSRH/FastMLLEstimRidgeParsRidgeRegrGeneralizedRidgeRegBigData.pdf:application/pdf}
}

@article{ruli_improved_2015,
	title = {Improved {Laplace} {Approximation} for {Marginal} {Likelihoods}},
	url = {http://arxiv.org/abs/1502.06440},
	abstract = {Intractable multidimensional integrals arise very often both in Bayesian and frequentist applications. The Laplace formula is widely used to approximate such integrals. However, in large dimensions, when the shape of the integrand function is far from that of the Gaussian density and/or the sample size is small, the standard Laplace approximation can be inaccurate. We propose an improved Laplace approximation which increases asymptotically the accuracy of the standard Laplace formula by one order of magnitude, and which is also accurate in high-dimensions. Applications to Bayesian inference in nonlinear regression models and to frequentist inference in a generalized linear mixed model with crossed random effects demonstrate the superiority of the proposed method with respect to the standard Laplace formula. The accuracy of the proposed method is comparable with that of other existing methods, which are computationally more demanding.},
	urldate = {2016-09-13},
	journal = {arXiv:1502.06440 [stat]},
	author = {Ruli, Erlis and Sartori, Nicola and Ventura, Laura},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.06440},
	keywords = {Statistics - Computation, Statistics - Methodology},
	file = {arXiv\:1502.06440 PDF:/Users/magnusmunch/Zotero/storage/UK7JU6JM/Ruli et al. - 2015 - Improved Laplace Approximation for Marginal Likeli.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/4MAZVF45/1502.html:text/html;ImprovedLaplaceApproximationMarginalLikelihoods.pdf:C\:\\Users\\Magnus\\Documents\\phd\\papers\\ImprovedLaplaceApproximationMarginalLikelihoods.pdf:application/pdf}
}

@article{zou_adaptive_2009,
	title = {On the adaptive elastic-net with a diverging number of parameters},
	volume = {37},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1245332831},
	doi = {10.1214/08-AOS625},
	language = {en},
	number = {4},
	urldate = {2015-09-14},
	journal = {The Annals of Statistics},
	author = {Zou, Hui and Zhang, Hao Helen},
	month = aug,
	year = {2009},
	pages = {1733--1751},
	file = {AdaptiveElasticNetDivergingNumberParameters.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\AdaptiveElasticNetDivergingNumberParameters.pdf:application/pdf;AdaptiveElasticNetDivergingNumberParameters.pdf:/Users/magnusmunch/Zotero/storage/WU4JGNGW/AdaptiveElasticNetDivergingNumberParameters.pdf:application/pdf;AdaptiveElasticNetDivergingNumberParameters.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\AdaptiveElasticNetDivergingNumberParameters.pdf:application/pdf}
}

@article{neuenschwander_summarizing_2010,
	title = {Summarizing historical information on controls in clinical trials},
	volume = {7},
	issn = {1740-7745},
	url = {http://ctj.sagepub.com/cgi/doi/10.1177/1740774509356002},
	doi = {10.1177/1740774509356002},
	language = {en},
	number = {5},
	urldate = {2016-09-13},
	journal = {Clinical Trials},
	author = {Neuenschwander, B. and Capkun-Niggli, G. and Branson, M. and Spiegelhalter, D. J.},
	month = jan,
	year = {2010},
	pages = {5--18},
	file = {SummarizingHistoricalInformationControlsClinicalTrials.pdf:/Users/magnusmunch/Zotero/storage/PZDAHZPA/SummarizingHistoricalInformationControlsClinicalTrials.pdf:application/pdf}
}

@article{yi_penalized_2015,
	title = {Penalized {Multimarker} vs. {Single}-{Marker} {Regression} {Methods} for {Genome}-{Wide} {Association} {Studies} of {Quantitative} {Traits}},
	volume = {199},
	issn = {0016-6731},
	url = {http://www.genetics.org/cgi/doi/10.1534/genetics.114.167817},
	doi = {10.1534/genetics.114.167817},
	language = {en},
	number = {1},
	urldate = {2015-09-14},
	journal = {Genetics},
	author = {Yi, H. and Breheny, P. and Imam, N. and Liu, Y. and Hoeschele, I.},
	month = jan,
	year = {2015},
	pages = {205--222},
	file = {PenalizedMultimarkerVsSingleMarkerGenomeWideAssociation.pdf:/Users/magnusmunch/Zotero/storage/9ZAF4RRH/PenalizedMultimarkerVsSingleMarkerGenomeWideAssociation.pdf:application/pdf;PenalizedMultimarkerVsSingleMarkerGenomeWideAssociation.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\PenalizedMultimarkerVsSingleMarkerGenomeWideAssociation.pdf:application/pdf}
}

@article{zhang_adaptive_2007,
	title = {Adaptive {Lasso} for {Cox}'s proportional hazards model},
	volume = {94},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asm037},
	doi = {10.1093/biomet/asm037},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Biometrika},
	author = {Zhang, H. H. and Lu, W.},
	month = aug,
	year = {2007},
	pages = {691--703},
	file = {AdaptiveLassoCoxProportionalHazardsModel.pdf:/Users/magnusmunch/Zotero/storage/ABQT8GA5/AdaptiveLassoCoxProportionalHazardsModel.pdf:application/pdf}
}

@article{shun_laplace_1995,
	title = {Laplace {Approximation} of {High} {Dimensional} {Integrals}},
	volume = {57},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2345941},
	abstract = {It is shown that the usual Laplace approximation is not a valid asymptotic approximation when the dimension of the integral is comparable with the limiting parameter n. The formal Laplace expansion for multidimensional integrals is given and used to construct asymptotic approximations for high dimensional integrals. One example is considered in which the dimension of the integral is O(n$^{\textrm{1/2}}$) and the relative error of the unmodified Laplace approximation is O(1). Nevertheless, it is possible to construct a valid asymptotic expansion by regrouping terms in the formal expansion according to asymptotic order in n.},
	number = {4},
	urldate = {2017-03-17},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Shun, Zhenming and McCullagh, Peter},
	year = {1995},
	pages = {749--760},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/KAI3QZ9V/Shun and McCullagh - 1995 - Laplace Approximation of High Dimensional Integral.pdf:application/pdf}
}

@article{casella_illustrating_1992,
	title = {Illustrating empirical {Bayes} methods},
	volume = {16},
	issn = {0169-7439},
	url = {http://www.sciencedirect.com/science/article/pii/016974399280050E},
	doi = {10.1016/0169-7439(92)80050-E},
	abstract = {Empirical Bayes methods have found increasing use in statistical analyses. These methods allow the modelling of complicated systems and provide a mechanism for obtaining parameter estimates. They are based on Bayesian models, but employ alternate estimation techniques. In this article these methods are explained and illustrated with many examples taken from real situations.},
	number = {2},
	urldate = {2016-12-09},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Casella, George},
	month = oct,
	year = {1992},
	keywords = {16: 107–125, 1992. Illustrating empirical Bayes methods., Casella, G.},
	pages = {107--125},
	file = {1-s2.0-016974399280050E-main.pdf:/Users/magnusmunch/Zotero/storage/CI9G35HG/1-s2.0-016974399280050E-main.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/98KFNMZP/016974399280050E.html:text/html}
}

@inproceedings{duvenaud_early_2016,
	title = {Early {Stopping} as {Nonparametric} {Variational} {Inference}},
	url = {http://www.jmlr.org/proceedings/papers/v51/duvenaud16.pdf},
	urldate = {2017-01-10},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Duvenaud, David and Maclaurin, Dougal and Adams, Ryan P.},
	year = {2016},
	pages = {1070--1077},
	file = {duvenaud16.pdf:/Users/magnusmunch/Zotero/storage/QFZF6FFN/duvenaud16.pdf:application/pdf}
}

@article{ormerod_grid_2011,
	title = {Grid based variational approximations},
	volume = {55},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947310001829},
	doi = {10.1016/j.csda.2010.04.024},
	abstract = {Variational methods for approximate Bayesian inference provide fast, flexible, deterministic alternatives to Monte Carlo methods. Unfortunately, unlike Monte Carlo methods, variational approximations cannot, in general, be made to be arbitrarily accurate. This paper develops grid-based variational approximations which endeavor to approximate marginal posterior densities in a spirit similar to the Integrated Nested Laplace Approximation (INLA) of Rue et al. (2009) but which may be applied in situations where INLA cannot be used. The method can greatly increase the accuracy of a base variational approximation, although not in general to arbitrary accuracy. The methodology developed is at least reasonably accurate on all of the examples considered in the paper.},
	number = {1},
	urldate = {2017-03-03},
	journal = {Computational Statistics \& Data Analysis},
	author = {Ormerod, John T.},
	month = jan,
	year = {2011},
	keywords = {Bayesian inference, Kullback–Liebler divergence, Markov chain Monte Carlo, Variational approximation},
	pages = {45--56},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/86UJRIH3/Ormerod - 2011 - Grid based variational approximations.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/ANWV8GZ2/S0167947310001829.html:text/html}
}

@article{cule_significance_2011,
	title = {Significance testing in ridge regression for genetic data},
	volume = {12},
	issn = {1471-2105},
	url = {http://dx.doi.org/10.1186/1471-2105-12-372},
	doi = {10.1186/1471-2105-12-372},
	abstract = {Technological developments have increased the feasibility of large scale genetic association studies. Densely typed genetic markers are obtained using SNP arrays, next-generation sequencing technologies and imputation. However, SNPs typed using these methods can be highly correlated due to linkage disequilibrium among them, and standard multiple regression techniques fail with these data sets due to their high dimensionality and correlation structure. There has been increasing interest in using penalised regression in the analysis of high dimensional data. Ridge regression is one such penalised regression technique which does not perform variable selection, instead estimating a regression coefficient for each predictor variable. It is therefore desirable to obtain an estimate of the significance of each ridge regression coefficient.},
	urldate = {2017-01-23},
	journal = {BMC Bioinformatics},
	author = {Cule, Erika and Vineis, Paolo and De Iorio, Maria},
	year = {2011},
	pages = {372},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/2MRAVCIQ/Cule et al. - 2011 - Significance testing in ridge regression for genet.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/J6M8ZW9V/1471-2105-12-372.html:text/html}
}

@article{silverman_needles_2004,
	title = {Needles and straw in haystacks: {Empirical} {Bayes} estimates of possibly sparse sequences},
	volume = {32},
	issn = {0090-5364},
	shorttitle = {Needles and straw in haystacks},
	url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1091626180/},
	doi = {10.1214/009053604000000030},
	language = {en},
	number = {4},
	urldate = {2017-03-01},
	journal = {The Annals of Statistics},
	author = {Silverman, Bernard W. and Johnstone, Iain M.},
	month = aug,
	year = {2004},
	pages = {1594--1649},
	file = {euclid.aos.1091626180.pdf:/Users/magnusmunch/Zotero/storage/SEMGX27V/euclid.aos.1091626180.pdf:application/pdf}
}

@article{zhou_quasi-newton_2011,
	title = {A quasi-{Newton} acceleration for high-dimensional optimization algorithms},
	volume = {21},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/article/10.1007/s11222-009-9166-3},
	doi = {10.1007/s11222-009-9166-3},
	abstract = {In many statistical problems, maximum likelihood estimation by an EM or MM algorithm suffers from excruciatingly slow convergence. This tendency limits the application of these algorithms to modern high-dimensional problems in data mining, genomics, and imaging. Unfortunately, most existing acceleration techniques are ill-suited to complicated models involving large numbers of parameters. The squared iterative methods (SQUAREM) recently proposed by Varadhan and Roland constitute one notable exception. This paper presents a new quasi-Newton acceleration scheme that requires only modest increments in computation per iteration and overall storage and rivals or surpasses the performance of SQUAREM on several representative test problems.},
	language = {en},
	number = {2},
	urldate = {2017-02-07},
	journal = {Statistics and Computing},
	author = {Zhou, Hua and Alexander, David and Lange, Kenneth},
	month = apr,
	year = {2011},
	pages = {261--273},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/VX72I8T9/Zhou et al. - 2011 - A quasi-Newton acceleration for high-dimensional o.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/ZBAM5P92/s11222-009-9166-3.html:text/html}
}

@article{simon_sparse-group_2013,
	title = {A {Sparse}-{Group} {Lasso}},
	volume = {22},
	issn = {1061-8600},
	url = {http://dx.doi.org/10.1080/10618600.2012.681250},
	doi = {10.1080/10618600.2012.681250},
	abstract = {For high-dimensional supervised learning problems, often using problem-specific assumptions can lead to greater accuracy. For problems with grouped covariates, which are believed to have sparse effects both on a group and within group level, we introduce a regularized model for linear regression with ℓ1 and ℓ2 penalties. We discuss the sparsity and other regularization properties of the optimal fit for this model, and show that it has the desired effect of group-wise and within group sparsity. We propose an algorithm to fit the model via accelerated generalized gradient descent, and extend this model and algorithm to convex loss functions. We also demonstrate the efficacy of our model and the efficiency of our algorithm on simulated data. This article has online supplementary material.},
	number = {2},
	urldate = {2016-11-29},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = apr,
	year = {2013},
	keywords = {Model, Nesterov, Penalize, Regression, Regularize},
	pages = {231--245},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/BJS8SUZI/Simon et al. - 2013 - A Sparse-Group Lasso.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/3FNIU6JK/10618600.2012.html:text/html}
}

@article{atchade_computational_2011,
	title = {A computational framework for empirical {Bayes} inference},
	volume = {21},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/article/10.1007/s11222-010-9182-3},
	doi = {10.1007/s11222-010-9182-3},
	abstract = {In empirical Bayes inference one is typically interested in sampling from the posterior distribution of a parameter with a hyper-parameter set to its maximum likelihood estimate. This is often problematic particularly when the likelihood function of the hyper-parameter is not available in closed form and the posterior distribution is intractable. Previous works have dealt with this problem using a multi-step approach based on the EM algorithm and Markov Chain Monte Carlo (MCMC). We propose a framework based on recent developments in adaptive MCMC, where this problem is addressed more efficiently using a single Monte Carlo run. We discuss the convergence of the algorithm and its connection with the EM algorithm. We apply our algorithm to the Bayesian Lasso of Park and Casella (J. Am. Stat. Assoc. 103:681–686, 2008) and on the empirical Bayes variable selection of George and Foster (J. Am. Stat. Assoc. 87:731–747, 2000).},
	language = {en},
	number = {4},
	urldate = {2017-02-16},
	journal = {Statistics and Computing},
	author = {Atchadé, Yves F.},
	month = oct,
	year = {2011},
	pages = {463--473},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/NBJKDVNI/Atchadé - 2011 - A computational framework for empirical Bayes infe.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/EBK77VCP/s11222-010-9182-3.html:text/html}
}

@article{rattray_inference_2009,
	title = {Inference algorithms and learning theory for {Bayesian} sparse factor analysis},
	volume = {197},
	issn = {1742-6596},
	url = {http://stacks.iop.org/1742-6596/197/i=1/a=012002},
	doi = {10.1088/1742-6596/197/1/012002},
	abstract = {Bayesian sparse factor analysis has many applications; for example, it has been applied to the problem of inferring a sparse regulatory network from gene expression data. We describe a number of inference algorithms for Bayesian sparse factor analysis using a slab and spike mixture prior. These include well-established Markov chain Monte Carlo (MCMC) and variational Bayes (VB) algorithms as well as a novel hybrid of VB and Expectation Propagation (EP). For the case of a single latent factor we derive a theory for learning performance using the replica method. We compare the MCMC and VB/EP algorithm results with simulated data to the theoretical prediction. The results for MCMC agree closely with the theory as expected. Results for VB/EP are slightly sub-optimal but show that the new algorithm is effective for sparse inference. In large-scale problems MCMC is infeasible due to computational limitations and the VB/EP algorithm then provides a very useful computationally efficient alternative.},
	language = {en},
	number = {1},
	urldate = {2017-03-07},
	journal = {Journal of Physics: Conference Series},
	author = {Rattray, Magnus and Stegle, Oliver and Sharp, Kevin and Winn, John},
	year = {2009},
	pages = {012002},
	file = {IOP Full Text PDF:/Users/magnusmunch/Zotero/storage/W8R66NBD/Rattray et al. - 2009 - Inference algorithms and learning theory for Bayes.pdf:application/pdf}
}

@misc{noauthor_rcpp:_nodate,
	title = {Rcpp: {Seamless} {R} and {C}++ {Integration} {\textbar} {Eddelbuettel} {\textbar} {Journal} of {Statistical} {Software}},
	shorttitle = {Rcpp},
	url = {https://www.jstatsoft.org/article/view/v040i08},
	urldate = {2017-03-17},
	file = {Snapshot:/Users/magnusmunch/Zotero/storage/SGEHX2T2/v040i08.html:text/html}
}

@article{mitchell_note_1994,
	title = {A {Note} on {Posterior} {Moments} for a {Normal} {Mean} with {Double}-{Exponential} {Prior}},
	volume = {56},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2346185},
	abstract = {General results on the behaviour of the posterior mean and variance of the mean of a normal distribution with known variance using a double-exponential prior are derived and complement the conjectures and particular examples given by Pericchi and Smith (1992) and Pericchi and Walley (1991).},
	number = {4},
	urldate = {2016-12-14},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Mitchell, Ann F. S.},
	year = {1994},
	pages = {605--610},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/U88PBGNB/Mitchell - 1994 - A Note on Posterior Moments for a Normal Mean with.pdf:application/pdf}
}

@article{jiang_gibbs_2008,
	title = {Gibbs {Posterior} for {Variable} {Selection} in {High}-{Dimensional} {Classification} and {Data} {Mining}},
	volume = {36},
	issn = {0090-5364},
	url = {http://www.jstor.org/stable/25464709},
	abstract = {In the popular approach of "Bayesian variable selection" (BVS), one uses prior and posterior distributions to select a subset of candidate variables to enter the model. A completely new direction will be considered here to study BVS with a Gibbs posterior originating in statistical mechanics. The Gibbs posterior is constructed from a risk function of practical interest (such as the classification error) and aims at minimizing a risk function without modeling the data probabilistically. This can improve the performance over the usual Bayesian approach, which depends on a probability model which may be misspecified. Conditions will be provided to achieve good risk performance, even in the presence of high dimensionality, when the number of candidate variables "K" can be much larger than the sample size "n." In addition, we develop a convenient Markov chain Monte Carlo algorithm to implement BVS with the Gibbs posterior.},
	number = {5},
	urldate = {2017-01-24},
	journal = {The Annals of Statistics},
	author = {Jiang, Wenxin and Tanner, Martin A.},
	year = {2008},
	pages = {2207--2231},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/MR3GRPPN/Jiang and Tanner - 2008 - Gibbs Posterior for Variable Selection in High-Dim.pdf:application/pdf}
}

@article{mackay_probable_1995,
	title = {Probable networks and plausible predictions — a review of practical {Bayesian} methods for supervised neural networks},
	volume = {6},
	issn = {0954-898X},
	url = {http://www.tandfonline.com/doi/abs/10.1088/0954-898X_6_3_011},
	doi = {10.1088/0954-898X_6_3_011},
	abstract = {Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well-matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. This review describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks.},
	number = {3},
	urldate = {2017-02-01},
	journal = {Network: Computation in Neural Systems},
	author = {Mackay, David J. C.},
	month = jan,
	year = {1995},
	pages = {469--505},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/4P9RQHKC/Mackay - 1995 - Probable networks and plausible predictions — a re.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/K3RMGB99/0954-898X_6_3_011.html:text/html}
}

@phdthesis{de_heide_safe-bayesian_nodate,
	address = {Leiden, The Netherlands},
	title = {The safe-bayesian lasso},
	url = {https://www.math.leidenuniv.nl/scripties/DeHeideMaster.pdf},
	urldate = {2017-02-17},
	school = {Leiden University},
	author = {de Heide, Rianne},
	file = {DeHeideMaster.pdf:/Users/magnusmunch/Zotero/storage/J4B4MUQD/DeHeideMaster.pdf:application/pdf}
}

@book{bishop_pattern_2006,
	address = {New York},
	title = {Pattern {Recognition} and {Machine} {Learning}},
	url = {http://www.springer.com/gp/book/9780387310732},
	abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the...},
	urldate = {2017-03-14},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	file = {Snapshot:/Users/magnusmunch/Zotero/storage/E9DIXD44/9780387310732.html:text/html}
}

@article{rue_approximate_2009,
	title = {Approximate {Bayesian} inference for latent {Gaussian} models by using integrated nested {Laplace} approximations},
	volume = {71},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2008.00700.x/abstract},
	doi = {10.1111/j.1467-9868.2008.00700.x},
	abstract = {Summary.  Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
	language = {en},
	number = {2},
	urldate = {2017-03-14},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Rue, Håvard and Martino, Sara and Chopin, Nicolas},
	month = apr,
	year = {2009},
	keywords = {Approximate Bayesian inference, Gaussian Markov random fields, Generalized additive mixed models, Laplace approximation, Parallel computing, Sparse matrices, Structured additive regression models},
	pages = {319--392},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/G68767ZE/Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/RDU9UF84/abstract.html:text/html}
}

@article{carvalho_horseshoe_2010,
	title = {The horseshoe estimator for sparse signals},
	volume = {97},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/content/97/2/465},
	doi = {10.1093/biomet/asq017},
	abstract = {This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator’s advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator’s tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.},
	language = {en},
	number = {2},
	urldate = {2016-12-14},
	journal = {Biometrika},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	month = jun,
	year = {2010},
	keywords = {Shrinkage, Sparsity, Normal scale mixture, Ridge regression, Robustness, Thresholding},
	pages = {465--480},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/2CXKDVXU/Carvalho et al. - 2010 - The horseshoe estimator for sparse signals.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/3F59BKRW/465.html:text/html}
}

@article{ormerod_explaining_2010,
	title = {Explaining {Variational} {Approximations}},
	volume = {64},
	issn = {0003-1305},
	url = {http://www.jstor.org/stable/20799885},
	abstract = {Variational approximations facilitate approximate inference for the parameters in complex statistical models and provide fast, deterministic alternatives to Monte Carlo methods. However, much of the contemporary literature on variational approximations is in Computer Science rather than Statistics, and uses terminology, notation, and examples from the former field. In this article we explain variational approximation in statistical terms. In particular, we illustrate the ideas of variational approximation using examples that are familiar to statisticians.},
	number = {2},
	urldate = {2017-03-03},
	journal = {The American Statistician},
	author = {Ormerod, J. T. and Wand, M. P.},
	year = {2010},
	pages = {140--153},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/BDECR8RI/Ormerod and Wand - 2010 - Explaining Variational Approximations.pdf:application/pdf}
}

@article{novianti_better_2017,
	title = {Better diagnostic signatures from {RNAseq} data through use of auxiliary co-data},
	volume = {33},
	url = {https://academic.oup.com/bioinformatics/article/doi/10.1093/bioinformatics/btw837/2884246/Better-diagnostic-signatures-from-RNAseq-data},
	doi = {10.1093/bioinformatics/btw837},
	number = {10},
	urldate = {2017-03-22},
	journal = {Bioinformatics},
	author = {Novianti, Putri W. and Snoek, Barbara C. and Wilting, Saskia M. and van de Wiel, Mark A.},
	year = {2017},
	pages = {1572--1574},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/G6NX2KVW/Novianti et al. - Better diagnostic signatures from RNAseq data thro.pdf:application/pdf;Novianti_etal_GRridgeApplNote_SuppMat.pdf:/Users/magnusmunch/Zotero/storage/PGZCG5XU/Novianti_etal_GRridgeApplNote_SuppMat.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/GRCRNEMS/btw837.html:text/html}
}

@article{lange_quasi-newton_1995,
	title = {A quasi-{Newton} acceleration of the {EM} algorithm},
	volume = {5},
	url = {http://www3.stat.sinica.edu.tw/statistica/oldpdf/A5n11.pdf},
	number = {1},
	urldate = {2017-02-07},
	journal = {Statistica Sinica},
	author = {Lange, Kenneth},
	year = {1995},
	pages = {1--18},
	file = {A5n11.pdf:/Users/magnusmunch/Zotero/storage/NX6VV6R2/A5n11.pdf:application/pdf}
}

@inproceedings{wang_inadequacy_2005,
	title = {Inadequacy of interval estimates corresponding to variational {Bayesian} approximations.},
	url = {http://core.ac.uk/download/pdf/22017.pdf#page=382},
	urldate = {2017-03-07},
	booktitle = {{AISTATS}},
	author = {Wang, Bo and Titterington, D. M.},
	year = {2005},
	pages = {373--380},
	file = {146.pdf:/Users/magnusmunch/Zotero/storage/Q2IBHA3Q/146.pdf:application/pdf}
}

@article{jamshidian_acceleration_1997,
	title = {Acceleration of the {EM} {Algorithm} by {Using} {Quasi}-{Newton} {Methods}},
	volume = {59},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2346010},
	abstract = {The EM algorithm is a popular method for maximum likelihood estimation. Its simplicity in many applications and desirable convergence properties make it very attractive. Its sometimes slow convergence, however, has prompted researchers to propose methods to accelerate it. We review these methods, classifying them into three groups: pure, hybrid and EM-type accelerators. We propose a new pure and a new hybrid accelerator both based on quasi-Newton methods and numerically compare these and two other quasi-Newton accelerators. For this we use examples in each of three areas: Poisson mixtures, the estimation of covariance from incomplete data and multivariate normal mixtures. In these comparisons, the new hybrid accelerator was fastest on most of the examples and often dramatically so. In some cases it accelerated the EM algorithm by factors of over 100. The new pure accelerator is very simple to implement and competed well with the other accelerators. It accelerated the EM algorithm in some cases by factors of over 50. To obtain standard errors, we propose to approximate the inverse of the observed information matrix by using auxiliary output from the new hybrid accelerator. A numerical evaluation of these approximations indicates that they may be useful at least for exploratory purposes.},
	number = {3},
	urldate = {2017-02-07},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Jamshidian, Mortaza and Jennrich, Robert I.},
	year = {1997},
	pages = {569--587},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/V6EXWG9T/Jamshidian and Jennrich - 1997 - Acceleration of the EM Algorithm by Using Quasi-Ne.pdf:application/pdf}
}

@techreport{titsias_variational_2009,
	address = {UK},
	title = {Variational {Model} {Selection} for {Sparse} {Gaussian} {Process} {Regression}},
	url = {https://pdfs.semanticscholar.org/db7b/e492a629a98db7f9d77d552fd3568ff42189.pdf?_ga=1.199424750.1939693154.1481208822},
	urldate = {2016-12-08},
	institution = {School of Computer Science, University of Manchester},
	author = {Titsias, Michalis K.},
	year = {2009},
	file = {e492a629a98db7f9d77d552fd3568ff42189.pdf:/Users/magnusmunch/Zotero/storage/6ZUFW88T/e492a629a98db7f9d77d552fd3568ff42189.pdf:application/pdf}
}

@inproceedings{carvalho_handling_2009,
	title = {Handling {Sparsity} via the {Horseshoe}.},
	volume = {5},
	url = {http://www.jmlr.org/proceedings/papers/v5/carvalho09a/carvalho09a.pdf},
	urldate = {2016-12-14},
	booktitle = {{AISTATS}},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	year = {2009},
	pages = {73--80},
	file = {carvalho09a.pdf:/Users/magnusmunch/Zotero/storage/ZVMGX2QT/carvalho09a.pdf:application/pdf}
}

@article{consonni_mean-field_2007,
	title = {Mean-field variational approximate {Bayesian} inference for latent variable models},
	volume = {52},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947306003951},
	doi = {10.1016/j.csda.2006.10.028},
	abstract = {The ill-posed nature of missing variable models offers a challenging testing ground for new computational techniques. This is the case for the mean-field variational Bayesian inference. The behavior of this approach in the setting of the Bayesian probit model is illustrated. It is shown that the mean-field variational method always underestimates the posterior variance and, that, for small sample sizes, the mean-field variational approximation to the posterior location could be poor.},
	number = {2},
	urldate = {2017-03-07},
	journal = {Computational Statistics \& Data Analysis},
	author = {Consonni, Guido and Marin, Jean-Michel},
	month = oct,
	year = {2007},
	keywords = {Bayesian inference, Bayesian probit model, Gibbs sampling, Latent variable models, Marginal distribution, Mean-field variational methods},
	pages = {790--798},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/9RRQZZAM/Consonni and Marin - 2007 - Mean-field variational approximate Bayesian infere.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/95F884TQ/S0167947306003951.html:text/html}
}

@article{wang_convergence_2006,
	title = {Convergence properties of a general algorithm for calculating variational {Bayesian} estimates for a normal mixture model},
	volume = {1},
	issn = {1936-0975, 1931-6690},
	url = {http://projecteuclid.org/euclid.ba/1340371055},
	doi = {10.1214/06-BA121},
	abstract = {In this paper we propose a generalised iterative algorithm for calculating variational Bayesian estimates for a normal mixture model and investigate its convergence properties. It is shown theoretically that the variational Bayesian estimator converges locally to the maximum likelihood estimator at the rate of O(1/n)O(1/n)O(1/\{n\}) in the large sample limit.},
	language = {EN},
	number = {3},
	urldate = {2017-03-07},
	journal = {Bayesian Analysis},
	author = {Wang, Bo and Titterington, D. M.},
	month = sep,
	year = {2006},
	mrnumber = {MR2221291},
	zmnumber = {1331.62168},
	keywords = {Laplace approximation, Local convergence, Mixture model, Variational Bayes},
	pages = {625--650},
	file = {euclid.ba.1340371055.pdf:/Users/magnusmunch/Zotero/storage/A58KEXNH/euclid.ba.1340371055.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/2K2WJ3BK/1340371055.html:text/html}
}

@article{alquier_properties_2015,
	title = {On the properties of variational approximations of {Gibbs} posteriors},
	url = {http://arxiv.org/abs/1506.04091},
	abstract = {The PAC-Bayesian approach is a powerful set of techniques to derive non- asymptotic risk bounds for random estimators. The corresponding optimal distribution of estimators, usually called the Gibbs posterior, is unfortunately intractable. One may sample from it using Markov chain Monte Carlo, but this is often too slow for big datasets. We consider instead variational approximations of the Gibbs posterior, which are fast to compute. We undertake a general study of the properties of such approximations. Our main finding is that such a variational approximation has often the same rate of convergence as the original PAC-Bayesian procedure it approximates. We specialise our results to several learning tasks (classification, ranking, matrix completion),discuss how to implement a variational approximation in each case, and illustrate the good properties of said approximation on real datasets.},
	urldate = {2017-01-18},
	journal = {arXiv:1506.04091 [math, stat]},
	author = {Alquier, Pierre and Ridgway, James and Chopin, Nicolas},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04091},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {arXiv\:1506.04091 PDF:/Users/magnusmunch/Zotero/storage/ZN5WCDHZ/Alquier et al. - 2015 - On the properties of variational approximations of.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/V2FIMV5Q/1506.html:text/html}
}

@book{rasmussen_gaussian_2008,
	address = {Cambridge, Mass.},
	edition = {3. print},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	language = {und},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2008},
	note = {OCLC: 552376743},
	keywords = {Gaussian processes Data processing, Gauß-Prozess, Machine learning Mathematical models, Maschinelles Lernen},
	file = {RW.pdf:/Users/magnusmunch/Zotero/storage/MV6A256Z/RW.pdf:application/pdf}
}

@article{le_cessie_ridge_1992,
	title = {Ridge {Estimators} in {Logistic} {Regression}},
	volume = {41},
	issn = {0035-9254},
	url = {http://www.jstor.org/stable/2347628},
	doi = {10.2307/2347628},
	abstract = {In this paper it is shown how ridge estimators can be used in logistic regression to improve the parameter estimates and to diminish the error made by further predictions. Different ways to choose the unknown ridge parameter are discussed. The main attention focuses on ridge parameters obtained by cross-validation. Three different ways to define the prediction error are considered: classification error, squared error and minus log-likelihood. The use of ridge regression is illustrated by developing a prognostic index for the two-year survival probability of patients with ovarian cancer as a function of their deoxyribonucleic acid (DNA) histogram. In this example, the number of covariates is large compared with the number of observations and modelling without restrictions on the parameters leads to overfitting. Defining a restriction on the parameters, such that neighbouring intervals in the DNA histogram differ only slightly in their influence on the survival, yields ridge-type parameter estimates with reasonable values which can be clinically interpreted. Furthermore the model can predict new observations more accurately.},
	number = {1},
	urldate = {2017-01-23},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Le Cessie, S. and Van Houwelingen, J. C.},
	year = {1992},
	pages = {191--201},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/GAICPDDM/Le Cessie and Van Houwelingen - 1992 - Ridge Estimators in Logistic Regression.pdf:application/pdf}
}

@article{pericchi_exact_1992,
	title = {Exact and {Approximate} {Posterior} {Moments} for a {Normal} {Location} {Parameter}},
	volume = {54},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2345859},
	abstract = {The forms of first and second posterior moments for a normal location parameter are identified for a rather general class of prior distributions. Exact and approximate illustrations are given where the prior distribution is double exponential or Student t respectively.},
	number = {3},
	urldate = {2016-12-14},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Pericchi, L. R. and Smith, A. F. M.},
	year = {1992},
	pages = {793--804},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/VJ3G7BE2/Pericchi and Smith - 1992 - Exact and Approximate Posterior Moments for a Norm.pdf:application/pdf}
}

@incollection{kaji_optimal_2009,
	title = {Optimal {Hyperparameters} for {Generalized} {Learning} and {Knowledge} {Discovery} in {Variational} {Bayes}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-10677-4_54},
	abstract = {Variational Bayes learning is widely used in statistical models that contain hidden variables, for example, normal mixtures, binomial mixtures, and hidden Markov models. To derive the variational Bayes learning algorithm, we need to determine the hyperparameters in the a priori distribution. In the present paper, we propose two different methods by which to optimize the hyperparameters for the two different purposes. In the first method, the hyperparameter is determined for minimization of the generalization error. In the second method, the hyperparameter is chosen so that the unknown hidden structure in the data can be discovered. Experiments are conducted to show that the optimal hyperparameters are different for the generalized learning and knowledge discovery.},
	language = {en},
	urldate = {2016-12-09},
	booktitle = {Neural {Information} {Processing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kaji, Daisuke and Watanabe, Sumio},
	month = dec,
	year = {2009},
	doi = {10.1007/978-3-642-10677-4_54},
	pages = {476--483},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/PDZI7BZZ/Kaji and Watanabe - 2009 - Optimal Hyperparameters for Generalized Learning a.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/UZNAHXUX/10.html:text/html}
}

@article{eddelbuettel_rcpp:_2011,
	title = {Rcpp: {Seamless} {R} and {C}++ integration},
	volume = {40},
	shorttitle = {Rcpp},
	url = {http://r.adu.org.za/web/packages/Rcpp/vignettes/Rcpp-introduction.pdf},
	number = {8},
	urldate = {2017-03-17},
	journal = {Journal of Statistical Software},
	author = {Eddelbuettel, Dirk and François, Romain and Allaire, J. and Chambers, John and Bates, Douglas and Ushey, Kevin},
	year = {2011},
	pages = {1--18},
	file = {v40i08.pdf:/Users/magnusmunch/Zotero/storage/GE5VCFXN/v40i08.pdf:application/pdf}
}

@article{andrews_scale_1974,
	title = {Scale {Mixtures} of {Normal} {Distributions}},
	volume = {36},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2984774},
	abstract = {This paper presents necessary and sufficient conditions under which a random variable X may be generated as the ratio Z/V where Z and V are independent and Z has a standard normal distribution. This representation is useful in Monte Carlo calculations. It is established that when 1/2V$^{\textrm{2}}$ is exponential, X is double exponential; and that when 1/2V has the asymptotic distribution of the Kolmogorov distance statistic, X is logistic.},
	number = {1},
	urldate = {2017-01-06},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Andrews, D. F. and Mallows, C. L.},
	year = {1974},
	pages = {99--102},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/SMG3B4I9/Andrews and Mallows - 1974 - Scale Mixtures of Normal Distributions.pdf:application/pdf}
}

@inproceedings{titsias_variational_2009-1,
	title = {Variational {Learning} of {Inducing} {Variables} in {Sparse} {Gaussian} {Processes}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/AISTATS09_Titsias},
	urldate = {2016-12-05},
	author = {Titsias, Michalis K.},
	year = {2009},
	pages = {567--574},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/5UZEVNCA/Titsias - 2009 - Variational Learning of Inducing Variables in Spar.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/6QCC3A7G/AISTATS09_Titsias.html:text/html}
}

@article{bartlett_convexity_2006,
	title = {Convexity, {Classification}, and {Risk} {Bounds}},
	volume = {101},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/30047445},
	abstract = {Many of the classification algorithms developed in the machine learning literature, including the support vector machine and boosting, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. The convexity makes these algorithms computationally efficient. The use of a surrogate, however, has statistical consequences that must be balanced against the computational virtues of convexity. To study these issues, we provide a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function-that it satisfies a pointwise form of Fisher consistency for classification. The relationship is based on a simple variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise, and show that in this case, strictly convex loss functions lead to faster rates of convergence of the risk than would be implied by standard uniform convergence arguments. Finally, we present applications of our results to the estimation of convergence rates in function classes that are scaled convex hulls of a finite-dimensional base class, with a variety of commonly used loss functions.},
	number = {473},
	urldate = {2017-01-19},
	journal = {Journal of the American Statistical Association},
	author = {Bartlett, Peter L. and Jordan, Michael I. and Mcauliffe, Jon D.},
	year = {2006},
	pages = {138--156},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/DZ9MIRBI/Bartlett et al. - 2006 - Convexity, Classification, and Risk Bounds.pdf:application/pdf}
}

@article{johnstone_ebayesthresh:_2005,
	title = {{EbayesThresh}: {R} {Programs} for {Empirical} {Bayes} {Thresholding}},
	volume = {12},
	issn = {1548-7660},
	shorttitle = {{EbayesThresh}},
	url = {http://dx.doi.org/10.18637/jss.v012.i08},
	doi = {10.18637/jss.v012.i08},
	number = {8},
	urldate = {2017-03-01},
	journal = {Journal of Statistical Software},
	author = {Johnstone, Iain and Silverman, Bernard},
	year = {2005},
	keywords = {ebayes, wavelets},
	file = {v12i08.pdf:/Users/magnusmunch/Zotero/storage/6JXS74SB/v12i08.pdf:application/pdf}
}

@incollection{mackay_bayesian_1992,
	series = {Fundamental {Theories} of {Physics}},
	title = {Bayesian {Interpolation}},
	copyright = {©1992 Springer Science+Business Media Dordrecht},
	isbn = {978-90-481-4220-0 978-94-017-2219-3},
	url = {http://link.springer.com/chapter/10.1007/978-94-017-2219-3_3},
	abstract = {Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularisation and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modelling problems. Regularising constants are set by examining their posterior probability distribution. Alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. ‘Occam’s razor’ is automatically embodied by this process. The way in which Bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.},
	language = {en},
	number = {50},
	urldate = {2016-12-09},
	booktitle = {Maximum {Entropy} and {Bayesian} {Methods}},
	publisher = {Springer Netherlands},
	author = {MacKay, David J. C.},
	editor = {Smith, C. Ray and Erickson, Gary J. and Neudorfer, Paul O.},
	year = {1992},
	doi = {10.1007/978-94-017-2219-3_3},
	keywords = {Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences, Analytical Chemistry, Economic Geology, Image Processing and Computer Vision},
	pages = {39--66},
	file = {MACnc92a.pdf:/Users/magnusmunch/Zotero/storage/4VNIVCTH/MACnc92a.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/JQVBGWXQ/978-94-017-2219-3_3.html:text/html}
}

@article{titz_alterations_2016,
	title = {Alterations in {Serum} {Polyunsaturated} {Fatty} {Acids} and {Eicosanoids} in {Patients} with {Mild} to {Moderate} {Chronic} {Obstructive} {Pulmonary} {Disease} ({COPD})},
	volume = {17},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1422-0067/17/9/1583},
	doi = {10.3390/ijms17091583},
	abstract = {Smoking is a major risk factor for several diseases including chronic obstructive pulmonary disease (COPD). To better understand the systemic effects of cigarette smoke exposure and mild to moderate COPD—and to support future biomarker development—we profiled the serum lipidomes of healthy smokers, smokers with mild to moderate COPD (GOLD stages 1 and 2), former smokers, and never-smokers (n = 40 per group) (ClinicalTrials.gov registration: NCT01780298). Serum lipidome profiling was conducted with untargeted and targeted mass spectrometry-based lipidomics. Guided by weighted lipid co-expression network analysis, we identified three main trends comparing smokers, especially those with COPD, with non-smokers: a general increase in glycero(phospho)lipids, including triglycerols; changes in fatty acid desaturation (decrease in ω-3 polyunsaturated fatty acids, and an increase in monounsaturated fatty acids); and an imbalance in eicosanoids (increase in 11,12- and 14,15-DHETs (dihydroxyeicosatrienoic acids), and a decrease in 9- and 13-HODEs (hydroxyoctadecadienoic acids)). The lipidome profiles supported classification of study subjects as smokers or non-smokers, but were not sufficient to distinguish between smokers with and without COPD. Overall, our study yielded further insights into the complex interplay between smoke exposure, lung disease, and systemic alterations in serum lipid profiles.},
	language = {en},
	number = {9},
	urldate = {2017-04-11},
	journal = {International Journal of Molecular Sciences},
	author = {Titz, Bjoern and Luettich, Karsta and Leroy, Patrice and Boue, Stephanie and Vuillaume, Gregory and Vihervaara, Terhi and Ekroos, Kim and Martin, Florian and Peitsch, Manuel C. and Hoeng, Julia},
	month = sep,
	year = {2016},
	keywords = {lipidomics, tobacco smoke, clinical study, chronic obstructive pulmonary disease, biomarker},
	pages = {1583},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/RVC3533B/Titz et al. - 2016 - Alterations in Serum Polyunsaturated Fatty Acids a.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/KNI67U3H/1583.html:text/html}
}

@article{bhattacharya_fast_2015,
	title = {Fast sampling with {Gaussian} scale-mixture priors in high-dimensional regression},
	url = {http://arxiv.org/abs/1506.04778},
	abstract = {We propose an efficient way to sample from a class of structured multivariate Gaussian distributions which routinely arise as conditional posteriors of model parameters that are assigned a conditionally Gaussian prior. The proposed algorithm only requires matrix operations in the form of matrix multiplications and linear system solutions. We exhibit that the computational complexity of the proposed algorithm grows linearly with the dimension unlike existing algorithms relying on Cholesky factorizations with cubic orders of complexity. The algorithm should be broadly applicable in settings where Gaussian scale mixture priors are used on high dimensional model parameters. We provide an illustration through posterior sampling in a high dimensional regression setting with a horseshoe prior on the vector of regression coefficients.},
	urldate = {2017-05-03},
	journal = {arXiv:1506.04778 [stat]},
	author = {Bhattacharya, Anirban and Chakraborty, Antik and Mallick, Bani K.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04778},
	keywords = {Statistics - Computation},
	file = {arXiv\:1506.04778 PDF:/Users/magnusmunch/Zotero/storage/DBUJSJS9/Bhattacharya et al. - 2015 - Fast sampling with Gaussian scale-mixture priors i.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/95Z5T5XF/1506.html:text/html}
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1198/016214508000000337},
	doi = {10.1198/016214508000000337},
	abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
	number = {482},
	urldate = {2017-05-11},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	keywords = {Empirical Bayes, Gibbs sampler, Hierarchical model, Inverse Gaussian, Linear regression, Penalized regression, Scale mixture of normals},
	pages = {681--686},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/E5ZB633R/Park and Casella - 2008 - The Bayesian Lasso.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/ZH2FNSVS/016214508000000337.html:text/html}
}

@article{kording_ten_2017,
	title = {Ten simple rules for structuring papers},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {http://biorxiv.org/content/early/2017/05/23/088278},
	doi = {10.1101/088278},
	abstract = {Good scientific writing is essential to career development and to the progress of science. A well-structured manuscript allows readers and reviewers to get excited about the subject matter, to understand and verify the paper's contributions, and to integrate these contributions into a broader context. However, many scientists struggle with producing high-quality manuscripts and typically get little training in paper writing. Focusing on how readers consume information, we present a set of 10 simple rules to help you get across the main idea of your paper. These rules are designed to make your paper more influential and the process of writing more efficient and pleasurable.},
	language = {en},
	urldate = {2017-06-13},
	journal = {bioRxiv},
	author = {Kording, Konrad P. and Mensh, Brett},
	month = may,
	year = {2017},
	pages = {088278},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/AREUE64T/Kording and Mensh - 2017 - Ten simple rules for structuring papers.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/AKWNIW94/088278.html:text/html}
}

@article{nettleton_convergence_1999,
	title = {Convergence properties of the {EM} algorithm in constrained parameter spaces},
	volume = {27},
	issn = {1708-945X},
	url = {http://onlinelibrary.wiley.com/doi/10.2307/3316118/abstract},
	doi = {10.2307/3316118},
	abstract = {The established general results on convergence properties of the EM algorithm require the sequence of EM parameter estimates to fall in the interior of the parameter space over which the likelihood is being maximized. This paper presents convergence properties of the EM sequence of likelihood values and parameter estimates in constrained parameter spaces for which the sequence of EM parameter estimates may converge to the boundary of the constrained parameter space contained in the interior of the unconstrained parameter space. Examples of the behavior of the EM algorithm applied to such parameter spaces are presented.},
	language = {en},
	number = {3},
	urldate = {2017-06-23},
	journal = {Canadian Journal of Statistics},
	author = {Nettleton, Dan},
	month = sep,
	year = {1999},
	keywords = {EM algorithm, GEM algorithm, ECM algorithm, AECM algorithm, incomplete data, inequality constraints, order-restricted inference, restricted maximum-likelihood estimate},
	pages = {639--648},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/H4WD87W4/Nettleton - 1999 - Convergence properties of the EM algorithm in cons.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/483FXF3W/abstract.html:text/html}
}

@article{nettleton_interval_1998,
	title = {Interval {Mapping} of {Quantitative} {Trait} {Loci} through {Order}-{Restricted} {Inference}},
	volume = {54},
	issn = {0006-341X},
	url = {http://www.jstor.org/stable/2533997},
	doi = {10.2307/2533997},
	abstract = {An order-restricted version of the standard interval mapping procedure is introduced. The usual LOD scores (base 10 logs of likelihood ratios) used in interval mapping are replaced with LOD scores based on restricted likelihood ratio test statistics, which make use of known orderings in the genotype effects at the putative quantitative trait loci (QTL). Simulations demonstrate that individual tests based on the restricted LOD scores can be more powerful than tests based on the standard LOD scores. The new procedure appears to improve QTL detection capability and estimates of both position and genotype effects in certain circumstances. Techniques from order-restricted inference are combined with the EM algorithm to estimate genotype effects and compute the restricted LOD scores.},
	number = {1},
	urldate = {2017-06-23},
	journal = {Biometrics},
	author = {Nettleton, Dan and Praestgaard, Jens},
	year = {1998},
	pages = {74--87},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/M9TNEMRN/Nettleton and Praestgaard - 1998 - Interval Mapping of Quantitative Trait Loci throug.pdf:application/pdf}
}

@article{forman_extensive_2003,
	title = {An {Extensive} {Empirical} {Study} of {Feature} {Selection} {Metrics} for {Text} {Classification}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v3/forman03a.html},
	number = {Mar},
	urldate = {2017-07-18},
	journal = {Journal of Machine Learning Research},
	author = {Forman, George},
	year = {2003},
	pages = {1289--1305},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/ZW9XG48M/Forman - 2003 - An Extensive Empirical Study of Feature Selection .pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/JN4SGZ5D/forman03a.html:text/html}
}

@article{jaemo_sung_latent-space_2008,
	title = {Latent-{Space} {Variational} {Bayes}},
	volume = {30},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/4670325/},
	doi = {10.1109/TPAMI.2008.157},
	number = {12},
	urldate = {2017-07-18},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Jaemo Sung} and Ghahramani, Z. and {Sung-Yang Bang}},
	month = dec,
	year = {2008},
	pages = {2236--2242},
	file = {Snapshot:/Users/magnusmunch/Zotero/storage/KWIAB2J3/ttp2008122236-abs.html:text/html;ttp2008122236.pdf:/Users/magnusmunch/Zotero/storage/GSMKJX2T/ttp2008122236.pdf:application/pdf}
}

@article{hensman_fast_2012,
	title = {Fast {Variational} {Inference} in the {Conjugate} {Exponential} {Family}},
	url = {http://arxiv.org/abs/1206.5162},
	abstract = {We present a general method for deriving collapsed variational inference algo- rithms for probabilistic models in the conjugate exponential family. Our method unifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound.},
	urldate = {2017-07-18},
	journal = {arXiv:1206.5162 [cs, stat]},
	author = {Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.5162},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1206.5162 PDF:/Users/magnusmunch/Zotero/storage/4FZ44225/Hensman et al. - 2012 - Fast Variational Inference in the Conjugate Expone.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/HN8H45QQ/1206.html:text/html}
}

@article{kucukelbir_automatic_2015,
	title = {Automatic {Variational} {Inference} in {Stan}},
	url = {http://arxiv.org/abs/1506.03431},
	abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
	urldate = {2017-08-01},
	journal = {arXiv:1506.03431 [stat]},
	author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03431},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1506.03431 PDF:/Users/magnusmunch/Zotero/storage/SD28TT5X/Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/UUE6QG6U/1506.html:text/html}
}

@incollection{sill_monotonicity_1997,
	address = {Cambridge, MA},
	title = {Monotonicity {Hints}},
	isbn = {978-0-262-10065-6},
	url = {http://resolver.caltech.edu/CaltechAUTHORS:20160223-161511946},
	abstract = {A hint is any piece of side information about the target function to be learned. We consider the monotonicity hint, which states that the function to be learned is monotonic in some or all of the input variables. The application of monotonicity hints is demonstrated on two real-world problems- a credit card application task, and a problem in medical diagnosis. A measure of the monotonicity error
of a candidate function is defined and an objective function for the enforcement of monotonicity is derived from Bayesian principles. We report experimental results which show that using monotonicity hints leads to a statistically significant improvement in performance
on both problems.},
	urldate = {2017-08-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 9 ({NIPS} 1996)},
	publisher = {MIT Press},
	author = {Sill, Joseph and Abu-Mostafa, Yaser S.},
	editor = {Mozer, Michael C. and Jordan, Michael I. and Petsche, Thomas},
	year = {1997},
	pages = {634--640},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/ZJJ2TS7D/Sill and Abu-Mostafa - 1997 - Monotonicity Hints.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8ZBCE4SJ/64696.html:text/html}
}

@article{meinshausen_stability_2010,
	title = {Stability selection},
	volume = {72},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2010.00740.x/abstract},
	doi = {10.1111/j.1467-9868.2010.00740.x},
	abstract = {Summary.  Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
	language = {en},
	number = {4},
	urldate = {2017-09-08},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Meinshausen, Nicolai and Bühlmann, Peter},
	month = sep,
	year = {2010},
	keywords = {High dimensional data, Resampling, Stability selection, Structure estimation},
	pages = {417--473},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/3TNVDV6Q/Meinshausen and Bühlmann - 2010 - Stability selection.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/BK492Q39/abstract.html:text/html}
}

@article{tibshirani_diagnosis_2002,
	title = {Diagnosis of {Multiple} {Cancer} {Types} by {Shrunken} {Centroids} of {Gene} {Expression}},
	volume = {99},
	issn = {0027-8424},
	url = {http://www.jstor.org/stable/3058706},
	abstract = {We have devised an approach to cancer class prediction from gene expression profiling, based on an enhancement of the simple nearest prototype (centroid) classifier. We shrink the prototypes and hence obtain a classifier that is often more accurate than competing methods. Our method of "nearest shrunken centroids" identifies subsets of genes that best characterize each class. The technique is general and can be used in many other classification problems. To demonstrate its effectiveness, we show that the method was highly efficient in finding genes for classifying small round blue cell tumors and leukemias.},
	number = {10},
	urldate = {2017-10-13},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Tibshirani, Robert and Hastie, Trevor and Narasimhan, Balasubramanian and Chu, Gilbert},
	year = {2002},
	pages = {6567--6572},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/Z8VFG2VF/Tibshirani et al. - 2002 - Diagnosis of Multiple Cancer Types by Shrunken Cen.pdf:application/pdf}
}

@article{efron_empirical_2009,
	title = {Empirical {Bayes} {Estimates} for {Large}-{Scale} {Prediction} {Problems}},
	volume = {104},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/40592271},
	abstract = {Classical prediction methods, such as Fisher's linear discriminant function, were designed for small-scale problems in which the number of predictors N is much smaller than the number of observations n. Modern scientific devices often reverse this situation. A microarray analysis, for example, might include n = 100 subjects measured on N = 10,000 genes, each of which is a potential predictor. This article proposes an empirical Bayes approach to large-scale prediction, where the optimum Bayes prediction rule is estimated employing the data from all of the predictors. Microarray examples are used to illustrate the method. The results demonstrate a close connection with the shrunken centroids algorithm of Tibshirani et al. (2002), a frequentist regularization approach to large-scale prediction, and also with false discovery rate theory.},
	number = {487},
	urldate = {2017-10-13},
	journal = {Journal of the American Statistical Association},
	author = {Efron, Bradley},
	year = {2009},
	pages = {1015--1028},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/4J28WNTP/Efron - 2009 - Empirical Bayes Estimates for Large-Scale Predicti.pdf:application/pdf}
}

@article{zou_regularization_2005,
	title = {Regularization and variable selection via the elastic net},
	volume = {67},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00503.x/abstract},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	abstract = {Summary.  We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
	language = {en},
	number = {2},
	urldate = {2017-10-16},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Zou, Hui and Hastie, Trevor},
	month = apr,
	year = {2005},
	keywords = {Lasso, Variable selection, Grouping effect, LARS algorithm, Penalization, p≫n problem},
	pages = {301--320},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/PZJDSG2I/Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/9KMS4639/abstract.html:text/html}
}

@article{mallick_bayesian_2013,
	title = {Bayesian {Methods} for {High} {Dimensional} {Linear} {Models}},
	volume = {1},
	issn = {2155-6180},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3914549/},
	doi = {10.4172/2155-6180.S1-005},
	abstract = {In this article, we present a selective overview of some recent
developments in Bayesian model and variable selection methods for high
dimensional linear models. While most of the reviews in literature are based on
conventional methods, we focus on recently developed methods, which have proven
to be successful in dealing with high dimensional variable selection. First, we
give a brief overview of the traditional model selection methods (viz.
Mallow’s Cp, AIC, BIC, DIC), followed by a discussion on some recently
developed methods (viz. EBIC, regularization), which have occupied the minds of
many statisticians. Then, we review high dimensional Bayesian methods with a
particular emphasis on Bayesian regularization methods, which have been used
extensively in recent years. We conclude by briefly addressing the asymptotic
behaviors of Bayesian variable selection methods for high dimensional linear
models under different regularity conditions.},
	urldate = {2017-10-18},
	journal = {Journal of biometrics \& biostatistics},
	author = {Mallick, Himel and Yi, Nengjun},
	month = jun,
	year = {2013},
	pmid = {24511433},
	pmcid = {PMC3914549},
	pages = {005},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/8S5QKJWI/Mallick and Yi - 2013 - Bayesian Methods for High Dimensional Linear Model.pdf:application/pdf}
}

@article{kyung_penalized_2010,
	title = {Penalized regression, standard errors, and {Bayesian} lassos},
	volume = {5},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/euclid.ba/1340218343},
	doi = {10.1214/10-BA607},
	abstract = {Penalized regression methods for simultaneous variable selection and coefficient estimation, especially those based on the lasso of Tibshirani (1996), have received a great deal of attention in recent years, mostly through frequentist models. Properties such as consistency have been studied, and are achieved by different lasso variations. Here we look at a fully Bayesian formulation of the problem, which is flexible enough to encompass most versions of the lasso that have been previously considered. The advantages of the hierarchical Bayesian formulations are many. In addition to the usual ease-of-interpretation of hierarchical models, the Bayesian formulation produces valid standard errors (which can be problematic for the frequentist lasso), and is based on a geometrically ergodic Markov chain. We compare the performance of the Bayesian lassos to their frequentist counterparts using simulations, data sets that previous lasso papers have used, and a difficult modeling problem for predicting the collapse of governments around the world. In terms of prediction mean squared error, the Bayesian lasso performance is similar to and, in some cases, better than, the frequentist lasso.},
	language = {EN},
	number = {2},
	urldate = {2017-10-18},
	journal = {Bayesian Analysis},
	author = {Kyung, Minjung and Gill, Jeff and Ghosh, Malay and Casella, George},
	month = jun,
	year = {2010},
	mrnumber = {MR2719657},
	zmnumber = {1330.62289},
	keywords = {Variable selection, Gibbs sampling, Hierarchical Models, Geometric Ergodicity},
	pages = {369--411},
	file = {euclid.ba.1340218343.pdf:/Users/magnusmunch/Zotero/storage/UZ7R4H27/euclid.ba.1340218343.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8TGZTMSB/1340218343.html:text/html}
}

@article{kim_overfitting_2014,
	title = {Overfitting, generalization, and {MSE} in class probability estimation with high-dimensional data},
	volume = {56},
	issn = {1521-4036},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/bimj.201300083/abstract},
	doi = {10.1002/bimj.201300083},
	abstract = {Accurate class probability estimation is important for medical decision making but is challenging, particularly when the number of candidate features exceeds the number of cases. Special methods have been developed for nonprobabilistic classification, but relatively little attention has been given to class probability estimation with numerous candidate variables. In this paper, we investigate overfitting in the development of regularized class probability estimators. We investigate the relation between overfitting and accurate class probability estimation in terms of mean square error. Using simulation studies based on real datasets, we found that some degree of overfitting can be desirable for reducing mean square error. We also introduce a mean square error decomposition for class probability estimation that helps clarify the relationship between overfitting and prediction accuracy.},
	language = {en},
	number = {2},
	urldate = {2017-10-31},
	journal = {Biometrical Journal},
	author = {Kim, Kyung In and Simon, Richard},
	month = mar,
	year = {2014},
	keywords = {Class probability estimation, Covariance penalty, High-dimensional data, Mean square error, Overfitting},
	pages = {256--269},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/UWGBXQ9J/Kim and Simon - 2014 - Overfitting, generalization, and MSE in class prob.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/IWFCIVNW/abstract.html:text/html}
}

@article{van_wieringen_survival_2009,
	series = {Statistical {Genetics} \& {Statistical} {Genomics}: {Where} {Biology}, {Epistemology}, {Statistics}, and {Computation} {Collide}},
	title = {Survival prediction using gene expression data: {A} review and comparison},
	volume = {53},
	issn = {0167-9473},
	shorttitle = {Survival prediction using gene expression data},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947308002946},
	doi = {10.1016/j.csda.2008.05.021},
	abstract = {Knowledge of transcription of the human genome might greatly enhance our understanding of cancer. In particular, gene expression may be used to predict the survival of cancer patients. Microarray data are characterized by their high-dimensionality: the number of covariates (p∼1000) greatly exceeds the number of samples (n∼100), which is a considerable challenge in the context of survival prediction. An inventory of methods that have been used to model survival using gene expression is given. These methods are critically reviewed and compared in a qualitative way. Next, these methods are applied to three real-life data sets for a quantitative comparison. The choice of the evaluation measure of predictive performance is crucial for the selection of the best method. Depending on the evaluation measure, either the L2-penalized Cox regression or the random forest ensemble method yields the best survival time prediction using the considered gene expression data sets. Consensus on the best evaluation measure of predictive performance is needed.},
	number = {5},
	urldate = {2017-11-06},
	journal = {Computational Statistics \& Data Analysis},
	author = {van Wieringen, Wessel N. and Kun, David and Hampel, Regina and Boulesteix, Anne-Laure},
	month = mar,
	year = {2009},
	pages = {1590--1603},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/2TJ36KAF/van Wieringen et al. - 2009 - Survival prediction using gene expression data A .pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/CV4P9TVR/S0167947308002946.html:text/html}
}

@article{royston_dichotomizing_2006,
	title = {Dichotomizing continuous predictors in multiple regression: a bad idea},
	volume = {25},
	issn = {1097-0258},
	shorttitle = {Dichotomizing continuous predictors in multiple regression},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/sim.2331/abstract},
	doi = {10.1002/sim.2331},
	abstract = {In medical research, continuous variables are often converted into categorical variables by grouping values into two or more categories. We consider in detail issues pertaining to creating just two groups, a common approach in clinical research. We argue that the simplicity achieved is gained at a cost; dichotomization may create rather than avoid problems, notably a considerable loss of power and residual confounding. In addition, the use of a data-derived ‘optimal’ cutpoint leads to serious bias. We illustrate the impact of dichotomization of continuous predictor variables using as a detailed case study a randomized trial in primary biliary cirrhosis. Dichotomization of continuous data is unnecessary for statistical analysis and in particular should not be applied to explanatory variables in regression models. Copyright © 2005 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2017-11-06},
	journal = {Statistics in Medicine},
	author = {Royston, Patrick and Altman, Douglas G. and Sauerbrei, Willi},
	month = jan,
	year = {2006},
	keywords = {Regression, continuous covariates, dichotomization, categorization, efficiency, clinical research},
	pages = {127--141},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/XDCQC3IC/Royston et al. - 2006 - Dichotomizing continuous predictors in multiple re.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/DC7J2NRN/abstract.html:text/html}
}

@article{papandreou_efficient_2011,
	title = {Efficient variational inference in large-scale {Bayesian} compressed sensing},
	url = {http://arxiv.org/abs/1107.4637},
	doi = {10.1109/ICCVW.2011.6130406},
	abstract = {We study linear models under heavy-tailed priors from a probabilistic viewpoint. Instead of computing a single sparse most probable (MAP) solution as in standard deterministic approaches, the focus in the Bayesian compressed sensing framework shifts towards capturing the full posterior distribution on the latent variables, which allows quantifying the estimation uncertainty and learning model parameters using maximum likelihood. The exact posterior distribution under the sparse linear model is intractable and we concentrate on variational Bayesian techniques to approximate it. Repeatedly computing Gaussian variances turns out to be a key requisite and constitutes the main computational bottleneck in applying variational techniques in large-scale problems. We leverage on the recently proposed Perturb-and-MAP algorithm for drawing exact samples from Gaussian Markov random fields (GMRF). The main technical contribution of our paper is to show that estimating Gaussian variances using a relatively small number of such efficiently drawn random samples is much more effective than alternative general-purpose variance estimation techniques. By reducing the problem of variance estimation to standard optimization primitives, the resulting variational algorithms are fully scalable and parallelizable, allowing Bayesian computations in extremely large-scale problems with the same memory and time complexity requirements as conventional point estimation techniques. We illustrate these ideas with experiments in image deblurring.},
	urldate = {2017-11-06},
	journal = {arXiv:1107.4637 [cs, math, stat]},
	author = {Papandreou, George and Yuille, Alan},
	month = nov,
	year = {2011},
	note = {arXiv: 1107.4637},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory},
	pages = {1332--1339},
	file = {arXiv\:1107.4637 PDF:/Users/magnusmunch/Zotero/storage/785EB7UH/Papandreou and Yuille - 2011 - Efficient variational inference in large-scale Bay.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/4SNDK6NC/1107.html:text/html}
}

@book{noauthor_bayesian_nodate,
	title = {Bayesian {Survival} {Analysis} {\textbar} {Joseph} {G}. {Ibrahim} {\textbar} {Springer}},
	url = {//www.springer.com/gp/book/9780387952772},
	abstract = {Survival analysis arises in many fields of study including medicine, biology, engineering, public health, epidemiology, and economics. This book provides...},
	urldate = {2017-11-07},
	file = {10.1007-978-1-4757-3447-8.pdf:/Users/magnusmunch/Zotero/storage/WSBX6TX5/10.1007-978-1-4757-3447-8.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/BQDFE2RT/9780387952772.html:text/html}
}

@article{gelfand_bayesian_1995,
	title = {Bayesian {Analysis} of {Proportional} {Hazards} {Models} {Built} from {Monotone} {Functions}},
	volume = {51},
	issn = {0006-341X},
	url = {http://www.jstor.org/stable/2532986},
	doi = {10.2307/2532986},
	abstract = {We consider the usual proportional hazards model in the case where the baseline hazard, the covariate link, and the covariate coefficients are all unknown. Both the baseline hazard and the covariate link are monotone functions and thus are characterized using a dense class of such functions which arises, upon transformation, as a mixture of Beta distribution functions. We take a Bayesian approach for fitting such a model. Since interest focuses more upon the likelihood, we consider vague prior specifications including Jeffreys's prior. Computations are carried out using sampling-based methods. Model criticism is also discussed. Finally, a data set studying survival of a sample of lung cancer patients is analyzed.},
	number = {3},
	urldate = {2017-11-07},
	journal = {Biometrics},
	author = {Gelfand, Alan E. and Mallick, Bani K.},
	year = {1995},
	pages = {843--852},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/D6HWX79Z/Gelfand and Mallick - 1995 - Bayesian Analysis of Proportional Hazards Models B.pdf:application/pdf}
}

@article{chen_bayesian_2014,
	title = {A {Bayesian} {Approach} for the {Cox} {Proportional} {Hazards} {Model} with {Covariates} {Subject} to {Detection} {Limit}},
	volume = {3},
	issn = {1929-6029},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998726/},
	doi = {10.6000/1929-6029.2014.03.01.5},
	abstract = {The research on biomarkers has been limited in its effectiveness because biomarker levels can only be measured within the thresholds of assays and laboratory instruments, a challenge referred to as a detection limit (DL) problem. In this paper, we propose a Bayesian approach to the Cox proportional hazards model with explanatory variables subject to lower, upper, or interval DLs. We demonstrate that by formulating the time-to-event outcome using the Poisson density with counting process notation, implementing the proposed approach in the OpenBUGS and JAGS is straightforward. We have conducted extensive simulations to compare the proposed Bayesian approach to the other four commonly used methods and to evaluate its robustness with respect to the distribution assumption of the biomarkers. The proposed Bayesian approach and other methods were applied to an acute lung injury study, in which a panel of cytokine biomarkers was studied for the biomarkers’ association with ventilation-free survival.},
	number = {1},
	urldate = {2017-11-07},
	journal = {International journal of statistics in medical research},
	author = {Chen, Qingxia and Wu, Huiyun and Ware, Lorraine B. and Koyama, Tatsuki},
	month = jan,
	year = {2014},
	pmid = {24772198},
	pmcid = {PMC3998726},
	pages = {32--43},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/4SMJ4WSQ/Chen et al. - 2014 - A Bayesian Approach for the Cox Proportional Hazar.pdf:application/pdf}
}

@article{wang_convergence_2012,
	title = {Convergence and asymptotic normality of variational {Bayesian} approximations for exponential family models with missing values},
	url = {http://arxiv.org/abs/1207.4159},
	abstract = {We study the properties of variational Bayes approximations for exponential family models with missing values. It is shown that the iterative algorithm for obtaining the variational Bayesian estimator converges locally to the true value with probability 1 as the sample size becomes inde nitely large. Moreover, the variational posterior distribution is proved to be asymptotically normal.},
	urldate = {2017-11-07},
	journal = {arXiv:1207.4159 [math, stat]},
	author = {Wang, Bo and Titterington, D.},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.4159},
	keywords = {Mathematics - Statistics Theory, Statistics - Computation, Statistics - Methodology},
	file = {arXiv\:1207.4159 PDF:/Users/magnusmunch/Zotero/storage/4F8P77EW/Wang and Titterington - 2012 - Convergence and asymptotic normality of variationa.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/89T4KWIG/1207.html:text/html}
}

@article{goeman_testing_2006,
	title = {Testing against a high dimensional alternative},
	volume = {68},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2006.00551.x/abstract},
	doi = {10.1111/j.1467-9868.2006.00551.x},
	abstract = {Summary.  As the dimensionality of the alternative hypothesis increases, the power of classical tests tends to diminish quite rapidly. This is especially true for high dimensional data in which there are more parameters than observations. We discuss a score test on a hyperparameter in an empirical Bayesian model as an alternative to classical tests. It gives a general test statistic which can be used to test a point null hypothesis against a high dimensional alternative, even when the number of parameters exceeds the number of samples. This test will be shown to have optimal power on average in a neighbourhood of the null hypothesis, which makes it a proper generalization of the locally most powerful test to multiple dimensions. To illustrate this new locally most powerful test we investigate the case of testing the global null hypothesis in a linear regression model in more detail. The score test is shown to have significantly more power than the F-test whenever under the alternative the large variance principal components of the design matrix explain substantially more of the variance of the outcome than do the small variance principal components. The score test is also useful for detecting sparse alternatives in truly high dimensional data, where its power is comparable with the test based on the maximum absolute t-statistic.},
	language = {en},
	number = {3},
	urldate = {2017-11-10},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Goeman, Jelle J. and Van De Geer, Sara A. and Van Houwelingen, Hans C.},
	month = jun,
	year = {2006},
	keywords = {High dimensional data, Empirical Bayes modelling, F-test, Hypothesis testing, Locally most powerful test, Power, Score test},
	pages = {477--493},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/2U56ZK3T/Goeman et al. - 2006 - Testing against a high dimensional alternative.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/SJDGA9MT/abstract.html:text/html}
}

@article{goeman_global_2004,
	title = {A global test for groups of genes: testing association with a clinical outcome},
	volume = {20},
	issn = {1367-4803},
	shorttitle = {A global test for groups of genes},
	abstract = {MOTIVATION: This paper presents a global test to be used for the analysis of microarray data. Using this test it can be determined whether the global expression pattern of a group of genes is significantly related to some clinical outcome of interest. Groups of genes may be any size from a single gene to all genes on the chip (e.g. known pathways, specific areas of the genome or clusters from a cluster analysis).
RESULT: The test allows groups of genes of different size to be compared, because the test gives one p-value for the group, not a p-value for each gene. Researchers can use the test to investigate hypotheses based on theory or past research or to mine gene ontology databases for interesting pathways. Multiple testing problems do not occur unless many groups are tested. Special attention is given to visualizations of the test result, focussing on the associations between samples and showing the impact of individual genes on the test result.
AVAILABILITY: An R-package globaltest is available from http://www.bioconductor.org},
	language = {eng},
	number = {1},
	journal = {Bioinformatics (Oxford, England)},
	author = {Goeman, Jelle J. and van de Geer, Sara A. and de Kort, Floor and van Houwelingen, Hans C.},
	month = jan,
	year = {2004},
	pmid = {14693814},
	keywords = {Humans, Algorithms, Cluster Analysis, Diagnosis, Computer-Assisted, Feasibility Studies, Gene Expression Profiling, Genetic Predisposition to Disease, Genetic Testing, Leukemia, Models, Genetic, Models, Statistical, Oligonucleotide Array Sequence Analysis, Reproducibility of Results, Sample Size, Sensitivity and Specificity, Statistics as Topic},
	pages = {93--99},
	file = {btg382.pdf:/Users/magnusmunch/Zotero/storage/EAHXDJJ2/btg382.pdf:application/pdf}
}

@article{fan_tuning_2013,
	title = {Tuning parameter selection in high dimensional penalized likelihood},
	volume = {75},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/rssb.12001/abstract},
	doi = {10.1111/rssb.12001},
	abstract = {Determining how to select the tuning parameter appropriately is essential in penalized likelihood methods for high dimensional data analysis. We examine this problem in the setting of penalized likelihood methods for generalized linear models, where the dimensionality of covariates p is allowed to increase exponentially with the sample size n. We propose to select the tuning parameter by optimizing the generalized information criterion with an appropriate model complexity penalty. To ensure that we consistently identify the true model, a range for the model complexity penalty is identified in the generlized information criterion. We find that this model complexity penalty should diverge at the rate of some power of  log (p) depending on the tail probability behaviour of the response variables. This reveals that using the Akaike information criterion or Bayes information criterion to select the tuning parameter may not be adequate for consistently identifying the true model. On the basis of our theoretical study, we propose a uniform choice of the model complexity penalty and show that the approach proposed consistently identifies the true model among candidate models with asymptotic probability 1. We justify the performance of the procedure proposed by numerical simulations and a gene expression data analysis.},
	language = {en},
	number = {3},
	urldate = {2017-11-29},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Fan, Yingying and Tang, Cheng Yong},
	month = jun,
	year = {2013},
	keywords = {Generalized linear model, Variable selection, Generalized information criterion, Penalized likelihood, Tuning parameter selection},
	pages = {531--552},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/TWASH6Z9/Fan and Tang - 2013 - Tuning parameter selection in high dimensional pen.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/42APWVEC/abstract.html:text/html}
}

@article{schwarz_estimating_1978,
	title = {Estimating the {Dimension} of a {Model}},
	volume = {6},
	issn = {0090-5364},
	url = {http://www.jstor.org/stable/2958889},
	abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
	number = {2},
	urldate = {2017-11-30},
	journal = {The Annals of Statistics},
	author = {Schwarz, Gideon},
	year = {1978},
	pages = {461--464},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/8DENAQK4/Schwarz - 1978 - Estimating the Dimension of a Model.pdf:application/pdf}
}

@article{zak-szatkowska_modified_2011,
	title = {Modified versions of the {Bayesian} {Information} {Criterion} for sparse {Generalized} {Linear} {Models}},
	volume = {55},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947311001459},
	doi = {10.1016/j.csda.2011.04.016},
	abstract = {The classical model selection criteria, such as the Bayesian Information Criterion (BIC) or Akaike information criterion (AIC), have a strong tendency to overestimate the number of regressors when the search is performed over a large number of potential explanatory variables. To handle the problem of the overestimation, several modifications of the BIC have been proposed. These versions rely on supplementing the original BIC with some prior distributions on the class of possible models. Three such modifications are presented and compared in the context of sparse Generalized Linear Models (GLMs). The related choices of priors are discussed and the conditions for the asymptotic equivalence of these criteria are provided. The performance of the modified versions of the BIC is illustrated with an extensive simulation study and a real data analysis. Also, simplified versions of the modified BIC, based on least squares regression, are investigated.},
	number = {11},
	urldate = {2017-11-30},
	journal = {Computational Statistics \& Data Analysis},
	author = {Żak-Szatkowska, Małgorzata and Bogdan, Małgorzata},
	month = nov,
	year = {2011},
	keywords = {Model selection, Generalized Linear Models, Bayesian Information Criterion, Sparse linear models},
	pages = {2908--2924},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/BX88NSVM/Żak-Szatkowska and Bogdan - 2011 - Modified versions of the Bayesian Information Crit.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/NVV9RUX4/S0167947311001459.html:text/html}
}

@article{wang_consistent_2011,
	title = {Consistent tuning parameter selection in high dimensional sparse linear regression},
	volume = {102},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X11000455},
	doi = {10.1016/j.jmva.2011.03.007},
	abstract = {An exhaustive search as required for traditional variable selection methods is impractical in high dimensional statistical modeling. Thus, to conduct variable selection, various forms of penalized estimators with good statistical and computational properties, have been proposed during the past two decades. The attractive properties of these shrinkage and selection estimators, however, depend critically on the size of regularization which controls model complexity. In this paper, we consider the problem of consistent tuning parameter selection in high dimensional sparse linear regression where the dimension of the predictor vector is larger than the size of the sample. First, we propose a family of high dimensional Bayesian Information Criteria (HBIC), and then investigate the selection consistency, extending the results of the extended Bayesian Information Criterion (EBIC), in Chen and Chen (2008) to ultra-high dimensional situations. Second, we develop a two-step procedure, the SIS+AENET, to conduct variable selection in p{\textgreater}n situations. The consistency of tuning parameter selection is established under fairly mild technical conditions. Simulation studies are presented to confirm theoretical findings, and an empirical example is given to illustrate the use in the internet advertising data.},
	number = {7},
	urldate = {2017-11-30},
	journal = {Journal of Multivariate Analysis},
	author = {Wang, Tao and Zhu, Lixing},
	month = aug,
	year = {2011},
	keywords = {Variable selection, Tuning parameter selection, Bayesian Information Criterion, Adaptive Elastic Net, High dimensionality, Sure independence screening},
	pages = {1141--1151},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/DSDH8IIV/Wang and Zhu - 2011 - Consistent tuning parameter selection in high dime.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/EG7WPHJ8/S0047259X11000455.html:text/html}
}

@incollection{akaike_information_1998,
	series = {Springer {Series} in {Statistics}},
	title = {Information {Theory} and an {Extension} of the {Maximum} {Likelihood} {Principle}},
	isbn = {978-1-4612-7248-9 978-1-4612-1694-0},
	url = {https://link.springer.com/chapter/10.1007/978-1-4612-1694-0_15},
	abstract = {In this paper it is shown that the classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion. This observation shows an extension of the principle to provide answers to many practical problems of statistical model fitting.},
	language = {en},
	urldate = {2017-11-30},
	booktitle = {Selected {Papers} of {Hirotugu} {Akaike}},
	publisher = {Springer, New York, NY},
	author = {Akaike, Hirotogu},
	year = {1998},
	doi = {10.1007/978-1-4612-1694-0_15},
	pages = {199--213},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/MT2DWUHA/Akaike - 1998 - Information Theory and an Extension of the Maximum.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/PAEDW6R9/978-1-4612-1694-0_15.html:text/html}
}

@article{scott_bayes_2010,
	title = {Bayes and empirical-{Bayes} multiplicity adjustment in the variable-selection problem},
	volume = {38},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1278861454},
	doi = {10.1214/10-AOS792},
	abstract = {This paper studies the multiplicity-correction effect of standard Bayesian variable-selection priors in linear regression. Our first goal is to clarify when, and how, multiplicity correction happens automatically in Bayesian analysis, and to distinguish this correction from the Bayesian Ockham’s-razor effect. Our second goal is to contrast empirical-Bayes and fully Bayesian approaches to variable selection through examples, theoretical results and simulations. Considerable differences between the two approaches are found. In particular, we prove a theorem that characterizes a surprising aymptotic discrepancy between fully Bayes and empirical Bayes. This discrepancy arises from a different source than the failure to account for hyperparameter uncertainty in the empirical-Bayes estimate. Indeed, even at the extreme, when the empirical-Bayes estimate converges asymptotically to the true variable-inclusion probability, the potential for a serious difference remains.},
	language = {EN},
	number = {5},
	urldate = {2017-12-05},
	journal = {The Annals of Statistics},
	author = {Scott, James G. and Berger, James O.},
	month = oct,
	year = {2010},
	mrnumber = {MR2722450},
	zmnumber = {1200.62020},
	keywords = {Empirical Bayes, Variable selection, Bayesian model selection, multiple testing},
	pages = {2587--2619},
	file = {euclid.aos.1278861454.pdf:/Users/magnusmunch/Zotero/storage/3GV7EHJD/euclid.aos.1278861454.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/PUMEIB5H/1278861454.html:text/html}
}

@article{glaus_identifying_2012,
	title = {Identifying differentially expressed transcripts from {RNA}-seq data with biological variation},
	volume = {28},
	issn = {1367-4803},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3381971/},
	doi = {10.1093/bioinformatics/bts260},
	abstract = {Motivation: High-throughput sequencing enables expression analysis at the level of individual transcripts. The analysis of transcriptome expression levels and differential expression (DE) estimation requires a probabilistic approach to properly account for ambiguity caused by shared exons and finite read sampling as well as the intrinsic biological variance of transcript expression., Results: We present Bayesian inference of transcripts from sequencing data (BitSeq), a Bayesian approach for estimation of transcript expression level from RNA-seq experiments. Inferred relative expression is represented by Markov chain Monte Carlo samples from the posterior probability distribution of a generative model of the read data. We propose a novel method for DE analysis across replicates which propagates uncertainty from the sample-level model while modelling biological variance using an expression-level-dependent prior. We demonstrate the advantages of our method using simulated data as well as an RNA-seq dataset with technical and biological replication for both studied conditions., Availability: The implementation of the transcriptome expression estimation and differential expression analysis, BitSeq, has been written in C++ and Python. The software is available online from http://code.google.com/p/bitseq/, version 0.4 was used for generating results presented in this article., Contact:
glaus@cs.man.ac.uk, antti.honkela@hiit.fi or m.rattray@sheffield.ac.uk, Supplementary information:
Supplementary data are available at Bioinformatics online.},
	number = {13},
	urldate = {2018-01-10},
	journal = {Bioinformatics},
	author = {Glaus, Peter and Honkela, Antti and Rattray, Magnus},
	month = jul,
	year = {2012},
	pmid = {22563066},
	pmcid = {PMC3381971},
	pages = {1721--1728},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/R96IT3J9/Glaus et al. - 2012 - Identifying differentially expressed transcripts f.pdf:application/pdf;supp_bts260_BitSeq-Supplementary_material.pdf:/Users/magnusmunch/Zotero/storage/6FB3VUBQ/supp_bts260_BitSeq-Supplementary_material.pdf:application/pdf}
}

@article{neerincx_mir_2015,
	title = {{MiR} expression profiles of paired primary colorectal cancer and metastases by next-generation sequencing},
	volume = {4},
	copyright = {2015 Nature Publishing Group},
	issn = {2157-9024},
	url = {https://www.nature.com/articles/oncsis201529},
	doi = {10.1038/oncsis.2015.29},
	abstract = {MiR expression profiles of paired primary colorectal cancer and metastases by next-generation sequencing},
	language = {En},
	number = {10},
	urldate = {2018-01-11},
	journal = {Oncogenesis},
	author = {Neerincx, M. and Sie, D. L. S. and Wiel, M. A. van de and Grieken, N. C. T. van and Burggraaf, J. D. and Dekker, H. and Eijk, P. P. and Ylstra, B. and Verhoef, C. and Meijer, G. A. and Buffart, T. E. and Verheul, H. M. W.},
	month = oct,
	year = {2015},
	pages = {e170},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/59KF4U9H/Neerincx et al. - 2015 - MiR expression profiles of paired primary colorect.pdf:application/pdf;oncsis201529x1.doc:/Users/magnusmunch/Zotero/storage/XA5RN3BM/oncsis201529x1.doc:application/msword;Snapshot:/Users/magnusmunch/Zotero/storage/SF2EJARE/oncsis201529.html:text/html}
}

@article{dao_bayesian_2017,
	title = {Bayesian {Adaptive} {Lasso} with {Variational} {Bayes} for {Variable} {Selection} in {High}-dimensional {Generalized} {Linear} {Mixed} {Models}},
	volume = {0},
	issn = {0361-0918},
	url = {https://doi.org/10.1080/03610918.2017.1387663},
	doi = {10.1080/03610918.2017.1387663},
	abstract = {This article describes a full Bayesian treatment for simultaneous fixed-effect selection and parameter estimation in high-dimensional generalized linear mixed models. The approach consists of using a Bayesian adaptive Lasso penalty for signal-level adaptive shrinkage and a fast Variational Bayes scheme for estimating the posterior mode of the coefficients. The proposed approach offers several advantages over the existing methods, for example, the adaptive shrinkage parameters are automatically incorporated, no Laplace approximation step is required to integrate out the random effects. The performance of our approach is illustrated on several simulated and real data examples. The algorithm is implemented in the R package glmmvb and is made available online.},
	number = {ja},
	urldate = {2018-01-22},
	journal = {Communications in Statistics - Simulation and Computation},
	author = {Dao, Thanh Tung and Tran, Minh-Ngoc and Tran, Manh Cuong},
	month = oct,
	year = {2017},
	keywords = {EM algorithm, Lasso, Posterior mode, High dimensions},
	pages = {0--0},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/6XQSQZ32/Dao et al. - 2017 - Bayesian Adaptive Lasso with Variational Bayes for.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/2WBMBKT7/03610918.2017.html:text/html}
}

@phdthesis{joo_bayesian_2017,
	address = {New York, NY, USA},
	title = {Bayesian lasso: {An} extension for genome-wide association study},
	url = {https://search.proquest.com/openview/197063cc5f32e5e0aaac827bbfa7791a/1?pq-origsite=gscholar&cbl=18750&diss=y},
	school = {New York University},
	author = {Joo, Li Jin},
	month = jan,
	year = {2017},
	file = {out.pdf:/Users/magnusmunch/Zotero/storage/6NASCVDU/out.pdf:application/pdf}
}

@article{lockhart_significance_2014,
	title = {A {SIGNIFICANCE} {TEST} {FOR} {THE} {LASSO}},
	volume = {42},
	issn = {0090-5364},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4285373/},
	doi = {10.1214/13-AOS1175},
	abstract = {In the sparse linear regression setting, we consider testing the significance of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path. We propose a simple test statistic based on lasso fitted values, called the covariance test statistic, and show that when the true model is linear, this statistic has an Exp(1) asymptotic distribution under the null hypothesis (the null being that all truly active variables are contained in the current lasso model). Our proof of this result for the special case of the first predictor to enter the model (i.e., testing for a single significant predictor variable against the global null) requires only weak assumptions on the predictor matrix X. On the other hand, our proof for a general step in the lasso path places further technical assumptions on X and the generative model, but still allows for the important high-dimensional case p {\textgreater} n, and does not necessarily require that the current lasso model achieves perfect recovery of the truly active variables., Of course, for testing the significance of an additional variable between two nested linear models, one typically uses the chi-squared test, comparing the drop in residual sum of squares (RSS) to a 
χ12 distribution. But when this additional variable is not fixed, and has been chosen adaptively or greedily, this test is no longer appropriate: adaptivity makes the drop in RSS stochastically much larger than 
χ12 under the null hypothesis. Our analysis explicitly accounts for adaptivity, as it must, since the lasso builds an adaptive sequence of linear models as the tuning parameter λ decreases. In this analysis, shrinkage plays a key role: though additional variables are chosen adaptively, the coefficients of lasso active variables are shrunken due to the 
l1 penalty. Therefore, the test statistic (which is based on lasso fitted values) is in a sense balanced by these two opposing properties—adaptivity and shrinkage—and its null distribution is tractable and asymptotically Exp(1).},
	number = {2},
	urldate = {2018-01-30},
	journal = {Annals of statistics},
	author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
	month = apr,
	year = {2014},
	pmid = {25574062},
	pmcid = {PMC4285373},
	pages = {413--468},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/XCT5JHBG/Lockhart et al. - 2014 - A SIGNIFICANCE TEST FOR THE LASSO.pdf:application/pdf}
}

@article{tibshirani_exact_2016,
	title = {Exact {Post}-{Selection} {Inference} for {Sequential} {Regression} {Procedures}},
	volume = {111},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2015.1108848},
	doi = {10.1080/01621459.2015.1108848},
	abstract = {We propose new inference tools for forward stepwise regression, least angle regression, and the lasso. Assuming a Gaussian model for the observation vector y, we first describe a general scheme to perform valid inference after any selection event that can be characterized as y falling into a polyhedral set. This framework allows us to derive conditional (post-selection) hypothesis tests at any step of forward stepwise or least angle regression, or any step along the lasso regularization path, because, as it turns out, selection events for these procedures can be expressed as polyhedral constraints on y. The p-values associated with these tests are exactly uniform under the null distribution, in finite samples, yielding exact Type I error control. The tests can also be inverted to produce confidence intervals for appropriate underlying regression parameters. The R package selectiveInference, freely available on the CRAN repository, implements the new inference tools described in this article. Supplementary materials for this article are available online.},
	number = {514},
	urldate = {2018-01-30},
	journal = {Journal of the American Statistical Association},
	author = {Tibshirani, Ryan J. and Taylor, Jonathan and Lockhart, Richard and Tibshirani, Robert},
	month = apr,
	year = {2016},
	keywords = {Lasso, Confidence interval, Forward stepwise regression, Inference after selection, Least angle regression, p-Value},
	pages = {600--620},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/RMQKK92H/Tibshirani et al. - 2016 - Exact Post-Selection Inference for Sequential Regr.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8AH5KG7X/01621459.2015.html:text/html}
}

@article{alhamzawi_bayesian_2017,
	title = {The {Bayesian} elastic net regression},
	volume = {0},
	issn = {0361-0918},
	url = {https://doi.org/10.1080/03610918.2017.1307399},
	doi = {10.1080/03610918.2017.1307399},
	abstract = {A Bayesian elastic net approach is presented for variable selection and coefficient estimation in linear regression models. A simple Gibbs sampling algorithm was developed for posterior inference using a location-scale mixture representation of the Bayesian elastic net prior for the regression coefficients. The penalty parameters are chosen through an empirical method that maximizes the data marginal likelihood. Both simulated and real data examples show that the proposed method performs well in comparison to the other approaches.},
	number = {0},
	urldate = {2018-02-07},
	journal = {Communications in Statistics - Simulation and Computation},
	author = {Alhamzawi, Rahim and Ali, Haithem Taha Mohammad},
	month = mar,
	year = {2017},
	keywords = {Bayesian inference, Lasso, Elastic net, MCMC, Prior distribution, 62Fxx, 62F15},
	pages = {1--11},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/VHAVJCEF/Alhamzawi and Ali - 2017 - The Bayesian elastic net regression.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/UVRHQZEU/03610918.2017.html:text/html}
}

@article{castillo_bayesian_2015,
	title = {Bayesian linear regression with sparse priors},
	volume = {43},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1438606851},
	doi = {10.1214/15-AOS1334},
	abstract = {We study full Bayesian procedures for high-dimensional linear regression under sparsity constraints. The prior is a mixture of point masses at zero and continuous distributions. Under compatibility conditions on the design matrix, the posterior distribution is shown to contract at the optimal rate for recovery of the unknown sparse vector, and to give optimal prediction of the response vector. It is also shown to select the correct sparse model, or at least the coefficients that are significantly different from zero. The asymptotic shape of the posterior distribution is characterized and employed to the construction and study of credible sets for uncertainty quantification.},
	language = {EN},
	number = {5},
	urldate = {2018-02-14},
	journal = {The Annals of Statistics},
	author = {Castillo, Ismaël and Schmidt-Hieber, Johannes and Vaart, Aad van der},
	month = oct,
	year = {2015},
	mrnumber = {MR3375874},
	zmnumber = {06502640},
	keywords = {Bayesian inference, Sparsity},
	pages = {1986--2018},
	file = {euclid.aos.1438606851.pdf:/Users/magnusmunch/Zotero/storage/DREMWEW7/euclid.aos.1438606851.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/UQ7M8H8J/1438606851.html:text/html}
}

@article{van_der_pas_horseshoe_2014,
	title = {The horseshoe estimator: {Posterior} concentration around nearly black vectors},
	volume = {8},
	issn = {1935-7524},
	shorttitle = {The horseshoe estimator},
	url = {https://projecteuclid.org/euclid.ejs/1418134265},
	doi = {10.1214/14-EJS962},
	abstract = {We consider the horseshoe estimator due to Carvalho, Polson and Scott (2010) for the multivariate normal mean model in the situation that the mean vector is sparse in the nearly black sense. We assume the frequentist framework where the data is generated according to a fixed mean vector. We show that if the number of nonzero parameters of the mean vector is known, the horseshoe estimator attains the minimax ℓ2ℓ2{\textbackslash}ell\_\{2\} risk, possibly up to a multiplicative constant. We provide conditions under which the horseshoe estimator combined with an empirical Bayes estimate of the number of nonzero means still yields the minimax risk. We furthermore prove an upper bound on the rate of contraction of the posterior distribution around the horseshoe estimator, and a lower bound on the posterior variance. These bounds indicate that the posterior distribution of the horseshoe prior may be more informative than that of other one-component priors, including the Lasso.},
	language = {EN},
	number = {2},
	urldate = {2018-02-15},
	journal = {Electronic Journal of Statistics},
	author = {van der Pas, S. L. and Kleijn, B. J. K. and van der Vaart, A. W.},
	year = {2014},
	mrnumber = {MR3285877},
	zmnumber = {1309.62060},
	keywords = {Bayesian inference, Empirical Bayes, Sparsity, horseshoe prior, worst case risk, posterior contraction, normal means model},
	pages = {2585--2618},
	file = {euclid.ejs.1418134265.pdf:/Users/magnusmunch/Zotero/storage/RIDG5K63/euclid.ejs.1418134265.pdf:application/pdf}
}

@article{cohen_coefficient_1960,
	title = {A {Coefficient} of {Agreement} for {Nominal} {Scales}},
	volume = {20},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/001316446002000104},
	doi = {10.1177/001316446002000104},
	language = {en},
	number = {1},
	urldate = {2018-02-28},
	journal = {Educational and Psychological Measurement},
	author = {Cohen, Jacob},
	month = apr,
	year = {1960},
	pages = {37--46},
	file = {SAGE PDF Full Text:/Users/magnusmunch/Zotero/storage/KHZA9CWI/Cohen - 1960 - A Coefficient of Agreement for Nominal Scales.pdf:application/pdf}
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459},
	shorttitle = {Variational {Inference}},
	url = {https://doi.org/10.1080/01621459.2017.1285773},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.},
	number = {518},
	urldate = {2018-03-23},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	keywords = {Algorithms, Computationally intensive methods, Statistical computing},
	pages = {859--877},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/7XISFUAE/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/ZR4DPV3P/01621459.2017.html:text/html}
}

@article{agarwal_predicting_nodate,
	title = {Predicting effective {microRNA} target sites in mammalian {mRNAs}},
	volume = {4},
	issn = {2050-084X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4532895/},
	doi = {10.7554/eLife.05005},
	abstract = {MicroRNA targets are often recognized through pairing between the miRNA seed region and complementary sites within target mRNAs, but not all of these canonical sites are equally effective, and both computational and in vivo UV-crosslinking approaches suggest that many mRNAs are targeted through non-canonical interactions. Here, we show that recently reported non-canonical sites do not mediate repression despite binding the miRNA, which indicates that the vast majority of functional sites are canonical. Accordingly, we developed an improved quantitative model of canonical targeting, using a compendium of experimental datasets that we pre-processed to minimize confounding biases. This model, which considers site type and another 14 features to predict the most effectively targeted mRNAs, performed significantly better than existing models and was as informative as the best high-throughput in vivo crosslinking approaches. It drives the latest version of TargetScan (v7.0; targetscan.org), thereby providing a valuable resource for placing miRNAs into gene-regulatory networks., DOI:
http://dx.doi.org/10.7554/eLife.05005.001, Proteins are built by using the information contained in molecules of messenger RNA (mRNA). Cells have several ways of controlling the amounts of different proteins they make. For example, a so-called ‘microRNA’ molecule can bind to an mRNA molecule to cause it to be more rapidly degraded and less efficiently used, thereby reducing the amount of protein built from that mRNA. Indeed, microRNAs are thought to help control the amount of protein made from most human genes, and biologists are working to predict the amount of control imparted by each microRNA on each of its mRNA targets., All RNA molecules are made up of a sequence of bases, each commonly known by a single letter—‘A’, ‘U’, ‘C’ or ‘G’. These bases can each pair up with one specific other base—‘A’ pairs with ‘U’, and ‘C’ pairs with ‘G’. To direct the repression of an mRNA molecule, a region of the microRNA known as a ‘seed’ binds to a complementary sequence in the target mRNA. ‘Canonical sites’ are regions in the mRNA that contain the exact sequence of partner bases for the bases in the microRNA seed. Some canonical sites are more effective at mRNA control than others. ‘Non-canonical sites’ also exist in which the pairing between the microRNA seed and mRNA does not completely match. Previous work has suggested that many non-canonical sites can also control mRNA degradation and usage., Agarwal et al. first used large experimental datasets from many sources to investigate microRNA activity in more detail. As expected, when mRNAs had canonical sites that matched the microRNA, mRNA levels and usage tended to drop. However, no effect was observed when the mRNAs only had recently identified non-canonical sites. This suggests that microRNAs primarily bind to canonical sites to control protein production., Based on these results, Agarwal et al. further developed a statistical model that predicts the effects of microRNAs binding to canonical sites. The updated model considers 14 different features of the microRNA, microRNA site, or mRNA—including the mRNA sequence around the site—to predict which sites within mRNAs are most effectively targeted by microRNAs. Tests showed that Agarwal et al.'s model was as good as experimental approaches at identifying the effective target sites, and was better than existing computational models., The model has been used to power the latest version of a freely available resource called TargetScan, and so could prove a valuable resource for researchers investigating the many important roles of microRNAs in controlling protein production., DOI:
http://dx.doi.org/10.7554/eLife.05005.002},
	urldate = {2018-07-19},
	journal = {eLife},
	author = {Agarwal, Vikram and Bell, George W and Nam, Jin-Wu and Bartel, David P},
	pmid = {26267216},
	pmcid = {PMC4532895},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/9JDMCIC4/Agarwal et al. - Predicting effective microRNA target sites in mamm.pdf:application/pdf}
}

@article{munch_adaptive_2018,
	title = {Adaptive group-regularized logistic elastic net regression},
	url = {http://arxiv.org/abs/1805.00389},
	abstract = {In high-dimensional data settings, additional information on the features is often available. Examples of such external information in omics research are: (a) p-values from a previous study, (b) a summary of prior information, and (c) omics annotation. The inclusion of this information in the analysis may enhance classification performance and feature selection, but is not straightforward in the standard regression setting. As a solution to this problem, we propose a group-regularized (logistic) elastic net regression method, where each penalty parameter corresponds to a group of features based on the external information. The method, termed gren, makes use of the Bayesian formulation of logistic elastic net regression to estimate both the model and penalty parameters in an approximate empirical-variational Bayes framework. Simulations and an application to a colon cancer microRNA study show that, if the partitioning of the features is informative, classification performance and feature selection are indeed enhanced.},
	urldate = {2018-07-19},
	journal = {arXiv:1805.00389 [stat]},
	author = {Münch, Magnus M. and Peeters, Carel F. W. and van der Vaart, Aad W. and van de Wiel, Mark A.},
	month = may,
	year = {2018},
	note = {arXiv: 1805.00389},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1805.00389 PDF:/Users/magnusmunch/Zotero/storage/4RZ5M98Z/Münch et al. - 2018 - Adaptive group-regularized logistic elastic net re.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/HWCM9IDB/1805.html:text/html}
}

@article{neerincx_combination_2018,
	title = {Combination of a six {microRNA} expression profile with four clinicopathological factors for response prediction of systemic treatment in patients with advanced colorectal cancer},
	volume = {13},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201809},
	doi = {10.1371/journal.pone.0201809},
	abstract = {Background First line chemotherapy is effective in 75 to 80\% of patients with metastatic colorectal cancer (mCRC). We studied whether microRNA (miR) expression profiles can predict treatment outcome for first line fluoropyrimidine containing systemic therapy in patients with mCRC. Methods MiR expression levels were determined by next generation sequencing from snap frozen tumor samples of 88 patients with mCRC. Predictive miRs were selected with penalized logistic regression and posterior forward selection. The prediction co-efficients of the miRs were re-estimated and validated by real-time quantitative PCR in an independent cohort of 81 patients with mCRC. Results Expression levels of miR-17-5p, miR-20a-5p, miR-30a-5p, miR-92a-3p, miR-92b-3p and miR-98-5p in combination with age, tumor differentiation, adjuvant therapy and type of systemic treatment, were predictive for clinical benefit in the training cohort with an AUC of 0.78. In the validation cohort the addition of the six miR signature to the four clinicopathological factors demonstrated a significant increased AUC for predicting treatment response versus those with stable disease (SD) from 0.79 to 0.90. The increase for predicting treatment response versus progressive disease (PD) and for patients with SD versus those with PD was not significant. in the validation cohort. MiR-17-5p, miR-20a-5p and miR-92a-3p were significantly upregulated in patients with treatment response in both the training and validation cohorts. Conclusion A six miR expression signature was identified that predicted treatment response to fluoropyrimidine containing first line systemic treatment in patients with mCRC when combined with four clinicopathological factors. Independent validation demonstrated added predictive value of this miR-signature for predicting treatment response versus SD. However, added predicted value for separating patients with PD could not be validated. The clinical relevance of the identified miRs for predicting treatment response has to be further explored.},
	language = {en},
	number = {8},
	urldate = {2018-08-17},
	journal = {PLOS ONE},
	author = {Neerincx, Maarten and Poel, Dennis and Sie, Daoud L. S. and Grieken, Nicole C. T. van and Shankaraiah, Ram C. and Lijster, Floor S. W. van der Wolf-de and Waesberghe, Jan-Hein T. M. van and Burggraaf, Jan-Dirk and Eijk, Paul P. and Verhoef, Cornelis and Ylstra, Bauke and Meijer, Gerrit A. and Wiel, Mark A. van de and Buffart, Tineke E. and Verheul, Henk M. W.},
	month = aug,
	year = {2018},
	keywords = {Forecasting, Cancer treatment, Differentiated tumors, Metastatic tumors, Biomarkers, Colorectal cancer, Next-generation sequencing, Tumor resection},
	pages = {e0201809},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/GD3HTE5I/Neerincx et al. - 2018 - Combination of a six microRNA expression profile w.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/3H22VGPP/article.html:text/html}
}

@article{huang_selective_2012,
	title = {A {Selective} {Review} of {Group} {Selection} in {High}-{Dimensional} {Models}},
	volume = {27},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1356098552},
	doi = {10.1214/12-STS392},
	abstract = {Grouping structures arise naturally in many statistical modeling problems. Several methods have been proposed for variable selection that respect grouping structure in variables. Examples include the group LASSO and several concave group selection methods. In this article, we give a selective review of group selection concerning methodological developments, theoretical properties and computational algorithms. We pay particular attention to group selection methods involving concave penalties. We address both group selection and bi-level selection methods. We describe several applications of these methods in nonparametric additive models, semiparametric regression, seemingly unrelated regressions, genomic data analysis and genome wide association studies. We also highlight some issues that require further study.},
	language = {EN},
	number = {4},
	urldate = {2018-09-04},
	journal = {Statistical Science},
	author = {Huang, Jian and Breheny, Patrick and Ma, Shuangge},
	month = nov,
	year = {2012},
	mrnumber = {MR3025130},
	zmnumber = {1331.62347},
	keywords = {Sparsity, Penalized regression, oracle property, Bi-level selection, group LASSO, concave group selection},
	pages = {481--499},
	file = {euclid.ss.1356098552.pdf:/Users/magnusmunch/Zotero/storage/RDD238GB/euclid.ss.1356098552.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/MDNF72I2/1356098552.html:text/html}
}

@article{breheny_penalized_2009,
	title = {Penalized methods for bi-level variable selection},
	volume = {2},
	issn = {1938-7989},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2904563/},
	abstract = {In many applications, covariates possess a grouping structure that can be incorporated into the analysis to select important groups as well as important members of those groups. This work focuses on the incorporation of grouping structure into penalized regression. We investigate the previously proposed group lasso and group bridge penalties as well as a novel method, group MCP, introducing a framework and conducting simulation studies that shed light on the behavior of these methods. To fit these models, we use the idea of a locally approximated coordinate descent to develop algorithms which are fast and stable even when the number of features is much larger than the sample size. Finally, these methods are applied to a genetic association study of age-related macular degeneration.},
	number = {3},
	urldate = {2018-09-04},
	journal = {Statistics and its interface},
	author = {Breheny, Patrick and Huang, Jian},
	month = jul,
	year = {2009},
	pmid = {20640242},
	pmcid = {PMC2904563},
	pages = {369--380},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/MZRCDKHX/Breheny and Huang - 2009 - Penalized methods for bi-level variable selection.pdf:application/pdf}
}

@article{breheny_group_2015,
	title = {The group exponential lasso for bi-level variable selection},
	volume = {71},
	copyright = {© 2015, The International Biometric Society},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12300},
	doi = {10.1111/biom.12300},
	abstract = {In many applications, covariates possess a grouping structure that can be incorporated into the analysis to select important groups as well as important members of those groups. One important example arises in genetic association studies, where genes may have several variants capable of contributing to disease. An ideal penalized regression approach would select variables by balancing both the direct evidence of a feature's importance as well as the indirect evidence offered by the grouping structure. This work proposes a new approach we call the group exponential lasso (GEL) which features a decay parameter controlling the degree to which feature selection is coupled together within groups. We demonstrate that the GEL has a number of statistical and computational advantages over previously proposed group penalties such as the group lasso, group bridge, and composite MCP. Finally, we apply these methods to the problem of detecting rare variants in a genetic association study.},
	language = {en},
	number = {3},
	urldate = {2018-09-05},
	journal = {Biometrics},
	author = {Breheny, Patrick},
	month = sep,
	year = {2015},
	keywords = {Penalized regression, Group variable selection, Rare variants},
	pages = {731--740},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/SBGPH35I/Breheny - 2015 - The group exponential lasso for bi-level variable .pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8UJQIQSA/biom.html:text/html}
}

@article{tutz_regularized_2016,
	title = {Regularized regression for categorical data},
	volume = {16},
	issn = {1471-082X},
	url = {https://doi.org/10.1177/1471082X16642560},
	doi = {10.1177/1471082X16642560},
	abstract = {In the last two decades, regularization techniques, in particular penalty-based methods, have become very popular in statistical modelling. Driven by technological developments, most approaches have been designed for high-dimensional problems with metric variables, whereas categorical data has largely been neglected. In recent years, however, it has become clear that regularization is also very promising when modelling categorical data. A specific trait of categorical data is that many parameters are typically needed to model the underlying structure. This results in complex estimation problems that call for structured penalties which are tailored to the categorical nature of the data. This article gives a systematic overview of penalty-based methods for categorical data developed so far and highlights some issues where further research is needed. We deal with categorical predictors as well as models for categorical response variables. The primary interest of this article is to give insight into basic properties of and differences between methods that are important with respect to statistical modelling in practice, without going into technical details or extensive discussion of asymptotic properties.},
	language = {en},
	number = {3},
	urldate = {2018-09-10},
	journal = {Statistical Modelling},
	author = {Tutz, Gerhard and Gertheiss, Jan},
	month = jun,
	year = {2016},
	pages = {161--200},
	file = {SAGE PDF Full Text:/Users/magnusmunch/Zotero/storage/EXC8NZ87/Tutz and Gertheiss - 2016 - Regularized regression for categorical data.pdf:application/pdf}
}

@article{leday_gene_2017,
	title = {Gene network reconstruction using global-local shrinkage priors},
	volume = {11},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/euclid.aoas/1491616871},
	doi = {10.1214/16-AOAS990},
	abstract = {Reconstructing a gene network from high-throughput molecular data is an important but challenging task, as the number of parameters to estimate easily is much larger than the sample size. A conventional remedy is to regularize or penalize the model likelihood. In network models, this is often done locally in the neighborhood of each node or gene. However, estimation of the many regularization parameters is often difficult and can result in large statistical uncertainties. In this paper we propose to combine local regularization with global shrinkage of the regularization parameters to borrow strength between genes and improve inference. We employ a simple Bayesian model with nonsparse, conjugate priors to facilitate the use of fast variational approximations to posteriors. We discuss empirical Bayes estimation of hyperparameters of the priors, and propose a novel approach to rank-based posterior thresholding. Using extensive model- and data-based simulations, we demonstrate that the proposed inference strategy outperforms popular (sparse) methods, yields more stable edges, and is more reproducible. The proposed method, termed ShrinkNet, is then applied to Glioblastoma to investigate the interactions between genes associated with patient survival.},
	language = {EN},
	number = {1},
	urldate = {2018-09-14},
	journal = {The Annals of Applied Statistics},
	author = {Leday, Gwenaël G. R. and Gunst, Mathisca C. M. de and Kpogbezan, Gino B. and Vaart, Aad W. van der and Wieringen, Wessel N. van and Wiel, Mark A. van de},
	month = mar,
	year = {2017},
	mrnumber = {MR3634314},
	zmnumber = {1366.62227},
	keywords = {Bayesian inference, Empirical Bayes, Shrinkage, Variational approximation, Undirected gene network},
	pages = {41--68},
	file = {euclid.aoas.1491616871.pdf:/Users/magnusmunch/Zotero/storage/P6Z8KDIC/euclid.aoas.1491616871.pdf:application/pdf;NIHMS70644-supplement-Supplmentary_Information.pdf:/Users/magnusmunch/Zotero/storage/IKP52PJ5/NIHMS70644-supplement-Supplmentary_Information.pdf:application/pdf;PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/K24764M3/Leday et al. - 2017 - Gene Network Reconstruction using Global-Local Shr.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/7KRURCR3/1491616871.html:text/html}
}

@article{augugliaro_differential-geometric_2016,
	title = {A differential-geometric approach to generalized linear models with grouped predictors},
	volume = {103},
	issn = {0006-3444},
	url = {https://academic-oup-com.ezproxy.leidenuniv.nl:2443/biomet/article/103/3/563/1743801},
	doi = {10.1093/biomet/asw023},
	abstract = {Abstract.  We propose an extension of the differential-geometric least angle regression method to perform sparse group inference in a generalized linear model.},
	language = {en},
	number = {3},
	urldate = {2018-10-01},
	journal = {Biometrika},
	author = {Augugliaro, Luigi and Mineo, Angelo M. and Wit, Ernst C.},
	month = sep,
	year = {2016},
	pages = {563--577},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/DQ5K6VN5/Augugliaro et al. - 2016 - A differential-geometric approach to generalized l.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/BWX5J3G7/1743801.html:text/html}
}

@article{augugliaro_differential_2013,
	title = {Differential geometric least angle regression: a differential geometric approach to sparse generalized linear models},
	volume = {75},
	issn = {1467-9868},
	shorttitle = {Differential geometric least angle regression},
	url = {https://rss-onlinelibrary-wiley-com.ezproxy.leidenuniv.nl:2443/doi/abs/10.1111/rssb.12000},
	doi = {10.1111/rssb.12000},
	language = {en},
	number = {3},
	urldate = {2018-10-01},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Augugliaro, Luigi and Mineo, Angelo M. and Wit, Ernst C.},
	month = jun,
	year = {2013},
	pages = {471--498},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/PH8PSN8D/Augugliaro et al. - 2013 - Differential geometric least angle regression a d.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/GWTAH5BT/rssb.html:text/html}
}

@article{roy_selection_2017,
	title = {Selection of {Tuning} {Parameters}, {Solution} {Paths} and {Standard} {Errors} for {Bayesian} {Lassos}},
	volume = {12},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/euclid.ba/1473276258},
	doi = {10.1214/16-BA1025},
	abstract = {Penalized regression methods such as the lasso and elastic net (EN) have become popular for simultaneous variable selection and coefficient estimation. Implementation of these methods require selection of the penalty parameters. We propose an empirical Bayes (EB) methodology for selecting these tuning parameters as well as computation of the regularization path plots. The EB method does not suffer from the “double shrinkage problem” of frequentist EN. Also it avoids the difficulty of constructing an appropriate prior on the penalty parameters. The EB methodology is implemented by efficient importance sampling method based on multiple Gibbs sampler chains. Since the Markov chains underlying the Gibbs sampler are proved to be geometrically ergodic, Markov chain central limit theorem can be used to provide asymptotically valid confidence band for profiles of EN coefficients. The practical effectiveness of our method is illustrated by several simulation examples and two real life case studies. Although this article considers lasso and EN for brevity, the proposed EB method is general and can be used to select shrinkage parameters in other regularization methods.},
	language = {EN},
	number = {3},
	urldate = {2018-10-15},
	journal = {Bayesian Analysis},
	author = {Roy, Vivekananda and Chakraborty, Sounak},
	month = sep,
	year = {2017},
	mrnumber = {MR3655875},
	zmnumber = {1384.62102},
	keywords = {Bayesian lasso, Empirical Bayes, Shrinkage, Markov chain Monte Carlo, Geometric Ergodicity, Elastic net, importance sampling, standard errors},
	pages = {753--778},
	file = {euclid.ba.1473276258.pdf:/Users/magnusmunch/Zotero/storage/AU9PP875/euclid.ba.1473276258.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/QUC9RAR7/1473276258.html:text/html}
}

@article{lee_variable_2017,
	title = {Variable selection for high-dimensional genomic data with censored outcomes using group lasso prior},
	volume = {112},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947317300440},
	doi = {10.1016/j.csda.2017.02.014},
	abstract = {The variable selection problem is discussed in the context of high-dimensional failure time data arising from the accelerated failure time model. A data augmentation approach is employed in order to deal with censored survival times and to facilitate prior-posterior conjugacy. To identify a set of grouped relevant covariates, a shrinkage prior distribution is specified for regression coefficients mimicking the effect of group lasso penalty. It is noted that unlike the corresponding frequentist method, a Bayesian penalized regression approach cannot shrink the estimates of coefficients to exact zeros in general. Towards resolving the issue, a two-stage thresholding method that exploits the scaled neighborhood criterion and the Bayesian information criterion is devised. Simulation studies are performed to assess the robustness and performance of the proposed method in terms of variable selection accuracy and predictive power. The method is successfully applied to a set of microarray data on the individuals diagnosed with diffuse large B-cell lymphoma. In addition, an R package called psbcGroup, which can be downloaded freely from CRAN, is developed for the implementation of the methods.},
	urldate = {2018-10-15},
	journal = {Computational Statistics \& Data Analysis},
	author = {Lee, Kyu Ha and Chakraborty, Sounak and Sun, Jianguo},
	month = aug,
	year = {2017},
	keywords = {Bayesian lasso, Gibbs sampler, Penalized regression, group LASSO, Accelerated failure time model},
	pages = {1--13},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/MCQR9MM3/Lee et al. - 2017 - Variable selection for high-dimensional genomic da.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/H5SBCXTD/S0167947317300440.html:text/html}
}

@article{zhang_novel_2018,
	title = {A novel variational {Bayesian} method for variable selection in logistic regression models},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947318302081},
	doi = {10.1016/j.csda.2018.08.025},
	abstract = {With high-dimensional data emerging in various domains, sparse logistic regression models have gained much interest of researchers. Variable selection plays a key role in both improving the prediction accuracy and enhancing the interpretability of built models. Bayesian variable selection approaches enjoy many advantages such as high selection accuracy, easily incorporating many kinds of prior knowledge and so on. Because Bayesian methods generally make inference from the posterior distribution with Markov Chain Monte Carlo (MCMC) techniques, however, they become intractable in high-dimensional situations due to the large searching space. To address this issue, a novel variational Bayesian method for variable selection in high-dimensional logistic regression models is presented. The proposed method is based on the indicator model in which each covariate is equipped with a binary latent variable indicating whether it is important. The Bernoulli-type prior is adopted for the latent indicator variable. As for the specification of the hyperparameter in the Bernoulli prior, we provide two schemes to determine its optimal value so that the novel model can achieve sparsity adaptively. To identify important variables and make predictions, one efficient variational Bayesian approach is employed to make inference from the posterior distribution. The experiments conducted with both synthetic and some publicly available data show that the new method outperforms or is very competitive with some other popular counterparts.},
	urldate = {2018-11-02},
	journal = {Computational Statistics \& Data Analysis},
	author = {Zhang, Chun-Xia and Xu, Shuang and Zhang, Jiang-She},
	month = sep,
	year = {2018},
	keywords = {Sparse model, Variable selection, Variational Bayes, High-dimensional data, Logistic regression, Indicator model},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/MVWHGW3T/Zhang et al. - 2018 - A novel variational Bayesian method for variable s.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/WXITJJSZ/S0167947318302081.html:text/html}
}

@article{ignatiadis_covariate_2017,
	title = {Covariate powered cross-weighted multiple testing},
	url = {http://arxiv.org/abs/1701.05179},
	abstract = {A fundamental task in the analysis of datasets with many variables is screening for associations. This can be cast as a multiple testing task, where the major challenge is achieving high detection power while controlling type I error. We consider \$m\$ hypothesis tests represented by pairs \$((P\_i, X\_i))\_\{1{\textbackslash}leq i {\textbackslash}leq m\}\$ of p-values \$P\_i\$ and covariates \$X\_i\$, such that \$P\_i {\textbackslash}perp X\_i\$ under the null hypothesis. Here, we show how to use information potentially available in the covariates about heterogeneities among hypotheses to increase power compared to conventional procedures that only use the \$P\_i\$. To this end, we upgrade existing weighted multiple testing procedures through the Independent Hypothesis Weighting (IHW) framework to use data-driven weights which are a function of the covariate \$X\_i\$. Finite sample guarantees, e.g. false discovery rate (FDR) control, are derived from cross-weighting, a novel data-splitting approach that enables learning the weight-covariate function without overfitting as long as the hypotheses can be partitioned into independent folds, with arbitrary within-fold dependence. We show how the increased power of IHW can be understood in terms of the conditional two-groups model. A key implication of IHW is that hypothesis rejection in many common multiple testing setups should not proceed according to the ranking of the p-values, but by an alternative ranking implied by the covariate-weighted p-values.},
	journal = {arXiv:1701.05179 [stat]},
	author = {Ignatiadis, Nikolaos and Huber, Wolfgang},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.05179},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1701.05179 PDF:/Users/magnusmunch/Zotero/storage/2V4DD68Z/Ignatiadis and Huber - 2017 - Covariate powered cross-weighted multiple testing.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/3WIGXNDB/1701.html:text/html}
}

@article{velten_adaptive_2018,
	title = {Adaptive penalization in high-dimensional regression and classification with external covariates using variational {Bayes}},
	url = {http://arxiv.org/abs/1811.02962},
	abstract = {Penalization schemes like Lasso or ridge regression are routinely used to regress a response of interest on a high-dimensional set of potential predictors. Despite being decisive, the question of the relative strength of penalization is often glossed over and only implicitly determined by the scale of individual predictors. At the same time, additional information on the predictors is available in many applications but left unused. Here, we propose to make use of such external covariates to adapt the penalization in a data-driven manner. We present a method that differentially penalizes feature groups defined by the covariates and adapts the relative strength of penalization to the information content of each group. Using techniques from the Bayesian tool-set our procedure combines shrinkage with feature selection and provides a scalable optimization scheme. We demonstrate in simulations that the method accurately recovers the true effect sizes and sparsity patterns per feature group. Furthermore, it leads to an improved prediction performance in situations where the groups have strong differences in dynamic range. In applications to data from high-throughput biology, the method enables re-weighting the importance of feature groups from different assays. Overall, using available covariates extends the range of applications of penalized regression, improves model interpretability and can improve prediction performance. We provide an open-source implementation of the method in the R package graper.},
	journal = {arXiv:1811.02962 [stat]},
	author = {Velten, Britta and Huber, Wolfgang},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.02962},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1811.02962 PDF:/Users/magnusmunch/Zotero/storage/GNAN5VJH/Velten and Huber - 2018 - Adaptive penalization in high-dimensional regressi.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/52U2EK9J/1811.html:text/html}
}

@article{leday_gene_2017-1,
	title = {Gene {Network} {Reconstruction} using {Global}-{Local} {Shrinkage} {Priors}},
	volume = {11},
	issn = {1932-6157},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5388190/},
	abstract = {Reconstructing a gene network from high-throughput molecular data is an important but challenging task, as the number of parameters to estimate easily is much larger than the sample size. A conventional remedy is to regularize or penalize the model likelihood. In network models, this is often done locally in the neighbourhood of each node or gene. However, estimation of the many regularization parameters is often difficult and can result in large statistical uncertainties. In this paper we propose to combine local regularization with global shrinkage of the regularization parameters to borrow strength between genes and improve inference. We employ a simple Bayesian model with non-sparse, conjugate priors to facilitate the use of fast variational approximations to posteriors. We discuss empirical Bayes estimation of hyper-parameters of the priors, and propose a novel approach to rank-based posterior thresholding. Using extensive model- and data-based simulations, we demonstrate that the proposed inference strategy outperforms popular (sparse) methods, yields more stable edges, and is more reproducible. The proposed method, termed ShrinkNet, is then applied to Glioblastoma to investigate the interactions between genes associated with patient survival.},
	number = {1},
	urldate = {2018-02-28},
	journal = {The annals of applied statistics},
	author = {Leday, Gwenaël G.R. and de Gunst, Mathisca C.M. and Kpogbezan, Gino B. and van der Vaart, Aad W. and van Wieringen, Wessel N. and van de Wiel, Mark A.},
	month = mar,
	year = {2017},
	pmid = {28408966},
	pmcid = {PMC5388190},
	pages = {41--68}
}