% load packages
<<packages, include=FALSE, echo=FALSE>>=
library(cambridge)
@

% set options
<<settings, include=FALSE, echo=FALSE>>=
opts_knit$set(root.dir="..", base.dir="../figs")
opts_chunk$set(fig.align='center', fig.path="../figs/", 
               echo=FALSE, cache=FALSE, message=FALSE, fig.pos='!ht')
@

% read figure code
<<figures, include=FALSE>>=
read_chunk("code/figures.R")
@

\documentclass[a4paper,hidelinks]{article}
\usepackage{bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,
rotating,caption,subcaption,natbib,appendix,titlesec,multicol,verbatim,
bbm,algorithm,algpseudocode,pgfplotstable,threeparttable,booktabs,mathtools,
dsfont,parskip,xr-hyper,hyperref}
\externaldocument[md-]{manuscript2}

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Em}{\mathbf{E}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\Lm}{\mathbf{L}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}

% settings
\graphicspath{{../figs/}}
\pgfplotsset{compat=1.16}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Supplementary material to: Empirical Bayes estimation of the normal
inverse Gaussian prior in 
simultaneous-equation models}
\date{\today}
\author{Magnus M. M\"unch$^{1,2}$\footnote{Correspondence to: 
\href{mailto:m.munch@vumc.nl}{m.munch@vumc.nl}}, Mark A. van de Wiel$^{1,3}$, 
Sylvia Richardson$^{3}$, \\ and Gwena{\"e}l G. R. Leday$^{3}$}

\begin{document}

	\maketitle
	
	\noindent
	1. Department of Epidemiology \& Biostatistics, Amsterdam Public Health
	research institute, Amsterdam University medical centers, PO Box 7057, 1007 MB
	Amsterdam, The Netherlands \\*
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\*
	3. MRC Biostatistics Unit, Cambridge Institute of Public Health, Cambridge, 
	United Kingdom
	
	\section{Content overview}
	This document contains the Supplementary Material (SM) to the document `Drug
	efficacy prediction in cell lines'. In the following, this document is
	referred to as Main Document (MD).
	
	\section{Unpenalized covariates}
  Inclusion of an intercept $\beta_0$ and other unpenalized covariates is 
  achieved by letting $\X = \begin{bmatrix}\X_u & 
  \X_r\end{bmatrix}$, where $\X_u$ are the $u$
  unpenalized variables (possibly with a column of ones if an intercept is
  to be included) and $\X_r$ are the $r$ penalized variables. 
  Additionally, we let 
  $$
  \mathbf{H} = 
  \begin{bmatrix}
  \mathbf{0}_{u \times u} & \mathbf{0}_{u \times r} \\
  \mathbf{0}_{r \times u} & \diag[\E(\gamma_{jd}^{-2})]. \\
  \end{bmatrix}
  $$
  Then,
  \begin{align}\label{eq:unpencov}
    \bSigma &= \E(\sigma_d^{-2})^{-1}\(\X \X + \mathbf{H} \)^{-1} \nonumber \\
    & = 
    \E(\sigma_d^{-2})^{-1}\begin{bmatrix} 
      \X_u \tr \X_u & \X_u \tr \X_r \\
      \X_r \tr \X_u & \X_r \tr \X_r + \diag[\E(\gamma_{jd}^{-2})]
    \end{bmatrix}^{-1}.
  \end{align}
  With the choice of blocks as in (\ref{eq:unpencov}), blockwise inversion
  renders the largest required matrix inverse $(\X_r \tr \bOmega \X_r +
  \diag[\E(\gamma_{jd}^{-2})])^{-1}$, of dimension 
  $r \times r$. Inversion of this matrix is done efficiently by applying the
  Woodbury matrix identity.
  
  \section{Starting values}
  \subsection{Method 1}
  Starting value for $\sigma_d^2$ as in \cite{chipman_bart:_2010}. They consider a scaled-Chi squared distribution for the error variance with degrees of freedom 3 and scale chosen such that the $q*100$th percentile matches the error variance in the data $s^2(\mathbf{y}_d)$. As starting value for $\sigma_d^2$ we take the mode of this distribution: $\hat{\sigma}_d^2 = 3a/5$, with $a$ the solution to:
  $$
  \Gamma\(\frac{3}{3}\) \[1 - q - F(3a/(2s^2(\mathbf{y}_d)); 3/2, 1) \] = 0,
  $$
  where $F(x; 3/2, 1)$ is the CDF of a gamma function with shape $3/2$ and scale $1$. In \cite{moran_variance_2018}, REFERENCE to Rockova and George (2018), and \cite{chipman_bart:_2010}, $q=0.9$ is suggested. We find that $q \in ( 0.9, 0.95 )$ gives good results.
  
  \subsection{Method 2}
  We generate starting values as follows. Consider the $\sigma_d^2$ and $\gamma_d^2$ as fixed and estimate them by MML of the regular ridge model:
  $$
  \hat{\sigma}_d^2, \hat{\gamma}_d^2= \underset{\sigma_d^2,\gamma_d^2}{\argmax} \int_{\bm{\beta}} p(\mathbf{y}_d | \bm{\beta}, \sigma_d^2) p(\bm{\beta} | \gamma_d^2, \sigma_d^2) d\bm{\beta}.
  $$
  The regular ridge model is conjugate, so MML is relatively simple. Next we estimate a common inverse Gaussian prior for the $\gamma_d^2$ by considering the estimates to be samples from the prior:
  \begin{align*}
  \hat{\theta} & = n^{-1} \sum_{d=1}^D \hat{\gamma}_d^2 / n, \\
  \hat{\lambda} & = n \left\{\sum_{d=1}^D \[ (\hat{\gamma}_d^2)^{-1} - \hat{\theta}^{-1}\]\right\}^{-1}.
  \end{align*}
  We estimate the mode of the $\hat{m}=\hat{\gamma}_d^2$ by kernel density estimation and equate it to the theoretical mode of the generalized inverse Gaussian distribution with the estimated $\hat{\theta}$ and $\hat{\lambda}$. We solve to find a common $\delta$:
  $$
  \hat{\delta} = \frac{\hat{m}^2 \hat{\lambda}}{\hat{\theta}^2} + (p + 3) \hat{m}.
  $$
  Lastly, we consider the $\hat{\sigma}^2_d$ fixed samples from the inverse Gamma distribution to estimate a common scale:
  $$
  \hat{\zeta} = \frac{D(n + p + 1)}{2 \sum_{d=1}^D[(\hat{\sigma}_d^2)^{-1}]}.
  $$
	
  \bibliographystyle{author_short3}  
	\bibliography{refs}

\end{document}
