% document settings
\documentclass[10pt,letterpaper]{beamer} %
\usetheme{CambridgeUS}
\usefonttheme{professionalfonts}

% loading packages
\usepackage{amsmath,verbatim,bm,caption,soul,graphicx,url}
\usepackage[plain]{algorithm}
\usepackage{algorithmic}
\usepackage[export]{adjustbox}
\usepackage[american]{babel}
\usepackage[backend=bibtex,citestyle=authoryear]{biblatex}
\usepackage[outdir=./]{epstopdf}
% \usepackage[outdir=../figs/]{epstopdf}

% beamer settings
\addtobeamertemplate{block begin}{\setlength\abovedisplayskip{0pt}}

\setbeamertemplate{sidebar right}{}% or get rid of navigation entries there somehow
\addtobeamertemplate{footline}{\hfill\usebeamertemplate***{navigation symbols}%
    \hspace*{0.1cm}\par\vskip 2pt}{}
\setbeamerfont{caption}{size=\scriptsize}
\makeatletter
\setbeamertemplate{headline}{%
	\begin{beamercolorbox}[ht=2.25ex,dp=1ex]{section in head/foot}
		\insertnavigation{\paperwidth}
	\end{beamercolorbox}%
}%
\makeatother

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Em}{\mathbf{E}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\Lm}{\mathbf{L}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

% content settings
\author[Magnus M\"unch]{Magnus M. M\"unch \\[3mm] \footnotesize{Joint work with Mark A. van de Wiel, Sylvia Richardson, and Gwena{\"e}l G. R. Leday}}
\title[EB for drug efficacy prediction]{Empirical Bayes for drug efficacy prediction in cell lines}
\institute[LU, AUMC, \& MRC BSU]{Leiden University, Amsterdam UMC, and MRC Biostatistics Unit}
\date{\today}
\bibliography{refs} % unfortunately custom bibliography doesn't work with biblatex

\begin{document}

% load packages and set options
<<include=FALSE>>=
library(cambridge)
library(statmod)
opts_knit$set(root.dir="..", base.dir="../figs")
opts_chunk$set(fig.align='center', echo=FALSE, fig.path="../figs/")
@

% read figure code
<<figures, include=FALSE>>=
read_chunk("code/figures.R")
@

  \begin{frame}
    \titlepage % Print the title page as the first slide
    \begin{figure}
      \raisebox{-0.5\height}{\includegraphics[width=0.3\linewidth]{../figs/logo-universiteitleiden-cmyk.pdf}}
		  \raisebox{-0.5\height}{\includegraphics[width=0.3\linewidth]{../figs/Amsterdam_UMC_UK_Logo_CMYK_U_150_dpi.jpg}}
		  \raisebox{-0.5\height}{\includegraphics[width=0.4\linewidth,trim={0 0.5cm 14cm 0},clip]{../figs/MRC_BSU_Cambridge_colour_profprint_A4.eps}}
	  \end{figure}
  \end{frame}
  
  \section{The challenge}
  \begin{frame}
    \frametitle{Motivation}
    \begin{figure}[h]
      {\centering \includegraphics[width=1\linewidth]{../figs/{F2.large}.jpg}}
			{\scriptsize{Source:~\cite{goodspeed_tumor-derived_2016}}}
		\end{figure}
  \end{frame}
  
  \begin{frame}
    \frametitle{Challenges}
    \begin{itemize}
      \item \textbf<2>{number of treatments \only<1>{$D$} \only<2>{$\bm{D}$} large}
			\item \only<1>{$p > n$} \only<2>{$\bm{p > n}$}
			\item translation from cell lines to patient
			\item quantification of drug efficacy
			\item ...
		\end{itemize}
  \end{frame}
  
  \begin{frame}
    \frametitle{Opportunities}
    \begin{columns}
	    \begin{column}{0.45\linewidth}
		    \begin{itemize}
          \item Highly structured setting
			    \item Correlated treatments
			    \item Extra information on drugs
			    \item ...
		    \end{itemize}
	    \end{column}
	    \begin{column}{0.45\linewidth}
	      \only<1>{\begin{figure}[h]
          \centering
			    \includegraphics[width=0.5\linewidth]{../figs/pubchem.png}
		    \end{figure}
		    \begin{columns}
		      \begin{column}{0.5\linewidth}
<<drug_effect, include=TRUE, echo=FALSE, fig.align="center", out.width="100%">>=
curve(dnorm(x), -4, 4, main="", yaxt='n', xaxt="n", bty="n", ann=FALSE, cex=10, asp=10,
      lwd=4)
axis(1, line=-6, cex=100, lwd=4)
title(xlab="Drug effect", line=-3.3, cex=10, cex.lab=1.5)
@
		        \begin{figure}[h]
              \centering
			        \includegraphics[width=0.9\linewidth]{../figs/FDA_approval.png}
		        \end{figure}
		      \end{column}
		      \begin{column}{0.5\linewidth}
		        \begin{figure}[h]
              {\centering \includegraphics[width=1\linewidth]{../figs/300px-SMILES.png}}
			        {\scriptsize{Source:~\cite{noauthor_simplified_2019}}}
		        \end{figure}
		      \end{column}
		    \end{columns}
		    }
		    \begin{enumerate}[$\Rightarrow$]
          \item<2-3> Simple models possible
			    \item<2-3> \textbf<3>{Learning across treatments}
			    \item<2-3> \textbf<3>{Incorporate to aid estimation}
			    \item<2-3> ...
		    \end{enumerate}
	    \end{column}
		\end{columns}
		\bigskip
		\only<3> {\centering \textbf{Suggest Bayesian model}}
  \end{frame}
  
  \section{Our solution}
  \begin{frame}
    \frametitle{Model}
    \textbf{Likelihood} for cell line $i$'s response $y_i$ to drug $d$, using $p$ omics features $\mathbf{x}_{ij}$.
    \begin{align*}
		  \only<1>{y_{id} & = \beta_{0d} + \x_i \tr \bbeta_d + \mathbf{z}_i \tr \mathbf{u}_d + \epsilon_{id},} 
		  \only<2>{y_{id} & = \beta_{0d} + \x_i \tr \bbeta_d \phantom{ + \mathbf{z}_i \tr \mathbf{u}_d} + \epsilon_{id},} \\
		  \epsilon_{id} & \sim \mathcal{N}(0, \sigma_d^2),
		\end{align*}
		\textbf{Model parameters}
		\begin{align*}
		  \beta_{jd} & \sim \mathcal{N} (0, \gamma_d^2 \sigma_d^2), \\
		  \only<1>{\mathbf{u}_{d} & \sim \mathcal{N}_T (\mathbf{0}, \bm{\Xi}_d \sigma_d^2),}
		\end{align*}
	  \textbf{Error and prior variance}
		\begin{align*}
			\sigma_d^{2} & \sim 1/\sigma_d^{3}, \\
			\gamma_d^{2} & \sim \mathcal{IG}(\theta_d, \lambda_d)\only<1>{,}\only<2>{.} \\
			\only<1>{\bm{\Xi}_d & \sim \mathcal{W}^{-1}_T(\bm{\Omega}, \nu).}
		\end{align*}
  \end{frame}
  
  \begin{frame}
    \frametitle{Inverse Gaussian distribution}
<<dens_igaussian_igamma, fig.cap="Several densities of the inverse Gaussian and inverse Gamma families", out.width="85%", fig.asp=2/3>>=
digauss <- function(x, theta, lambda, eta) {
  sqrt(lambda/(x^3*2*pi))*exp(-lambda*(x - theta)^2/(2*theta^2*x))
}

digamma <- function(x, theta, lambda, eta) {
  alpha <- eta/2
  beta <- lambda/2
  beta^alpha/gamma(alpha)*x^(-alpha - 1)*exp(-beta/x)
}

eta <- c(1, 2, 1, 1, 5)
lambda <- c(1, 1, 2, 1/2, 2)
theta <- c(1000, 1000, 1, 1, 1)

labels <- c("inv. Gaussian", "inv. Gamma")
col <- c(1, 2)

opar <- par(no.readonly=TRUE)
par(mar=opar$mar*c(1, 1.3, 1, 1), cex=1.2, lwd=2)
layout(matrix(c(rep(c(1, 1, 2, 2, 3, 3), 2), rep(c(0, 4, 4, 5, 5, 0), 2)),
                 nrow=4, ncol=6, byrow=TRUE))
for(i in 1:length(eta)) {
  modes <- c(theta[i]*(sqrt(1 + 9*theta[i]^2/
                            (4*lambda[i]^2)) - 3*theta[i]/(2*lambda[i])),
             lambda[i]/(eta[i] + 2))
  ylim <- c(0, max(digauss(modes[1], theta[i], lambda[i], eta[i]),
                   digamma(modes[2], theta[i], lambda[i], eta[i])))
  curve(digauss(x, theta[i], lambda[i], eta[i]), 0.001, 5, n=1000,
        ylim=ylim, ylab="Density", "x", col=col[1], 
        main=bquote(eta==.(eta[i])*","~lambda==.(lambda[i])*","~theta==.(
          theta[i])))
  curve(digamma(x, theta[i], lambda[i], eta[i]), add=TRUE, 
        col=col[2], n=1000)
}
legend("topright", legend=labels, lty=1, col=col, seg.len=1)
par(opar)
@
  \end{frame}
  
  \begin{frame}
    \frametitle{Normal inverse Gaussian model}
<<dens_igaussian_marginalbeta, fig.cap="Marginal $\\beta_j$ prior for several choices of $\\gamma^2_d$ hyperpior, scaled to $\\V(\\beta_j)=1$.", out.width="80%", fig.asp=2/3>>=
library(GeneralizedHyperbolic)
# all scaled such that variance is one
sigma <- 1
theta <- 1/sigma^2
lambda <- c(0.1, 2)
lambda1 <- sqrt(2)
col <- c(1:4)
lty <- c(1:4)
labels <- as.expression(c(bquote("IG, "~lambda==.(lambda[1])~", "~
                                   theta==.(theta)),
                          bquote("IG, "~lambda==.(lambda[2])~", "~
                                   theta==.(theta)),
                          "Point mass (ridge)", "Exponential (lasso)"))
dprior <- function(x, lambda, theta, sigma) {
  dnig(x, 0, sigma*sqrt(lambda), sqrt(lambda/(theta*sigma)), 0)
}
dlasso <- function(x, lambda1) {
  0.5*lambda1*exp(-lambda1*abs(x))
}

opar <- par(no.readonly=TRUE)
par(mar=opar$mar*c(1, 1.3, 1/1.3, 1), cex.lab=1.5, lwd=3)
curve(dprior(x, lambda[1], theta, sigma), -3, 3, ylab=expression(p(beta[j])), 
      xlab=expression(beta[j]), n=1000, 
      col=col[1], lty=lty[1])
curve(dprior(x, lambda[2], theta, sigma), add=TRUE, n=1000, 
      col=col[2], lty=lty[2])
curve(dnorm(x, 0, 1), add=TRUE, n=1000, col=col[3], lty=lty[3])
curve(dlasso(x, lambda1), add=TRUE, n=1000, col=col[4], lty=lty[4])
legend("topright", legend=labels, col=col, lty=lty, 
       title="Hyperprior")
par(opar)
@
  \end{frame}
  
  \begin{frame}
    \frametitle{Hyperparameters}
    \textbf{How to choose $\theta_d$ and $\lambda_d$?} We want to somehow:
    \begin{columns}
      \begin{column}{0.5\linewidth}
        \begin{itemize}
          \item Learn across drugs
          \item Be objective
          \item Incorporate drug information $\mathbf{c}_d$
        \end{itemize}
      \end{column}
      \begin{column}{0.5\linewidth}
        
      \end{column}
    \end{columns}
    \bigskip
    \only<2>{
      \textbf{Empirical Bayes} (EB) \\
      Let $\theta_d = \E_{\gamma_d^2}(\gamma_d^2) = (\mathbf{c}_d \tr \bm{\alpha})^{-1}$ $\Rightarrow$ estimate $\bm{\alpha}$ and $\lambda_d$ \\
      \bigskip
      \textbf{Note:} smaller dimension of $\bm{\alpha}$ simplifies estimation of $\theta_d$
    }
  \end{frame}
  
  \begin{frame}
    \frametitle{Empirical Bayes}
    \textbf{Canonical EB}: Maximise the marginal likelihood (MML)
    \begin{itemize}
      \item analytical MML
      \item Laplace approximation
      \item using EM
      \begin{itemize}
        \item MCMC
        \item \textbf<2>{Variational Bayes} (VB)
      \end{itemize}
    \end{itemize}
    \only<1>{
      \bigskip
      $$
      \text{ML}(\bm{\alpha})=\int_{\bm{\beta}} \mathcal{L} (\y | \bm{\beta}) \pi_{\bm{\alpha}}(\bm{\beta}) d \bm{\beta}
      $$
    }
    \only<2>{
    \begin{block}{Empirical Bayes EM}
      $$
      \bm{\alpha}^{(l+1)}, \bm{\lambda}^{(l+1)} = \underset{\bm{\alpha}, \bm{\lambda}}{\argmax} \, \E_{\bbeta | \y} \[ \log \L_{\bm{\alpha}, \bm{\lambda}}(\y, \bbeta) | \bm{\alpha}^{(l)}, \bm{\lambda}^{(l)} \]
      $$
    \end{block}
    }
  \end{frame}
  
  \begin{frame}
    \frametitle{Variational Bayes}
    $\E_{\bbeta | \y}$ hard to compute, so we approximate:
    $$
    p(\bbeta , \bm{\gamma}^2, \bm{\sigma}^2 | \y) \approx Q(\cdot) = q(\bbeta)q(\bm{\gamma}^2)q(\bm{\sigma}^2)
    $$
    Variational calculus tells us $D_{KL}(Q || P)$ is minimised by:
    \begin{align*}
    q(\bbeta) & \overset{D}{=} \prod_{d=1}^D \mathcal{N}_p(\bm{\mu}_d,\bm{\Sigma}_d), \\
    q(\bm{\gamma}^2) & \overset{D}{=} \prod_{d=1}^D \mathcal{GIG}(-1/2,\lambda_d/\theta^2_d,\delta_d), \text{ with } \theta_d = (\mathbf{c}_d \tr \bm{\alpha})^{-1} \\
    q(\bm{\sigma}^2) & \overset{D}{=} \prod_{d=1}^D \Gamma^{-1} ((n + p + 1)/2, \zeta_d).
    \end{align*}
    Requires iteratively updating $\bm{\mu}_d$, $\bm{\Sigma}_d$, $\delta_d$, and $\zeta_d$
  \end{frame}
  
  \begin{frame}
    \frametitle{Full algorithm}
    \begin{columns}
      \begin{column}{0.5\linewidth}
        \begin{algorithm}[H]
        \caption{Empirical-variational Bayes EM}
          \begin{algorithmic}[1]
            \WHILE{EB not converged}
              \WHILE{VB not converged}
                \FOR{$d=1,\dots,D$}
                  \STATE {update $\bm{\mu}_d$, $\bm{\Sigma}_d$, $\delta_d$, and $\zeta_d$}
                \ENDFOR
              \ENDWHILE
              \STATE {update $\bm{\alpha}$, and $\bm{\lambda}$}
            \ENDWHILE
          \end{algorithmic}
        \end{algorithm}
      \end{column}
      \begin{column}{0.5\linewidth}
      \smallskip
      \bigskip \\
      $\Bigg\}$ E-step \\
      \bigskip
      \medskip
      M-step
      \end{column}
    \end{columns}
  \end{frame}
  
  \begin{frame}
    \frametitle{Output}
    \textbf{Hyperparameters} $\hat{\lambda}_d$ (scale) and $\hat{\theta}_d=(\mathbf{c}_d \tr \hat{\bm{\alpha}})^{-1}$ (location)
    \begin{itemize}
      \item adaptively estimated (non-informative if bad prior info)
      \item indication of important drugs
      \item indication of important drug `covariates'
    \end{itemize}
    \begin{columns}
      \begin{column}{0.45\linewidth}
<<eb_effect1, include=TRUE, echo=FALSE, fig.align="center", out.width="80%", fig.asp=2/3>>=
colors <- sp::bpy.colors(4)[-c(1, 4)]
curve(dinvgauss(x, 1, 1), 0, 5, col=colors[1], lwd=4, main="", yaxt="n", xaxt="n", bty="n", ann=FALSE)
curve(dinvgauss(x, 5, 3), 0, 5, add=TRUE, col=colors[2], lwd=4, main="", yaxt="n", xaxt="n", bty="n", ann=FALSE)
title(xlab=expression(gamma[d]^2), cex=10, cex.lab=3)
legend("topright", legend=c(expression(hat(lambda)[d]~","~hat(theta)[d]~"small"), expression(hat(lambda)[d]~","~hat(theta)[d]~"large")), lty=1, col=colors, lwd=4, cex=3, box.col=NA)
@
      \end{column}
      \begin{column}{0.1\linewidth}
        $$\Rightarrow$$
      \end{column}
      \begin{column}{0.45\linewidth}
<<eb_effect2, include=TRUE, echo=FALSE, fig.align="center", out.width="80%", fig.asp=2/3>>=
colors <- sp::bpy.colors(4)[-c(1, 4)]
curve(dnorm(x, 0, 1), -7, 7, col=colors[1], lwd=4, main="", yaxt="n", xaxt="n", bty="n", ann=FALSE)
curve(dnorm(x, 0, 3), -7, 7, col=colors[2], lwd=4, add=TRUE, main="", yaxt="n", xaxt="n", bty="n", ann=FALSE)
title(xlab=expression(beta[jd]^2~"|"~gamma[d]^2), cex=10, cex.lab=3)
@
      \end{column}
    \end{columns}
    \textbf{Posterior} $p(\bbeta_d | \y)$ parameters $\bm{\mu}_d$ and $\bm{\Sigma}_d$
    \begin{itemize}
      \item predict future cell line data
      \item feature selection (with caution)
    \end{itemize}
  \end{frame}

  \section{Simulations}
  \begin{frame}
    \frametitle{Motivation}
    \textbf{How well does the EB estimation work?} \\ 
    We consider a simple setting with $K$ classes of drugs:
    $$
	  c_{dk} = \begin{cases}
	    1 & \text{if } \text{class}_d = k, \\
	    0 & \text{otherwise}.
	  \end{cases}
	  $$
	  and compare to
	  \begin{itemize}
	    \item `default' inverse Gamma prior (scaled to match inverse Gaussian):
        $$
        \gamma_d^{2} \sim \Gamma^{-1} (\eta_d/2, \lambda_d/2) = \Gamma^{-1} (\eta_{\text{class}_d}/2, \lambda_{\text{class}_d}/2).
        $$
      \item non-conjugate models $p(\beta_{jd}|\sigma_d^2,\gamma_d^2)=p(\beta_{jd}|\gamma_d^2)$ \\
        (conjugate model: \textit{a priori} dependence of $\beta_{jd}$ on $\sigma_d^2$)    
    \end{itemize}
  \end{frame}
  
  \begin{frame}
    \frametitle{Setup}
    We use $n=100$, $p=100$, $D=100$, $\text{class}=4$, $\forall d: \lambda_d,\sigma_d^2=1$, $x_{ij} \overset{ind}{\sim} \mathcal{N}(0,1)$
    and simulate $\gamma_d^2$ from
    \begin{enumerate}
      \item inv. Gaussian model (correct) with $\bm{\alpha} =\begin{bmatrix} 1 & 2 & 3 & 4 \end{bmatrix} \tr$ and $\theta_d =(\mathbf{c}_d \tr \bm{\alpha})^{-1}$
      \item inv. Gamma model (incorrect) with $\bm{\alpha} = \begin{bmatrix} 3 & 4 & 5 & 6 \end{bmatrix} \tr$ and $\eta_d =\mathbf{c}_d \tr \bm{\alpha}$
    \end{enumerate}
    \bigskip
    \textbf{Note:} results in same $\theta_d$
  \end{frame}
  
  \begin{frame}
    \frametitle{Prior means 1}
<<boxplot_igaussian_res1_prior_mean, cache=FALSE, echo=FALSE, fig.cap="Prior mean estimates", out.width="100%", fig.align="center", fig.asp=2/3>>=
@
  \end{frame}
  
  \begin{frame}
    \frametitle{Model parameters 1}
<<boxplot_igaussian_res1_mu_mse, cache=FALSE, echo=FALSE, fig.cap="MSE of model parameters", out.width="100%", fig.align="center", fig.asp=2/3>>=
@
  \end{frame}
  
  \begin{frame}
    \frametitle{Prior means 2}
<<boxplot_igaussian_res2_prior_mean, cache=FALSE, echo=FALSE, fig.cap="Prior mean estimates", out.width="100%", fig.align="center", fig.asp=2/3>>=
@
  \end{frame}
  
  \begin{frame}
    \frametitle{Model parameters 2}
<<boxplot_igaussian_res2_mu_mse, cache=FALSE, echo=FALSE, fig.cap="MSE of model parameters", out.width="100%", fig.align="center", fig.asp=2/3>>=
@
  \end{frame}
  
  \begin{frame}
    \frametitle{Convergence issues}
<<lines_igaussian_res2_igauss_conj_convergence, cache=FALSE, echo=FALSE, fig.cap="a) lambda, b) prior mean, and c) prior variance estimates", out.width="50%", fig.align="center">>=
@
  \end{frame}
  
  \section{Outlook}
  \begin{frame}
    \frametitle{Future work}
    Lots to do
    \begin{itemize}
      \item solve convergence issues
      \item more simulations
      \item implement tissue effects
      \item investigate real data (GDSC and CCLE)
      \item write award-winning nature/science paper
    \end{itemize}
  \end{frame}
  
  \begin{frame}
    \frametitle{Thank you}
    \centering {\usebeamercolor[fg]{structure} \textbf{Material}} \\
    \textit{Paper:} Working progress... \\
    \textit{R-package (devel):} \url{https://github.com/magnusmunch/cambridge}
    \bigskip

    \centering {\usebeamercolor[fg]{structure} \textbf{Website}} \\
    \textit{Group:} \url{https://www.bigstatistics.nl/} \\
    \textit{Personal:} \url{http://pub.math.leidenuniv.nl/~munchmm/}
    \bigskip

    \centering {\usebeamercolor[fg]{structure} \textbf{Acknowledgements}} \\
    This research has received funding from the European Research Council under ERC Grant Agreement 320637
  \end{frame}
  
  \begin{frame}{Bibliography}
    \frametitle{References}
	  \printbibliography
	\end{frame}

\end{document}
