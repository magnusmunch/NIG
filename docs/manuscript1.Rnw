% set options
<<settings, include=FALSE, echo=FALSE>>=
opts_knit$set(root.dir="..", base.dir="../figs")
opts_chunk$set(fig.align='center', fig.path="../figs/", 
               echo=FALSE, cache=FALSE, message=FALSE, fig.pos='!ht')
knit_hooks$set(document=function(x) {
  sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed=TRUE)})
@

% read figure code
<<figures, include=FALSE>>=
read_chunk("code/figures.R")
@

\documentclass[a4paper,hidelinks]{article}
\usepackage{bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,
rotating,caption,subcaption,natbib,appendix,titlesec,multicol,hyperref,verbatim,
bbm,algorithm,algpseudocode,pgfplotstable,threeparttable,booktabs,mathtools,
dsfont,parskip,xr,tikz}
\externaldocument[sm-]{supplement1}
\usetikzlibrary{bayesnet}

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Em}{\mathbf{E}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\Lm}{\mathbf{L}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}

% settings
\pgfplotsset{compat=1.16}
\graphicspath{{../figs/}}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Drug sensitivity prediction with the normal inverse Gaussian prior 
informed by external data}
\date{\today}
\author{Magnus M. M\"unch$^{1,2}$\footnote{Correspondence to: 
\href{mailto:m.munch@amsterdamumc.nl}{m.munch@amsterdamumc.nl}}, Mark A. van de 
Wiel$^{1,3}$, Sylvia Richardson$^{3}$, \\ and Gwena{\"e}l G. R. Leday$^{3}$}

\begin{document}

	\maketitle
	
	\noindent
	1. Department of Epidemiology \& Biostatistics, Amsterdam UMC, VU University, 
	PO Box 7057, 1007 MB Amsterdam, The Netherlands \\
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\
	3. MRC Biostatistics Unit, Cambridge Institute of Public Health, Cambridge,
	United Kingdom
	
	\begin{abstract}
		{In precision medicine, a common problem is drug sensitivity prediction from
		cancer tissue cell lines. These types of problems entail modelling 
		multivariate drug responses on high dimensional molecular feature sets in 
		typically $>1000$ cell lines. The dimensions of the problem require 
		specialised models and estimation methods. In addition, external information
		on both the drugs and the features is often available. We propose to 
		model the drug responses through a linear regression with the normal
		inverse Gaussian prior. We let the prior depend on the external information,
		and estimate the model and external information dependence in an 
		empirical-variational Bayes framework. We demonstrate the usefullness of
		this model in both a simulated setting and in the publicly available
		Genomics of Drug Sensitivity in Cancer data.}
	\end{abstract}
	
	\noindent\textbf{Keywords}: Drug sensitivity; Empirical Bayes; 
	Genomics of Drug Sensitivity in Cancer (GDSC); Variational Bayes
	
	\noindent\textbf{Software available from}: 
	\url{https://github.com/magnusmunch/cambridge}
	
	\section{Introduction}
	Recently, promising results in precision medicine 
	have sparked an interest in cancer drug sensitivity prediction models
	\cite[]{iorio_landscape_2016}. Typically, these models 
	predict the drug sensitivity for new patients from a set of molecular 
	features.
	Development of such models is often done in well-characterised human cancer
	tissue cell lines. The current paper presents a novel drug sensitivity
	prediction model and an application to a real drug
	sensitivity data set.
	
	Development of such models from cell lines has proven to be difficult
	(see e.g., the DREAM 7 challenge in \cite{costello_community_2014}). 
	Difficulties arise, among others, from the dimensions of the problem. 
	Typically, the data contains
	hundreds of drugs, thousands of cell lines, and thousands of molecular 
	features. An example of a large database of drug responses and molecular
	features is the Genomics of Drug Sensitivity in Cancer (GDSC) data 
	\cite[]{yang_genomics_2013}, which we will further 
	investigate in Section \ref{sec:gdsc}. Other examples of such databases 
	include
	the Cancer Cell Line Encyclopedia (CCLE) \cite[]{li_landscape_2019} and 
	the US National Cancer Institute 60 human tumour cell line anticancer drug
	screen (NCI60) \cite[]{shoemaker_NCI60_2006}. The dimensions of these data
	prohibit the estimation of standard regression models and
	typically require some form of regularisation.
	
	The GDSC database contains additional information on both the drugs and 
	molecular features, such as the target 
	pathways and developmental stages of the drugs. Additional online
	repositories may provide extra information such as the molecular weight of
	the compounds or the publication signatures of the molecular features. In some
	cases, prior knowledge on the drug efficacies may be available,
	from previous experiments. We propose to include these possibly beneficial 
	information sources in the estimation of the sensitivity prediction models in 
	a data-driven manner. More specifically, we estimate a normal 
	inverse Gaussian model, where the extent of regularisation is estimated by an 
	adaptive empirical Bayes procedure, guided by the external information.
	
	We are not the first to work on drug sensitivity prediction models. Reviews on 
	the topic are \cite{azuaje_computational_2017} and \cite{ali_machine_2019}.
	\cite{zhao_structured_2019} and \cite{mai_composite_2019} consider a
	structured penalized multivariate regression approach. 
	\cite[]{aben_tandem:_2016} introduces a two-stage penalized regression 
	model that includes two different types of molecular features.
	\cite{ammad-ud-din_drug_2016} and \cite{costello_community_2014} tackle the
	problem through a multiple kernel learning approach.

  Our solution allows for the adaptive incorporation of the external 
  information on drugs and features. This is done by
  pooling, both across drugs and features. Estimation of the model
  is through computational feasible variational bayes approximations, while 
  empirical Bayes estimation of tuning parameters pools information across drugs
  and features in a data-driven manner.
  
	The rest of the paper is structured as follows. In Section 
	\ref{sec:model} we introduce our model, the estimation of which is detailed
	in Section \ref{sec:estimation}. In Section \ref{sec:gdsc} we analyse the
	GDSC data, and we end with a discussion in Section \ref{sec:discussion} on the 
	pros and cons of the proposed method.
	
	\section{Model}\label{sec:model}
	\subsection{Simultaneous equations model}\label{sec:SEM}
	Let $y_{id}$ be the continuous sensitivity measures 
	for cell lines $i=1,\dots, n$, and drug $d=1, \dots, D$. 
	We predict sensitivity from molecular features 
	$x_{ij}$, $j=1,\dots, p$, collected in 
	$\x_i = \begin{bmatrix} x_{i1} & \dots & x_{ip} \end{bmatrix} \tr$. 
	We assume that both covariates and responses have been centred per drug
	and regress the drug sensitivities on the molecular features:
	\begin{equation}\label{eq:likelihood}
	  y_{id} = \x_i \tr \bbeta_d + \epsilon_{id}, 
	  \text{ with } \epsilon_{id} \sim \mathcal{N}(0, \sigma_d^2),
	\end{equation}
	where the $p$-dimensional 
	$\bbeta_d = \begin{bmatrix} \beta_{1d} & \cdots & \beta_{pd} \end{bmatrix} 
	\tr$ are the drug-specific omics feature 
	effects. Note that (\ref{eq:likelihood}) gives rise to a system of 
	D linear regression equations.
	
	The cell lines used in drug response models are often taken from different
	tissues. In addition, other clinical covariates might be available. 
	To obtain unbiased feature effects, one may wish to account for these. 
	We do so by introducing unpenalized covariates, the $\beta_{jd}$ coefficients 
	of which are endowed with a flat prior. For the sake of clarity, in the
	following, such unpenalized covariates are omitted. However, the available
	software allows for their inclusion.
	
	\subsection{Bayesian prior model}
	We carry out inference by endowing the parameters with the following priors:
	\begin{subequations}\label{eq:prior}
		\begin{align}
		  \beta_{jd} | \gamma^2_{jd}, \tau_d^2, \sigma^2_d & \sim \mathcal{N}_{p} 
		  (0, \gamma_{jd}^2 \tau_d^2 \sigma_d^2), 
		  \label{eq:betaprior}\\
		  \gamma_{jd}^{2} & \sim \mathcal{IG}(\phi_{jd}, \lambda_{\text{feat}}), \\
		  \tau_d^{2} & \sim \mathcal{IG}(\chi_d, \lambda_{\text{drug}}), 
		  \label{eq:tauprior}\\
		  \sigma_d^{2} & \sim 1/\sigma_d^{3},
		\end{align}
	\end{subequations}
	where $\mathcal{IG}(\phi, \lambda)$ denotes an inverse Gaussian distribution 
	with mean $\phi$ and shape $\lambda$. Prior distributions of the form
	(\ref{eq:prior}) are often referred to as global-local shrinkage rules 
	\cite[]{bernardo_shrink_2011}, due to the multiplicative separation of the 
	prior 
	variance into a local component $\gamma_{jd}^2$ and a global component
	$\tau_d^2$. The local components are supposed to capture local variation in
	the model parameters $\beta_{jd}$, while the global component is supposed to
	capture the general trend in $\bm{\beta}_{d}$. For appropriate 
	local shrinkage in global-local 
	shrinkage models it is important  to account for 
  different noise levels $\sigma_d^2$ by scaling the $\beta_{jd}$ variances 
  accordingly.

	The normal inverse Gaussian (NIG) prior model was introduced in 
  \cite{barndorff-nielsen_hyperbolic_1978} and since 
  \cite{barndorff-nielsen_normal_1997} it is routinely applied in mathematical 
  finance (see, e.g., \cite{kalemanova_normal_2007}). Here we extend it with an
  additional global variance component $\tau_d^2$. Supplementary Material
  (SM) Section \ref{sm-sec:prior} contains more details on the NIG prior.
  To illustrate the effect of the NIG prior on the posterior mean, we consider 
  the prior reparametrised as in 
  \cite{carvalho_handling_2009}, i.e., in terms of shrinkage weights 
  $\kappa_{jd}=1/(1 + \gamma_{jd}^2) \in (0, 1)$.
  Under the (simplified) normal means model, i.e., 
  $\X= \begin{bmatrix} \x_1 & \cdots \x_n \end{bmatrix} \tr =\I_p$, with fixed 
  $\tau_d^2=\sigma_d^2=1$, the resulting conditional
  posterior mean for the $\beta_{jd}$ is
  $\E(\beta_{jd} | y_{jd}, \kappa_{jd}) = (1 - \kappa_{jd}) y_{jd}$. 
  Thus, $\kappa_{jd}=0$ implies no shrinkage of $\beta_{jd}$ and $\kappa_{jd}=1$
  implies full shrinkage towards zero. Figure 
  \ref{fig:dens_kappa} depicts the prior on $\kappa_{jd}$ implied by 
  several choices of $\beta_{jd}$ prior.
  
<<dens_kappa, fig.cap="Implied prior densities $\\pi(\\kappa_{jd})$ for the (a) NIG, (b) Student's $t$, and (c) lasso priors. Different line types correspond to different hyperparameter settings. The hyperparameter settings were chosen to show some possible, distinct shapes that each of the priors can take.", out.width="100%", fig.asp=1/3>>=
@
  
  Figure \ref{fig:dens_kappa} shows that, depending on the choice
  of hyperparameters, the NIG prior can behave similarly to the Student's $t$ 
  prior (decreasing form zero, with substantial mass close to 0), 
  but also rather differently. Our argumentation to model the 
  $\gamma^2_{jd}$ by an inverse Gaussian 
  distribution, as has been suggested in \cite{fabrizi_specification_2016} and 
  \cite{caron_sparse_2008}, is three-fold: (i) the NIG model is more flexible 
  than the lasso prior (as seen from Figure 
  \ref{fig:dens_kappa}), (ii) the NIG prior allows to model
  the means of the $\gamma_{jd}^2$ ($\phi_{jd}$) and $\tau_{jd}^2$ 
  ($\chi_{d}$) as a function of external data 
  more conveniently than the Student's $t$ prior, as explained in Section 
  \ref{sec:empiricalbayes}, and (iii) like the horseshoe, the NIG shrinkage
  weights prior can put mass both near 0 and 1, a desirable property of 
  shrinkage priors.

  A few remarks on the choice of error variance prior are justified here: 
  many authors endow error variance components with vague gamma priors. 
  \cite{gelman_prior_2006}, 
  among others, advises against this practice. The degree of `vagueness' has a 
  large influence on the posterior, while degree of `vagueness' is a difficult 
  parameter to set. This influence is especially pronounced if the likelihood is
  relatively flat, as may be reasonably expected in the large $p$, 
  small $n$ setting. We therefore model the error variance with Jeffrey's 
  objective prior 
  \cite[]{jeffreys_invariant_1946} that does not depend on any 
  subjective specification of hyperparameters. In the derivation of our 
  Jeffrey's prior
  for the error variance, we jointly consider an unknown mean and variance 
  \cite[]{kass_selection_1996}. This
  joint consideration results in the somewhat unorthodox $1/\sigma^3$ Jeffrey's
  prior.
	
	\subsection{External information}\label{sec:external}
	In drug sensitivity prediction models, external information on both the drugs
	and features is often available. Here, we assume this information to be 
	available as
	external feature `covariates' $\mathbf{c}_{jdg}$, for $g=1, \dots, G$, and 
	drug `covariates' $\mathbf{z}_{dh}$, for $h=1,\dots, H$. An example of a 
	(binary) feature covariate is target pathway presence, with $c_{jdg}=0$ if 
	gene $j$ is present in the 
	target pathway of drug $d$ and $c_{jdg}=1$ if it is not. An example of a 
	(ternary) drug
	covariate is developmental phase, with levels experimental phase, 
	clinical development, and approved by a governing agency.
	
	The external covariates come in through our mean models for 
	the $\gamma_{jd}^2$ and $\tau_d^2$ hyperpriors:
	$\phi_{jd} = (\mathbf{c}_{jd} \tr \bm{\alpha}_{\text{feat}})^{-1}$ and
	$\chi_d = (\mathbf{z}_{d} \tr \bm{\alpha}_{\text{drug}})^{-1}$, with 
	$\mathbf{c}_{jd} = \begin{bmatrix} c_{jd1} & \cdots & c_{jdG} \end{bmatrix}$
	and $\mathbf{z}_{d} = \begin{bmatrix} z_{d1} & \cdots & z_{dH} \end{bmatrix}$,
	where categorical external covariates are dummy coded.
	The model now requires hyperparameters $\bm{\alpha}_{\text{feat}}$, 
	$\lambda_{\text{feat}}$, $\bm{\alpha}_{\text{drug}}$, and $
	\lambda_{\text{drug}}$, which we estimate in a data-driven manner (see Section
	\ref{sec:empiricalbayes}).
  
  A representation of our model as a Bayesian DAG is given in Figure
  \ref{fig:dag}. We note that in many settings, the set of features might be
  different for different drugs. In that case the covariates are indexed by
  the drug $d$: $\X^d$, a trivial extension of model 
  (\ref{eq:likelihood}) and (\ref{eq:prior}). This extension is included in the
  available software, but for clarity it is omitted in the following. 
  Furthermore, several additional extensions of the model are 
  described in [! ref to second paper here].
  
  \begin{figure}
    \centering
    \tikz{ %
      % Y
      \node[obs] (y) {$y_{id}$}; %
      \factor[right=of y, yshift=1.5cm, xshift=-0.1cm] {y-f} 
      {above:$\mathcal{N}$} {} {} ; %
      
      % X
      \node[det, above=of y, yshift=1cm] (x) {$x_{ij}$}; %
      
      % sigma2
      \node[latent, right=of y] (sigma2) {$\sigma_d^2$}; %
      
      % beta
      \node[latent, right=of x] (beta) {$\beta_{jd}$}; %
      \factor[right=of beta, yshift=-1.4cm, xshift=-0.1cm] {beta-f} 
      {above:$\mathcal{N}$} {} {} ; %
      
      % gamma2
      \node[latent, right=of beta] (gamma2) {$\gamma_{jd}^2$}; %
      \node[const, right=of gamma2, yshift=0.5cm] (phi) {$\phi_{jd}$}; %
      \node[const, right=of gamma2, yshift=-0.5cm] (lambdaf) 
      {$\lambda_{\text{feat}}$}; %
      \factor[right=of gamma2] {gamma2-f} {above:$\mathcal{IG}$} {phi,lambdaf} 
      {gamma2} ; %
      \node[det, right=of gamma2, xshift=2cm, yshift=0.5cm] (c) {$c_{jdg}$}; %
      
      % tau2
      \node[latent, right=of sigma2] (tau2) {$\tau_d^2$}; %
      \node[const, right=of tau2, yshift=0.5cm] (chi) {$\chi_{d}$}; %
      \node[const, right=of tau2, yshift=-0.5cm] (lambdad) 
      {$\lambda_{\text{drug}}$}; %
      \factor[right=of tau2] {tau2-f} {above:$\mathcal{IG}$} {chi,lambdad} 
      {tau2} ; %
      \node[det, right=of tau2, xshift=2cm, yshift=0.5cm] (z) {$z_{dh}$}; %
      
      % edges
      \factoredge {x,beta,sigma2} {y-f} {y} ; %
      \factoredge {sigma2,gamma2,tau2} {beta-f} {beta} ; %
      \edge {c} {phi}; %
      \edge {z} {chi}; %
      
      % plates
      \plate{plateg} {(c)} {$g=1,\dots,G$}
      \plate{plateh} {(z)} {$h=1,\dots,H$}
      \plate{platei} {(y) (x)}{$i=1,\dots,n$}; %
      \plate{platej} {(x) (beta) (gamma2) (phi) (lambdaf) (c) (plateg)}
      {$j=1,\dots,p$}; %
      \plate{plated} 
      {(y) (x) (sigma2) (beta) (gamma2) (tau2) (phi) (lambdaf)
      (chi) (lambdad) (c) (z) (platei) (platej) (plateg) (plateh)}
      {$d=1,\dots,D$}; %
    }
    \caption{Hierarchical representation of the drug sensitivity prediction 
    model. Grey circles 
    represent observed variables, white circles represent unobserved variables,
    squares represent fixed data, and unenclosed letters are parameters to be
    estimated. Cell lines are indexed by $i$, features
    by $j$, drugs by $d$, drug covariates by $h$, and feature covariates by 
    $g$. The $y_{id}$ are the drug sensitivities, $x_{ij}$ the 
    molecular features, 
    $c_{jdg}$ the external feature covariates, $z_{dh}$ the external drug
    covariates, $\beta_{jd}$ the regression coefficients, $\sigma_d^2$ the 
    error variances, $\tau_d^2$ and $\gamma_{jd}^2$ the drug and feature
    specific variance components, respectively, and $\phi_{jd}$, 
    $\lambda_{\text{feat}}$, $\chi_{d}$, and $\lambda_{\text{drug}}$ the 
    hyperparameters.}
    \label{fig:dag}
  \end{figure}

	\section{Estimation}\label{sec:estimation}
	\subsection{Variational Bayes}\label{sec:variationalbayes}
	The posterior corresponding to the model described in (\ref{eq:likelihood})
	and (\ref{eq:prior}) is not available in closed form. 
	To avoid computationally intensive markov chain Monte Carlo (MCMC) 
	algorithms, we approximate the joint posterior by variational Bayes (see 
	\cite{blei_variational_2017} for a review), where the approximate
	posterior density factorises as: $p(\bbeta_d, \bm{\gamma}^2_d, \tau_{d}^2, 
	\sigma_d^2 | \y_d) \approx 
	Q_d(\cdot) = q(\bbeta_d) \cdot q(\bm{\gamma}_{d}^2) \cdot q(\tau_{d}^2) 
	\cdot q(\sigma_d^2)$, where $\bm{\gamma}_{d}^2 = 
	\begin{bmatrix} \gamma_{1d}^2 & \cdots & \gamma_{pd}^2 \end{bmatrix} \tr$. For
  notational convenience, we slightly abuse notation and let $q(\cdot)$ denote
	different densities for different inputs.
	Under such a factorisation, the marginal variational posteriors that 
	minimise the Kullback-Leibler divergence of the true posterior to the 
	variational Bayes approximation \cite[]{neal_view_1998} are given by:
	\begin{align*}
    q(\bm{\beta}_d) & \overset{D}{=} \mathcal{N}_{p} 
    (\bm{\mu}_d, \bm{\Sigma}_d), \\
    q(\bm{\gamma}_{d}^2) & \overset{D}{=} \prod_{j=1}^p \mathcal{GIG}
    \(-1, \lambda_{\text{feat}}/\phi_{jd}^2, \delta_{jd}\), \\
    q(\tau^2_d) & \overset{D}{=} \mathcal{GIG} \(-\frac{p + 1}{2}, 
    \lambda_{\text{drug}}/\chi_d^2, \eta_d\), \\
    q(\sigma^2_d) & \overset{D}{=} \Gamma^{-1} \(\frac{n + p + 1}{2}, \zeta_d\).
  \end{align*}
	See SM Section \ref{sm-sec:vbderivations} for the derivations. The variational
	parameters $\bm{\mu}_d$, $\bm{\Sigma}_d$, $\delta_{jd}$, $\eta_d$, and
	$\zeta_d$ contain cyclic dependencies and are iteratively 
	updated by:
	\begin{subequations}\label{eq:vbequations}
    \begin{align}
      \bm{\Sigma}_d^{(h+1)} & = (a_d^{(h)})^{-1} 
      \[\X \tr \X + g_{d}^{(h)} \diag(b_{jd}^{(h)})\]^{-1}, \\
      \bm{\mu}_d^{(h+1)} & = \[\X \tr \X + g_{d}^{(h)} 
      \diag(b_{jd}^{(h)})\]^{-1} 
      \X \tr \y_d, \\
      \delta_{jd}^{(h+1)} & = a_d^{(h)} g_{d}^{(h)} \[(\bmu^{(h+1)}_{jd})^2 + 
      (\bSigma^{(h + 1)}_d)_{jj}\] + \lambda_{\text{feat}}, \\ 
      \eta_d^{(h+1)} & = a_d^{(h)} \sum_{j=1}^{p} b_{jd}^{(h + 1)}
      \[ (\bmu^{(h+1)}_{jd})^2 + (\bSigma^{(h + 1)}_d)_{jj} \] + 
      \lambda_{\text{drug}}, \\
      \zeta_d^{(h+1)} & = \frac{1}{2} \bigg[\mathbf{y}_d \tr \mathbf{y}_d -
      2 \mathbf{y}_d \tr \X \bm{\mu}_d^{(h+1)} + 
      \trace ( \X \tr \X \bm{\Sigma}_d^{(h+1)}) + 
      (\bm{\mu}_d^{(h+1)}) \tr \X \tr \X \bm{\mu}_d^{(h+1)} \\
      & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + 
      g_{d}^{(h + 1)} \trace \[ \diag (b_{jd}^{(h+1)}) \bm{\Sigma}_d^{(h+1)}\] + 
      g_{d}^{(h + 1)} (\bm{\mu}_d^{(h+1)}) \tr \diag (b_{jd}^{(h+1)}) 
      \bm{\mu}_d^{(h+1)}\bigg],
    \end{align}
  \end{subequations}
  until convergence, where $\y_d = \begin{bmatrix} y_{1d} & \cdots & y_{nd}
  \end{bmatrix} \tr$. Here, we set
  \begin{align}
    a_d^{(h)} & =\E_{Q^{(h)}}(\sigma_d^{-2})=(n + p + 1)/(2 \zeta_d^{(h)}), 
    \nonumber\\
    b_{jd}^{(h)} & = \E_{Q^{(h)}}(\gamma_{jd}^{-2}) = 
    \sqrt{\frac{\lambda_{\text{feat}}}{\phi_{jd}^2 \delta_{jd}^{(h)}}}
    \frac{K_0\(\sqrt{\delta_{jd}^{(h)}\lambda_{\text{feat}}/\phi_{jd}^2}\)}
    {K_1\(\sqrt{\delta_{jd}^{(h)}\lambda_{\text{feat}}/\phi_{jd}^2}\)} + 
    \frac{2}{\delta_{jd}^{(h)}}, \label{eq:ratiomodifiedbessel} \\
    g_d^{(h)} & =\E_{Q^{(h)}}(\tau_d^{-2})=
    \sqrt{\frac{\lambda_{\text{drug}}}{\chi_{d}^2 \eta_{d}^{(h)}}}
    \frac{K_{(p-1)/2}\(\sqrt{\eta_{d}^{(h)}\lambda_{\text{drug}}/\chi_{d}^2}\)}
    {K_{(p+1)/2}\(\sqrt{\eta_{d}^{(h)}\lambda_{\text{drug}}/\chi_{d}^2}\)} + 
    \frac{p + 1}{\eta_{d}^{(h)}}.
    \nonumber
  \end{align}
  where $K_{\nu}(x)$ denotes the modified Bessel function of the second kind.
  A method for fast and numerically stable calculation of ratios of
  modified Bessel functions of the second kind, as in 
  (\ref{eq:ratiomodifiedbessel}), is given in SM Section 
  \ref{sm-sec:ratiosmodifiedbessels}.
  
  \subsection{Empirical Bayes}\label{sec:empiricalbayes}
  We parametrised the prior 
  mean of the $\gamma_{jd}^2$ as $\phi_{jd}=(\mathbf{c}_{jd} \tr 
  \bm{\alpha}_{\text{feat}})^{-1}$ and the prior mean of $\tau_d^2$ as
  $\chi_{d}=(\mathbf{z}_{d} \tr \bm{\alpha}_{\text{drug}})^{-1}$.
  This parametrisation allows us to include feature and drug covariates,
  both continuous and discrete, into the 
  model. Additionally, it reduces the number of hyperparameters from 
  $pD$ to $|\bm{\alpha}_{\text{feat}}| + |\bm{\alpha}_{\text{drug}}| + 2$. The
  Bayesian model
  then requires the specification of the hyperparameters 
  $\bm{\alpha}=\begin{bmatrix} \bm{\alpha}_{\text{feat}} \tr & 
  \bm{\alpha}_{\text{drug}} \tr 
  \end{bmatrix} \tr$ and $\bm{\lambda} = \begin{bmatrix} \lambda_{\text{feat}} &
  \lambda_{\text{drug}}
  \end{bmatrix} \tr$. These are
  abstract and hard to interpret parameters 
  for which we generally lack expert knowledge. They do, however, have a 
  significant influence on the shape of the posterior distribution. We 
  therefore propose to estimate these hyperparameters by empirical Bayes.
  In our case, this results in an   
  objective and data-driven inclusion of the feature and drug covariates.
	The canonical method for empirical Bayes is to maximise the marginal 
	likelihood with respect to the hyperparameters. In 
	\cite{casella_empirical_2001} the marginal likelihood is maximised by an EM 
	algorithm:
	\begin{align*}
	  \bm{\alpha}^{(l+1)},\bm{\lambda}^{(l+1)} & = \underset{\bm{\alpha},
	  \bm{\lambda}}{\argmax}\E_{\cdot | \Y} 
	  [\log p(\Y, \B, \bm{\Gamma}^2, \bm{\tau}^2, \bsigma^2) | \bm{\alpha}^{(l)},
	  \bm{\lambda}^{(l)}] \\
	  & = \underset{\bm{\alpha},\bm{\lambda}}{\argmax} 
	  \E_{\cdot | \Y} [\log \pi (\bm{\Gamma}^2) | \bm{\alpha}_{\text{feat}}^{(l)},
	  \bm{\lambda}_{\text{feat}}^{(l)}] + 
	  \E_{\cdot | \Y} [\log \pi (\bm{\tau}^2) | 
	  \bm{\alpha}_{\text{drug}}^{(l)}, \bm{\lambda}_{\text{drug}}^{(l)}],
	\end{align*}
	where $\Y = \begin{bmatrix} \y_1 & \cdots & \y_D \end{bmatrix}$,
	$\B = \begin{bmatrix} \bbeta_1, \dots, \bbeta_D \end{bmatrix}$, 
	$\bm{\tau}^2 = \begin{bmatrix} \tau^2_1 & \cdots & 
	\tau^2_D \end{bmatrix} \tr$,
	$\bsigma^2 = \begin{bmatrix} \sigma^2_1 & \cdots & 
	\sigma^2_D \end{bmatrix} \tr$,
	and $\bm{\Gamma}^2 = \begin{bmatrix} \bgamma^2_1 & \cdots & \bgamma^2_D 
	\end{bmatrix}$,
	and the expectation is with respect to the joint posterior. In our case, this 
	posterior is not available in closed form, which renders the expectation 
	difficult. While \cite{casella_empirical_2001} suggests to approximate the 
	expectation by a Monte Carlo sample, we propose to use the variational Bayes 
	approximation developed in Section \ref{sec:variationalbayes}:
	$$
	\bm{\alpha}^{(l+1)},\bm{\lambda}^{(l+1)} = 
	\underset{\bm{\alpha},\bm{\lambda}}{\argmax}\E_{Q^{(l)}} 
	[\log \pi(\bm{\Gamma}^2)| \bm{\alpha}_{\text{feat}}^{(l)}, 
	\bm{\lambda}_{\text{feat}}^{(l)}] + 
	\E_{Q^{(l)}} [\log \pi(\bm{\tau}^2)| 
	\bm{\alpha}_{\text{drug}}^{(l)}, \bm{\lambda}_{\text{drug}}^{(l)}],
	$$
	where now the expectation is with respect to the converged variational 
	posterior $Q^{(l)}=\prod_{d=1}^D Q_d^{(l)}$. 
	
	If we stack the drug and feature covariates:
	\begin{align*}
	  \mathbf{C} = 
	  \begin{bmatrix} \mathbf{c}_{11} \tr \\ 
	    \vdots \\
	    \mathbf{c}_{p1} \tr \\
	    \vdots \\
	    \mathbf{c}_{1D} \tr \\
	    \vdots \\
	    \mathbf{c}_{pD} \tr
	  \end{bmatrix} \text{ and }
	  \mathbf{Z} =
	  \begin{bmatrix} \mathbf{z}_{1} \tr \\ 
	    \vdots \\
	    \mathbf{z}_D \tr
	  \end{bmatrix},
	\end{align*}
	the empirical Bayes updates are given by:
  \begin{align*}
    \bm{\alpha}^{(l+1)}_{\text{feat}} & = \[ \mathbf{C} \tr \diag 
    (e_{jd}^{(l)}) \mathbf{C} \]^{-1} \mathbf{C} \tr 
    \mathbf{1}_{pD \times 1}, \\
    \lambda^{(l+1)}_{\text{feat}} & = pD \[ \sum_{d=1}^D \sum_{j=1}^{p} 
    b_{jd}^{(l)} + (\bm{\alpha}^{(l+1)}_{\text{feat}}) \tr \mathbf{C} \tr 
    \diag (e_{jd}^{(l)}) \mathbf{C} \bm{\alpha}^{(l+1)}_{\text{feat}} - 
    2 (\bm{\alpha}^{(l+1)}_{\text{feat}}) \tr \mathbf{C} \tr 
    \mathbf{1}_{pD \times 1} \]^{-1},\\
    \bm{\alpha}^{(l+1)}_{\text{drug}} & = \[ \mathbf{Z} \tr \diag 
    (f_{d}^{(l)}) \mathbf{Z} \]^{-1} \mathbf{Z} \tr 
    \mathbf{1}_{D \times 1}, \\
    \lambda^{(l+1)}_{\text{drug}} & = D \[ \sum_{d=1}^D g_{d}^{(l)} + 
    (\bm{\alpha}^{(l+1)}_{\text{drug}}) \tr \mathbf{Z} \tr \diag (f_{d}^{(l)})
    \mathbf{Z} \bm{\alpha}^{(l+1)}_{\text{drug}} - 
    2 (\bm{\alpha}^{(l+1)}_{\text{drug}}) \tr \mathbf{Z} \tr 
    \mathbf{1}_{D \times 1} \]^{-1},
  \end{align*}
  where 
  \begin{align*}
    e_{jd}^{(l)} & = \E_{Q^{(l)}}(\gamma_{jd}^2| 
    \bm{\alpha}_{\text{feat}}^{(l)}, 
    \lambda_{\text{feat}}^{(l)}) = (b_{jd}^{(l)} - 
    2/\delta_{jd}^{(l)}) \cdot \delta_{jd}^{(l)} (\phi_{jd}^{(l)})^2/
    \lambda_{\text{feat}}^{(l)}, \\
    f_{d}^{(l)} & = \E_{Q^{(l)}}(\tau_{d}^2| \bm{\alpha}_{\text{drug}}^{(l)}, 
    \lambda_{\text{drug}}^{(l)}) = (g_{d}^{(l)} - 
    (p + 1)/\eta_{d}^{(l)}) \cdot \eta_{d}^{(l)} (\chi_{d}^{(l)})^2/
    \lambda_{\text{drug}}^{(l)}.
  \end{align*}
  
  To ensure proper and unbiased shrinkage, intercepts are included in 
  $\bm{\alpha}_{\text{feat}}$ and $\bm{\alpha}_{\text{drug}}$. This is achieved 
  by appending both
  $\mathbf{C}$ and $\mathbf{Z}$ with a column of 
  ones. These intercepts are roughly interpreted as the
  expected prior precisions $\E(\gamma_{jd}^{-2})$ and $\E(\tau_d^{-2})$
  if the feature and drug covariates are all zero. 
  Likewise, an $\alpha$ corresponding to an external covariate may be
  interpreted as an additive effect of the external covariate on the prior
  expected precision. So an $\alpha=1$ translates to an increase in expected 
  prior precision of
  1 for every increase in the external covariate of 1,
	keeping all the other external covariates fixed.
  
  Variational Bayes approximations are known to underestimate posterior 
  variances \cite[]{rue_approximate_2009,consonni_mean-field_2007,
  bishop_pattern_2006,wang_inadequacy_2005}. If posterior variances are 
  required, 
  we suggest generating samples from the posterior with fixed hyperparameters
  estimates (after the procedure described in Section \ref{sec:empiricalbayes}
  has converged). A Gibbs sampler is described in 
  SM Section \ref{sm-sec:gibbssampler}. Alternatively, we provide an
  implementation of the proposed model in stan using the R package rstan 
  \cite[]{guo_rstan:_2018} at 
  \url{https://github.com/magnusmunch/cambridge}.
	
	\section{Simulations}
	\subsection{Setup}
	This section investigates the empirical 
	Bayes estimation properties of the model in a simulated setting; its main aim 
	is to assess hyperparameter recovery. It is a data based simulation,
	wherein the responses are simulated from a synthetic model, but the
	features are taken from the real GDSC data introducted in Section 
	\ref{sec:gdsc}. 
	The real GDSC features contain strong collinearities. Such strong
	collinearities in the design matrix impede correct parameter estimation. 
	We therefore replace the ambition of correctly estimating the $\beta_{jd}$
	with the more modest aim of approximately correct estimation of the 
	hyperparameters.
	
	A pre-processing step selects 100 features with largest variance, while the 
	168 drug sensitivities for 482 cell lines are 
	simulated from three versions of model (\ref{eq:likelihood}) and 
	(\ref{eq:prior}). All versions draw the error variances as 
	$\forall d: \sigma_d^2 \sim \Gamma^{-1}(3,2)$,
	such that the prior $\sigma_d^2$ mean and variance are both one. 
	Version 1 fixes $\forall d: \tau_d^2=1$, version 2 fixes
	$\forall j,d: \gamma_{jd}^2=1$, while version 3 draws both the $\tau_d^2$ and
	$\gamma_{jd}^2$ according to (\ref{eq:prior}). In version 1, we
	create four external dummy feature covariates that code for four 
	randomly assigned approximately equally sized groups of features. We set
	$\bm{\alpha}_{\text{feat}}$ such
	that the $\gamma_{jd}^2$ of the four groups of features have prior means
	$\phi_{jd} \in \{ 1, 1/2, 1/4, 1/8 \}$ 
	($\bm{\alpha}_{\text{feat}}=\begin{bmatrix} 1 & 1 & 3 & 7 \end{bmatrix} \tr$). 
	The prior scale parameter is set to $\lambda_{\text{feat}}=1$. Version 2 
	follows a similar procedure for the $\tau_d^2$. We randomly create four
	groups of drugs with corresponding external drug dummy variables and set
	$\bm{\alpha}_{\text{drug}}=\begin{bmatrix} 1 & 1 & 3 & 7 \end{bmatrix} \tr$,
	such that we have $\chi_{d} \in \{ 1, 1/2, 1/4, 1/8 \}$. The
	scale is set to $\lambda_{\text{drug}}=1$. Version 3 combines the procedures
	from versions 1 and 2 to draw the $\gamma_{jd}^2$ and $\tau_d^2$.

	We estimate four models: (i) an NIG model that does include external 
	covariates in the estimation, (ii) the NIG model estimated as in Section 
	\ref{sec:estimation}, and regular (iii) lasso and (iv) ridge models.
	Exclusion of the external covariates as in model (i)
	amounts to direct estimation of one common expected 
	prior means $\phi$ and/or $\chi$, instead of regression estimates for the
	$\phi_{jd}$ and/or $\chi_d$ as in model (ii). All 
	models are fit on 241 randomly selected cell lines, while performance measures 
	are estimated from the remaining 241 cell lines. 
	
	\subsection{Results}
<<simulation_gdsc>>=
res1 <- read.table("results/simulations_gdsc_res1.txt", row.names=NULL)
res2 <- read.table("results/simulations_gdsc_res2.txt", row.names=NULL)
res3 <- read.table("results/simulations_gdsc_res3.txt", row.names=NULL)
temp1 <- res1[, 1]
temp2 <- res2[, 1]
temp3 <- res3[, 1]
res1 <- as.matrix(res1[, -1])
res2 <- as.matrix(res2[, -1])
res3 <- as.matrix(res3[, -1])
rownames(res1) <- temp1
rownames(res2) <- temp2
rownames(res3) <- temp3
lambda <- apply(res1[rownames(res1)=="lambdaf", c(1, 2)], 2, median)
apply(res2[rownames(res2)=="lambdad", c(1, 2)], 2, median)
apply(res3[rownames(res3)=="lambdaf", c(1, 2)], 2, median)
apply(res3[rownames(res3)=="lambdad", c(1, 2)], 2, median)
est.phi <- median(1/res[rownames(res)=="alphaf0", 1])
true.phi <- mean(1/(c(1, 1, 3, 7) %*% t(unname(model.matrix(~ factor(1:4))))))
melbo <- apply(res[rownames(res)=="elbo", c(1, 2)], 2, median)
melbot <- apply(res[rownames(res)=="elbot", c(1, 2)], 2, median)

mphi <- median(1/res[rownames(res)=="alphaf0", 1])
mlambdaf <- median(1/res[rownames(res)=="lambdaf", 1])
@	
	Figure \ref{fig:simulations_gdsc_est1} shows the estimated
	$\bm{\alpha}_{\text{feat}}$ together with its true value for model
	(ii) in version 1 of the simulation study. Figure 
	\ref{fig:simulations_gdsc_est1}a shows that 
	$\bm{\alpha}_{\text{feat}}$ is slightly but 
	consistently underestimated. On the $\phi_{jd}$ scale this underestimation is
	almost undetectable as depicted in Figure
	\ref{fig:simulations_gdsc_est1}b. $\lambda_{\text{feat}}$ is slightly 
	overestimated with median estimate \Sexpr{round(lambda[2], 2)} (the
	true value is 1). The overestimated $\lambda_{\text{feat}}$ translates 
	into underestimated $\gamma_{jd}^2$ variances. Underestimation of variances is 
	a common phenomenon in variational Bayes approximations. In addition, 
	$\lambda_{\text{feat}}$ is a higher level scale
	parameter, which are, in general, hard to estimate. 
<<simulations_gdsc_est1, fig.cap="Estimated (a) hyperparameters $\\bm{\\alpha}_{\\text{feat}}$ and (b) prior means $\\bm{\\phi}_{jd}$ from simulations with fixed $\\tau_d^2$.", out.width="100%", fig.asp=1/2>>= 
@
	
	Model (i) gives a median $\phi$ estimate of \Sexpr{round(est.phi, 2)},  
	equal to the true mean of the $\phi_{jd}$, \Sexpr{round(true.phi, 2)}. 
	$\lambda_{\text{feat}}$ is slightly underestimated with median estimate
	\Sexpr{round(lambda[1], 2)} (the true value is one) and thus overestimates the 
	prior $\gamma_{jd}^2$ variance. This is likely to compensate for the common
	prior mean $\phi$, that is not able to capture part of the variation in the 
	true feature specific $\phi_{jd}$. 
	
	The median evidence 
	lower bounds (ELBO), which approximate the marginal likelihoods of models
	(i)-(iv), are estimated as \Sexpr{round(melbo, 2)[1:3]}, and 
	\Sexpr{round(melbo, 2)[4]}, respectively. This implies that model (i),
	with estimated $\bm{\alpha}_{\text{feat}}$ and $\lambda_{\text{feat}}$ fits
	the data best (the log Bayes factors comparing models (ii)-(iv) to model (i) 
	are \Sexpr{round(melbo[-c(1, 4)] - melbo[1], 2)}, and 
	\Sexpr{round(melbo[4] - melbo[1], 2)}, respectively, and thus highly favouring
	model (i)). The prediction MSEs for the four models on the test 
	data and gaps between training and test prediction MSE are displayed in Table
	\ref{tab:mse1}. Table \ref{tab:mse1} confirms that the models that
	include the external data (models (i) and (ii)) learn the underlying structure 
	in the data better than models (iii) and (iv) that do not include the external 
	data.
<<mse1>>= 
library(knitr, quietly=TRUE, verbose=FALSE)
suppressWarnings(library(kableExtra))
res <- read.table("results/simulations_gdsc_res1.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp
memse <- apply(res[rownames(res)=="emse", ], 2, median)
mpmse <- apply(res[rownames(res)=="pmse", ], 2, median)
mpmset <- apply(res[rownames(res)=="pmset", ], 2, median)
tab <- rbind(memse, mpmse, mpmse - mpmset)
rownames(tab) <- c("$\\text{EMSE}_{\\text{test}}$",
                   "$\\text{PMSE}_{\\text{test}}$",
                   "$\\text{PMSE}_{\\text{test}} - \\text{PMSE}_{\\text{train}}$")
options(knitr.kable.NA="", digits=3)
kable(tab,
      col.names=colnames(tab),
      caption="Median estimation MSE, prediction MSEs, and difference between 
      prediction MSEs on the test and training data in the fixed 
      $\\tau_d^2$ simulation setting.", 
      format="latex", booktabs=TRUE, escape=FALSE) %>%
  kable_styling(latex_options=c("HOLD_position"))
@

  
	To investigate the interplay of the feature and drug variance components, we
	repeat the whole simulation study with random $\tau_d^2$. Similarly as with
	the $\gamma_{jd}^2$, we randomly create four groups of drugs and corresponding
	external drug dummy covariates. We simulate the $\tau_d^2$ as in
	(\ref{eq:tauprior}), where we set $\bm{\alpha}_{\text{drug}} =
	\begin{bmatrix} 1 & 1 & 3 & 7 \end{bmatrix} \tr$, such
	that the $\tau_{d}^2$ of the four groups of features have prior means
	$\chi_{d} \in \{ 1, 1/2, 1/4, 1/8 \}$. The scale $\lambda_{\text{drug}}$ is
	set to one. We estimate the same four models as in the fixed $\tau_d^2$
	setting, but in addition estimate the $\tau_d^2$ priors as well. Again, we
	use two different versions: one where we estimate
	$\bm{\lambda}$ and one where we fix it to its true value.
	
	Figures \ref{fig:simulations_gdsc_est2} displays the estimated $\bm{\alpha}$,
	$\phi_{jd}$, and $\chi_d$ for models (i) and (ii). The scale parameters were
	estimated as $\lambda_{\text{feat}}=\Sexpr{round(rnorm(1), 2)}$ and
	$\lambda_{\text{drug}}=\Sexpr{round(rnorm(1)[1], 2)}$ in model (i). From
	Figure \ref{fig:simulations_gdsc_est2} we see that estimation of
	$\bm{\alpha}_{\text{feat}}$ is again biased. However, from
	Figure \ref{fig:simulations_gdsc_phi2} we see that the scale and ordering of
	the $\phi_{jd}$ are correclty estimated. We conjecture that the drop in
	estimation accuracy compared to the previous setting stems from the
	difficult estimation of the $\tau_d^2$ prior. Estimation of this prior
	depends on only 168 observed drugs, while the $\gamma_{jd}^2$ prior is
	is estimated from approximately $300 \cdot 168=50400$ features.
	\cite{wiel_learning_2019} show in a simple regression setting that if the
	number of samples and the number of parameters on which the EB is based are of
	the same order, empirical Bayes performs worse than when one of the two is
	large compared to the other.
	Here, the number of samples $n=241$ and $D=168$ are of the same order, leading
	to a suboptimal EB estimation performance.
<<simulations_gdsc_est2, fig.cap="Estimated (a) hyperparameters $\\bm{\\alpha}_{\\text{feat}}$ and (b) prior means $\\bm{\\phi}_{jd}$ from simulations with fixed $\\tau_d^2$.", out.width="100%", fig.asp=1/2>>= 
@

  We argue however, that even though the prior parameters are not estimated
  correctly, there is still some gain in predictive performance.
  Prediction MSEs on the test data the training-test gaps in prediction MSE are
  shown in Table \ref{tab:pmse2}. The comparisons of models (i) and
	(ii) vesus models (iii) and (iv) confirm that even though EB estimation is
	suboptimal, the external data do add something in predictive performance.

	\section{GDSC data}\label{sec:gdsc}
	\subsection{Primary data}
	The GDSC project's \cite[]{yang_genomics_2013} aim is ``to improve cancer 
	treatments by discovering therapeutic biomarkers that can be used to identify 
	patients most likely to respond to anticancer drug". Part of the project is to
	screen $>1000$ human cancer cell lines for drug sensitivities. The cell lines
	have been genetically characterised and several drug sensitivity measures are
	recorded.
	The data is freely available from \cite{garnett_systematic_2012} and consist 
	of: (i) the sensitivity measures of the cell lines 
	to the drugs, (ii) annotation of the 
	screened compounds, and (iii) the cell lines' genomic profile (mutations, 
	copy numbers, methylation profiles, and gene expression). We will attempt to 
	predict drug sensitivities of the cell lines, as quantified by half maximal
	inhibitory concentration (IC50), using the
	gene expression data. Other choices of sensitivity measures than IC50 are 
	possible, but a discussion on the pros and cons of different sensitivity
	measures is beyond the aim of this paper. A comprehensive penalized regression
	approach such as in 
	{\cite{mai_composite_2019} uses all the available molecular markers 
	instead of just gene expression (68 mutations, 426 copy numbers, 
	and 2602 gene expression levels for 498 cell lines and 97 drugs). The 
	current version of our software is not able to handle such problem sizes,
	due to the repeated calculation of posterior parameters in the empirical
	Bayes iterations. Currently, problems of up to 1000 outcomes, 1000 features,
	and 1000 samples are computationally feasible.

	We average repeated measures over cell line-drug combinations and model the
	logarithm of the IC50 values. In the following, IC50 refers 
	to the log-transformed values. 
	After iteratively removing the cell line or drug with the highest proportion 
	of missings until no more missing values remain, we end up with IC50 
	estimates for 482 cell lines on 168 drugs. As a preliminary step, we fit
	separate elastic net models for each drug with the \texttt{glmnet} package
	\cite[]{friedman_regularization_2010}, with $\alpha=0.01$ and $\lambda$
	chosen such that we select approximately 300 genes per drug.
	
	\subsection{External data}
	Two ternary drug covariates are available: the developmental stage
	(experimental, in clinical development, or clinically approved) of the
	drugs and the action (unkown, cytotoxic, or targeted) of the drugs. 
	Furthermore, we have a binary feature covariate available that indicates
	whether a gene belongs to the drug target pathway. The drug covariates are 
	taken
	directly from the GDSC database's annotation file. The feature covariate
	was created by comparing the target pathways in the GDSC annotation to
	the KEGG \cite[]{kanehisa_kegg:_2000} and reactome 
	\cite[]{fabregat_reactome_2018} repositories. The external covariates are 
	dummy-coded with
	reference categories clinically approved drugs, cytotoxic drugs, and 
	features that are not in the target pathway.
	
	We expect that drugs that have been clinically approved are easiest to 
	predict and hence yield the largest prior $\beta_{jd}$ variances, followed by
	the drugs in clinical development, and the experimental drugs. Likewise we 
	expect the targeted drugs to yield the largest prior $\beta_{jd}$ variances, 
	followed by the cytotoxic drugs, and the unkown target drugs. For the
	feature covariate, we expect that genes that are in the pathway of the drug
	are more predictive than genes that are not, i.e., 
	they have larger prior $\beta_{jd}$ variances than drugs that are not in 
	the pathway.
	
	\subsection{Results}
	To compute performance measures, we randomly split the data into training and
	test sets, containing 
	241 cell lines each, estimate the models on the training data and compute 
	performance measures on the test data. We repeated this procedure 50 times
	and average performance measures over the splits.
	The estimated 
	models are: two models like in (\ref{eq:prior}) but with different external 
	variables, named (i) NIG1 and (ii) NIG2 in the following, (iii) a model 
	as described in \cite{kpogbezan_empirical_2017}, named bSEM, (iv) a regular
	ridge model, and (v) a regular lasso model. The NIG1 incorporates no external
	variables and estimates the common prior $\gamma_{jd}^2$ and $\tau_d^2$ 
	means $\phi$ and $\chi$ directly, while the NIG2 model includes the
	developmental phase and action drug covariates, and the pathway
	feature covariate, and estimates $\bm{\alpha}$. The bSEM model is similar to 
	the NIG model in that it 
	draws the $\beta_{jd}$ from a conditionally normal distribution, but 
	fixes $\tau_d^2=1$ and draws 
	$\gamma_{jd}^2 \sim \Gamma^{-1} (\phi_{jd}, \lambda_{jd})$, instead of 
	from the inverse Gaussian distribution. Additionally, 
	the bSEM model can 
	only incorporate one binary external covariate that divides the features into 
	two groups, i.e., $\bm{\alpha} = \begin{bmatrix} \alpha_0 & \alpha_1 
	\end{bmatrix} \tr$ and $\bm{c}_{jd} = \begin{bmatrix} 1 & 0 \end{bmatrix} 
	\tr$ if feature $jd$ belongs to group one and $\bm{c}_{jd} = \begin{bmatrix} 
	1 & 1 \end{bmatrix} \tr$ if feature $jd$ belongs to group two. Both 
	$\bm{\alpha}_{\text{feat}}$ and $\lambda_{\text{feat}}$ are
	estimated in a similar fashion as in Section \ref{sec:estimation}. 

  From the $\bm{\alpha}$ estimates in Tables \ref{tab:results_drug} and
  \ref{tab:results_feat} we see that there
  seems to be an effect of the
  external variables. If we look at the NIG1 model, that includes all external 
  covariates, we see that approved drugs are easier to predict, due to small
  prior precision, followed by drugs that are in clinical development,
  as we expected. The cytotoxic drugs are the hardest to predict, 
  with the largest prior precision (smallest prior variance), followed by the 
  targeted drugs and the unknown drugs. This is again according to expectation.
  For the unkown drugs, we are unable to confirm the small precision. Genes 
  that are in the target pathway have 
  smaller prior precisions, according to expectation, which confirms that these
  genes are more important for prediction of drug sensitivity. The estimates 
  from
  the bSEM model are also according to expectation: genes that are in the target
  pathway have a much smaller precision than genes that are not in the target
  pathway. The 
  effects of the pathway indicator seem to be much larger in the bSEM model. 
<<results_drug>>= 
library(knitr, quietly=TRUE, verbose=FALSE)
suppressWarnings(library(kableExtra))
# load("results/data_gdsc_res1.Rdata")
# tab1 <- cbind(tab[, 1], c(NA, 0, NA), tab[, c(3, 2)], c(NA, 0, NA), 
#               tab[, c(4, 5)])
# options(knitr.kable.NA="", digits=3)
# kable(tab1,
#       col.names=c("intercept", "approved", "development", 
#                   "experimental", "targeted", "unkown", "cytotoxic"),
#       caption="$\\bm{\\alpha}_{\\text{drug}}$ estimates from GDSC data.", 
#       format="latex", booktabs=TRUE, escape=FALSE) %>%
#   kable_styling(latex_options=c("HOLD_position"))
kable(matrix(1, nrow=1, ncol=2),
      caption="$\\bm{\\alpha}_{\\text{drug}}$ estimates from GDSC data.", 
      format="latex", booktabs=TRUE, escape=FALSE) %>%
  kable_styling(latex_options=c("HOLD_position"))
@

<<results_feat>>= 
library(knitr, quietly=TRUE, verbose=FALSE)
suppressWarnings(library(kableExtra))
# load("results/data_gdsc_res1.Rdata")
# tab2 <- cbind(tab[, 6], c(NA, 0, 0), tab[, 7])
# options(knitr.kable.NA="", digits=3)
# kable(tab2,
#       col.names=c("intercept", "not in pathway", "in pathway"),
#       caption="$\\bm{\\alpha}_{\\text{feat}}$ estimates from GDSC data.", 
#       format="latex", booktabs=TRUE, escape=FALSE) %>%
#   kable_styling(latex_options=c("HOLD_position"))
kable(matrix(1, nrow=1, ncol=2),
      caption="$\\bm{\\alpha}_{\\text{feat}}$ estimates from GDSC data.", 
      format="latex", booktabs=TRUE, escape=FALSE) %>%
  kable_styling(latex_options=c("HOLD_position"))
@
  
  The median prediction MSEs and the median ELBOs, calculated on the test data
  are displayed in Table \ref{tab:results_data}. In terms of 
  is lowest in the NIG models 
  (\Sexpr{round(rnorm(1))} and \Sexpr{rnorm(1)}, in the
  NIG1 and NIG2 models, respectively). The bSEM model is not able to 
  improve on the null model with an median prediction MSE of 
  \Sexpr{round(rnorm(1))} compared to the null model's MSE of 
  \Sexpr{round(rnorm(1))}.
<<results_data>>= 
library(knitr, quietly=TRUE, verbose=FALSE)
suppressWarnings(library(kableExtra))
# load("results/data_gdsc_res1.Rdata")
# mapmse <- apply(apmse, 2, median)
# melbo <- apply(elbo, 2, median)
# tab <- rbind(mapmse, c(melbo, NA, NA))
# rownames(tab) <- c("MSE", "ELBO")
# options(knitr.kable.NA="", digits=3)
# kable(tab,
#       col.names=c("NIG1", "NIG2", "bSEM", "lasso", "ridge"),
#       caption="Median prediction MSE and median ELBO estimated on the test data", 
#       format="latex", booktabs=TRUE, escape=FALSE) %>%
#   kable_styling(latex_options=c("HOLD_position"))
options(knitr.kable.NA="", digits=3)
kable(matrix(1, nrow=1, ncol=2),
      caption="Median prediction MSE and median ELBO estimated on the test data", 
      format="latex", booktabs=TRUE, escape=FALSE) %>%
  kable_styling(latex_options=c("HOLD_position"))
@
  
  In addition to the empirical Bayes estimates and the prediction MSE, we 
  investigated the stability of the estimates by ranking the posterior means
  $\E_{Q}(\bbeta_d)$ and calculating the Spearman's rank
  correlation over the 50 data splits. They are depicted in Figure 
  \ref{fig:data_gdsc_res1_brank_hist}. In both NIG models the correlations
  between splits are higher than in the bSEM model; an indication that 
  estimation in the NIG model is more stable than in the bSEM model.
<<data_gdsc_res1_brank_hist, fig.cap="Spearman's rank correlation between the estimated posterior means $\\E_{Q}(\\bbeta_d)$ in the 50 data splits.", out.width="70%", fig.asp=2/3>>= 
@  

	\section{Discussion}\label{sec:discussion}
	The preceding presents a novel model for drug sensitivity
	prediction from a set of high dimensional molecular features. The model allows
	for the inclusion of discrete and continuous external covariates on both the 
	drugs and features. Inclusion of the external information is through 
	data-driven and adaptive empirical Bayes estimation of the hyperparameters in 
	the normal inverse Gaussian prior model (\ref{eq:prior}). Estimation is 
	efficient and scales well with the number of features and samples.
	
	The model is put into practice on the GDSC data. The results in Section
	\ref{sec:gdsc} show that the inclusion of the external covariates is 
	beneficial for drug sensitivity prediction. Although the prediction MSE is
	not lower if the external covariates are included, 
	the resulting hyperparameter estimates are still informative
	and may guide future research efforts.
	
	The comparison to the bSEM model that allows for the inclusion of 
	one binary external feature covariate favours the proposed NIG model. 
	Prediction MSEs are almost uniformly lower over the drugs for the NIG model 
	compared to bSEM. Consequently, the average prediction MSE is significantly
	lower for the NIG model. 
	
	Although the empirical Bayes estimates of the external covariate effects 
	$\bm{\alpha}$ are informative, they are small compared to 
	bSEM's estimates. Consequently, the posteriors of the models with and without
	the external covariates (NIG2 and NIG1 in Section \ref{sec:gdsc}) do not 
	differ by much. This results similar prediction MSEs with
	the inclusion of the external covariates. This is a topic that deserves 
	further attention. One proposed solution is to include an extra 
	prior variance component that promotes a more heavy-tailed $\beta_{jd}$ prior
	distribution. An example is a variance component with a half-cauchy prior 
	distribution, i.e., the horseshoe \cite[]{carvalho_handling_2009}. 
	This solution is similar to the pInc model from 
	\cite{kpogbezan_incorporating_2019}. 
	Like the bSEM model, the pInc model gives significantly larger external
	covariate effects, 
	so we conjecture that the inclusion of a similar heavy-tailed prior variance 
	component will have a similar effect in the NIG model.

	\section*{Supplementary Material}
	Supplementary Material is available online from 
	\url{https://github.com/magnusmunch/cambridge}. 
	
	\section*{Reproducible Research}
	All results and documents may be recreated from 
	\url{https://github.com/magnusmunch/cambridge}.
	
	\section*{Acknowledgements}
	We thank Zhi Zhao and Leiv Tore Salte R{\o}nneberg from the Oslo Centre for 
	Biostatistics and Epidemiology for their suggestions on the pre-processing of 
	the GDSC data. \textit{Conflict of Interest}: None declared.
	
	\bibliographystyle{author_short3} 
	\bibliography{refs}
	
	\section*{Session info}

<<r session-info, eval=TRUE, echo=TRUE, tidy=TRUE>>=
devtools::session_info()
@

\end{document}