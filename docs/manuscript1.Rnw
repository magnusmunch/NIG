% set options
<<settings, include=FALSE, echo=FALSE>>=
opts_knit$set(root.dir="..", base.dir="../figs")
opts_chunk$set(fig.align='center', fig.path="../figs/", 
               echo=FALSE, cache=FALSE, message=FALSE, fig.pos='!ht')
knit_hooks$set(document=function(x) {
  sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed=TRUE)})
@

% read figure code
<<figures, include=FALSE>>=
read_chunk("code/figures.R")
@

\documentclass[a4paper,hidelinks]{article}
\usepackage{bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,
rotating,caption,subcaption,natbib,appendix,titlesec,multicol,hyperref,verbatim,
bbm,algorithm,algpseudocode,pgfplotstable,threeparttable,booktabs,mathtools,
dsfont,parskip,xr,tikz}
\externaldocument[sm-]{supplement1}
\usetikzlibrary{bayesnet}

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Em}{\mathbf{E}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\Lm}{\mathbf{L}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}

% settings
\pgfplotsset{compat=1.16}
\graphicspath{{../figs/}}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Drug sensitivity prediction with normal inverse Gaussian shrinkage 
informed by external data}
\date{\today}
\author{Magnus M. M\"unch$^{1,2,3}$\footnote{Correspondence to: 
\href{mailto:m.munch@amsterdamumc.nl}{m.munch@amsterdamumc.nl}}, Mark A. van de 
Wiel$^{1,3}$, Sylvia Richardson$^{3}$, \\ and Gwena{\"e}l G. R. Leday$^{3}$}

\begin{document}

	\maketitle
	
	\noindent
	1. Department of Epidemiology \& Biostatistics, Amsterdam UMC, VU University, 
	PO Box 7057, 1007 MB Amsterdam, The Netherlands \\
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\
	3. MRC Biostatistics Unit, Cambridge Institute of Public Health, Cambridge,
	United Kingdom
	
	\begin{abstract}
		{In precision medicine, a common problem is drug sensitivity prediction from
		cancer tissue cell lines. These types of problems entail modelling 
		multivariate drug responses on high dimensional molecular feature sets in 
		typically $>1000$ cell lines. The dimensions of the problem require 
		specialised models and estimation methods. In addition, external information
		on both the drugs and the features is often available. We propose to 
		model the drug responses through a linear regression with shrinkage enforced
		through a normal
		inverse Gaussian prior. We let the prior depend on the external information,
		and estimate the model and external information dependence in an 
		empirical-variational Bayes framework. We demonstrate the usefullness of
		this model in both a simulated setting and in the publicly available
		Genomics of Drug Sensitivity in Cancer data.}
	\end{abstract}
	
	\noindent\textbf{Keywords}: Drug sensitivity; Empirical Bayes; 
	Genomics of Drug Sensitivity in Cancer (GDSC); Variational Bayes
	
	\noindent\textbf{Software available from}: 
	\url{https://github.com/magnusmunch/cambridge}
	
	\section{Introduction}\label{sec:introduction}
	Recently, promising results in precision medicine 
	have sparked an interest in cancer drug sensitivity prediction models
	\cite[]{iorio_landscape_2016}. Typically, these models 
	predict the drug sensitivity for new patients from a set of molecular 
	features.
	Development of such models is often done in well-characterised human cancer
	tissue cell lines. The current paper presents a novel drug sensitivity
	prediction model and an application to a real drug
	sensitivity data set.
	
	Development of such models from cell lines has proven to be difficult
	(see e.g., the DREAM 7 challenge in \cite{costello_community_2014}). 
	Difficulties arise, among others, from the dimensions of the problem. 
	Typically, the data contains
	hundreds of drugs, thousands of cell lines, and thousands of molecular 
	features. An example of a large database of drug responses and molecular
	features is the Genomics of Drug Sensitivity in Cancer (GDSC) data 
	\cite[]{yang_genomics_2013}, which we will further 
	investigate in Section \ref{sec:gdsc}. Other examples of such databases 
	include
	the Cancer Cell Line Encyclopedia (CCLE) \cite[]{li_landscape_2019} and 
	the US National Cancer Institute 60 human tumour cell line anticancer drug
	screen (NCI60) \cite[]{shoemaker_NCI60_2006}. The dimensions of these data
	prohibit the estimation of standard regression models and
	typically require some form of regularisation.
	
	The GDSC database contains additional information on both the drugs and 
	molecular features, such as the target 
	pathways and developmental stages of the drugs. Additional online
	repositories may provide extra information such as the molecular weight of
	the compounds or the publication signatures of the molecular features. In some
	cases, prior knowledge on the drug efficacies may be available,
	from previous experiments. We propose to include these possibly beneficial 
	information sources in the estimation of the sensitivity prediction models in 
	a data-driven manner. More specifically, we estimate a normal 
	inverse Gaussian model, where the extent of regularisation is estimated by an 
	adaptive empirical Bayes procedure, guided by the external information.
	
	We are not the first to work on drug sensitivity prediction models. Reviews on 
	the topic are \cite{azuaje_computational_2017} and \cite{ali_machine_2019}.
	\cite{zhao_structured_2019} and \cite{mai_composite_2019} consider a
	structured penalized multivariate regression approach. 
	\cite{aben_tandem:_2016} introduce a two-stage penalized regression 
	model that includes two different types of molecular features.
	\cite{ammad-ud-din_drug_2016} and \cite{costello_community_2014} tackle the
	problem through a multiple kernel learning approach. Our solution allows for
	the adaptive incorporation of the external 
  information on drugs and features. This is done by
  pooling information, both across drugs and features. Estimation of the model
  is through computational feasible variational bayes approximations, while 
  empirical Bayes estimation of tuning parameters pools information across drugs
  and features in a data-driven manner.
  
	The rest of the paper is structured as follows. In Section 
	\ref{sec:model} we introduce our model, the estimation of which is detailed
	in Section \ref{sec:estimation}. Section \ref{sec:simulations} describes a 
	simulation study that investigates the estimation of hyperparameters by the
	proposed method. In Section \ref{sec:gdsc} we analyse the
	GDSC data, and we end with a discussion in Section \ref{sec:discussion} on the 
	pros and cons of the proposed method.
	
	\section{Model}\label{sec:model}
	\subsection{Simultaneous equations model}\label{sec:SEM}
	Let $y_{id}$ be the continuous sensitivity measures 
	for cell lines $i=1,\dots, n$, and drug $d=1, \dots, D$. 
	We predict sensitivity from molecular features 
	$x_{ij}$, $j=1,\dots, p$, collected in 
	$\x_i = \begin{bmatrix} x_{i1} & \dots & x_{ip} \end{bmatrix} \tr$. 
	We assume that both covariates and responses have been centred per drug
	and regress the drug sensitivities on the molecular features:
	\begin{equation}\label{eq:likelihood}
	  y_{id} = \x_i \tr \bbeta_d + \epsilon_{id}, 
	  \text{ with } \epsilon_{id} \sim \mathcal{N}(0, \sigma_d^2),
	\end{equation}
	where the $p$-dimensional 
	$\bbeta_d = \begin{bmatrix} \beta_{1d} & \cdots & \beta_{pd} \end{bmatrix} 
	\tr$ are the drug-specific omics feature 
	effects. Note that (\ref{eq:likelihood}) gives rise to a system of 
	D linear regression equations.
	
	The cell lines used in drug response models are often taken from different
	tissues. In addition, other clinical covariates might be available. 
	To obtain unbiased feature effects, one may wish to account for these. 
	We do so by introducing unpenalized covariates, the $\beta_{jd}$ coefficients 
	of which are endowed with a flat prior. For the sake of clarity, in the
	following, such unpenalized covariates are omitted. However, the available
	software allows for their inclusion.
	
	\subsection{Bayesian prior model}\label{sec:prior}
	We carry out inference by endowing the parameters with the following priors:
	\begin{subequations}\label{eq:prior}
		\begin{align}
		  \beta_{jd} | \gamma^2_{jd}, \tau_d^2, \sigma^2_d & \sim \mathcal{N}_{p} 
		  (0, \gamma_{jd}^2 \tau_d^2 \sigma_d^2), 
		  \label{eq:betaprior}\\
		  \gamma_{jd}^{2} & \sim \mathcal{IG}(\phi_{jd}, \lambda_{\text{feat}}), 
		  \label{eq:gammaprior}\\
		  \tau_d^{2} & \sim \mathcal{IG}(\chi_d, \lambda_{\text{drug}}), 
		  \label{eq:tauprior}\\
		  \sigma_d^{2} & \sim 1/\sigma_d^{3}, \label{eq:sigmaprior}
		\end{align}
	\end{subequations}
	where $\mathcal{IG}(\phi, \lambda)$ denotes an inverse Gaussian distribution 
	with mean $\phi$ and shape $\lambda$. 
	
	In model (\ref{eq:prior}), $\gamma_{jd}^2$ in (\ref{eq:gammaprior}) denotes a 
	local variance 
	component that is supposed to capture local, feature-specific variation in the
	model parameters $\beta_{jd}$ in (\ref{eq:betaprior}), 
	while the global variance components $\tau_d^2$ in (\ref{eq:tauprior})
	capture the drug-specific, general trend in $\bm{\beta}_{d}$. Each drug 
	response is endowed with a random error variance $\sigma_d^2$, distributed
	according to (\ref{eq:sigmaprior}).
	
	Prior distributions of the form
	(\ref{eq:prior}) are often referred to as global-local shrinkage rules 
	\cite[]{bernardo_shrink_2011}, due to the multiplicative separation of the 
	prior variance into a local component $\gamma_{jd}^2$ and a global component
	$\tau_d^2$. For appropriate local shrinkage in global-local 
	shrinkage models it is important  to account for 
  different noise levels $\sigma_d^2$ by scaling the $\beta_{jd}$ variances 
  accordingly.

	The normal inverse Gaussian (NIG) prior model was introduced in 
  \cite{barndorff-nielsen_hyperbolic_1978} and since 
  \cite{barndorff-nielsen_normal_1997} it is routinely applied in mathematical 
  finance (see, e.g., \cite{kalemanova_normal_2007}). Here we extend it with an
  additional global variance component $\tau_d^2$. Supplementary Material
  (SM) Section \ref{sm-sec:prior} contains more details on the NIG prior.
  To illustrate the effect of the NIG prior on the posterior mean, we consider 
  the prior reparametrised as in 
  \cite{carvalho_handling_2009}, i.e., in terms of shrinkage weights 
  $\kappa_{jd}=1/(1 + \gamma_{jd}^2) \in (0, 1)$.
  Under the (simplified) normal means model, i.e., 
  $\X= \begin{bmatrix} \x_1 & \cdots \x_n \end{bmatrix} \tr =\I_p$, with fixed 
  $\tau_d^2=\sigma_d^2=1$, the resulting conditional
  posterior mean for the $\beta_{jd}$ is
  $\E(\beta_{jd} | y_{jd}, \kappa_{jd}) = (1 - \kappa_{jd}) y_{jd}$. 
  Thus, $\kappa_{jd}=0$ implies no shrinkage of $\beta_{jd}$ and $\kappa_{jd}=1$
  implies full shrinkage towards zero. Figure 
  \ref{fig:dens_kappa} depicts the prior on $\kappa_{jd}$ implied by 
  several choices of $\beta_{jd}$ prior.
  
<<dens_kappa, fig.cap="Implied prior densities $\\pi(\\kappa_{jd})$ for the (a) NIG, (b) Student's $t$, and (c) lasso priors. Different line types correspond to different hyperparameter settings. The hyperparameter settings (given in Section \\ref{sm-sec:hyperparameters} of the SM) were chosen to show some possible, distinct shapes that each of the priors can take.", out.width="100%", fig.asp=1/3>>=
@
  
  Figure \ref{fig:dens_kappa} shows that, depending on the choice
  of hyperparameters, the NIG prior can behave similarly to the Student's $t$ 
  prior (decreasing form zero, with substantial mass close to zero and 
  little mass close to one, like the
  solid lines in Figure \ref{fig:dens_kappa}a-\ref{fig:dens_kappa}b), 
  but also rather differently (dashed and dotted lines in 
  Figure \ref{fig:dens_kappa}a-\ref{fig:dens_kappa}b). Our argumentation to 
  model the $\gamma^2_{jd}$ by an inverse Gaussian 
  distribution, as has been suggested in \cite{fabrizi_specification_2016} and 
  \cite{caron_sparse_2008}, is three-fold: (i) the NIG model is more flexible 
  than the lasso prior (as seen from Figure 
  \ref{fig:dens_kappa}), (ii) the NIG prior allows to model
  the means of the $\gamma_{jd}^2$ ($\phi_{jd}$) and $\tau_{jd}^2$ 
  ($\chi_{d}$) as a function of external data 
  more conveniently than the Student's $t$ prior, as explained in Section 
  \ref{sec:empiricalbayes}, and (iii) like the horseshoe 
  \cite[]{carvalho_handling_2009}, the NIG shrinkage
  weights prior can put mass both near zero and one, a desirable property of 
  shrinkage priors \cite[]{bernardo_shrink_2011}.

  A few remarks on the choice of error variance prior are justified here: 
  many authors endow error variance components with vague gamma priors. 
  \cite{gelman_prior_2006}, 
  among others, advises against this practice. The degree of `vagueness' has a 
  large influence on the posterior, while degree of `vagueness' is a difficult 
  parameter to set. This influence is especially pronounced if the likelihood is
  relatively flat, as may be reasonably expected in the large $p$, 
  small $n$ setting. We therefore model the error variance with Jeffrey's 
  objective prior 
  \cite[]{jeffreys_invariant_1946} that does not depend on any 
  subjective specification of hyperparameters. In the derivation of our 
  Jeffrey's prior
  for the error variance, we jointly consider an unknown data mean and variance 
  \cite[]{kass_selection_1996}. This
  joint consideration results in the somewhat unorthodox $1/\sigma^3$ Jeffrey's
  prior.
	
	\subsection{External information}\label{sec:external}
	In drug sensitivity prediction models, external information on both the drugs
	and features is often available. Here, we assume this information to be 
	available as
	external feature `covariates' $\mathbf{c}_{jdg}$, for $g=1, \dots, G$, and 
	drug `covariates' $\mathbf{z}_{dh}$, for $h=1,\dots, H$. An example of a 
	(binary) feature covariate is target pathway presence, with $c_{jdg}=0$ if 
	gene $j$ is present in the 
	target pathway of drug $d$ and $c_{jdg}=1$ if it is not. An example of a 
	(ternary) drug
	covariate is developmental phase, with levels experimental phase, 
	clinical development, and approved by a governing agency.
	
	The external covariates come in through our mean models for 
	the $\gamma_{jd}^2$ and $\tau_d^2$ hyperpriors:
	$\phi_{jd} = (\mathbf{c}_{jd} \tr \bm{\alpha}_{\text{feat}})^{-1}$ and
	$\chi_d = (\mathbf{z}_{d} \tr \bm{\alpha}_{\text{drug}})^{-1}$, with 
	$\mathbf{c}_{jd} = \begin{bmatrix} c_{jd1} & \cdots & c_{jdG} \end{bmatrix}$
	and $\mathbf{z}_{d} = \begin{bmatrix} z_{d1} & \cdots & z_{dH} \end{bmatrix}$,
	where categorical external covariates are dummy coded.
	The model now requires hyperparameters $\bm{\alpha}_{\text{feat}}$, 
	$\lambda_{\text{feat}}$, $\bm{\alpha}_{\text{drug}}$, and $
	\lambda_{\text{drug}}$, which we estimate in a data-driven manner (see Section
	\ref{sec:empiricalbayes}).
  
  A representation of our model as a Bayesian DAG is given in Figure
  \ref{fig:dag}. We note that in many settings, the set of features might be
  different for different drugs. In that case the covariates are indexed by
  the drug $d$: $\X^d$, a trivial extension of model 
  (\ref{eq:likelihood}) and (\ref{eq:prior}). This extension is included in the
  available software, but for clarity it is omitted in the following. 
  
  \begin{figure}
    \centering
    \tikz{ %
      % Y
      \node[obs] (y) {$y_{id}$}; %
      \factor[right=of y, yshift=1.5cm, xshift=-0.1cm] {y-f} 
      {above:$\mathcal{N}$} {} {} ; %
      
      % X
      \node[det, above=of y, yshift=1cm] (x) {$x_{ij}$}; %
      
      % sigma2
      \node[latent, right=of y] (sigma2) {$\sigma_d^2$}; %
      
      % beta
      \node[latent, right=of x] (beta) {$\beta_{jd}$}; %
      \factor[right=of beta, yshift=-1.4cm, xshift=-0.1cm] {beta-f} 
      {above:$\mathcal{N}$} {} {} ; %
      
      % gamma2
      \node[latent, right=of beta] (gamma2) {$\gamma_{jd}^2$}; %
      \node[const, right=of gamma2, yshift=0.5cm] (phi) {$\phi_{jd}$}; %
      \node[const, right=of gamma2, yshift=-0.5cm] (lambdaf) 
      {$\lambda_{\text{feat}}$}; %
      \factor[right=of gamma2] {gamma2-f} {above:$\mathcal{IG}$} {phi,lambdaf} 
      {gamma2} ; %
      \node[det, right=of gamma2, xshift=2cm, yshift=0.5cm] (c) {$c_{jdg}$}; %
      
      % tau2
      \node[latent, right=of sigma2] (tau2) {$\tau_d^2$}; %
      \node[const, right=of tau2, yshift=0.5cm] (chi) {$\chi_{d}$}; %
      \node[const, right=of tau2, yshift=-0.5cm] (lambdad) 
      {$\lambda_{\text{drug}}$}; %
      \factor[right=of tau2] {tau2-f} {above:$\mathcal{IG}$} {chi,lambdad} 
      {tau2} ; %
      \node[det, right=of tau2, xshift=2cm, yshift=0.5cm] (z) {$z_{dh}$}; %
      
      % edges
      \factoredge {x,beta,sigma2} {y-f} {y} ; %
      \factoredge {sigma2,gamma2,tau2} {beta-f} {beta} ; %
      \edge {c} {phi}; %
      \edge {z} {chi}; %
      
      % plates
      \plate{plateg} {(c)} {$g=1,\dots,G$}
      \plate{plateh} {(z)} {$h=1,\dots,H$}
      \plate{platei} {(y) (x)}{$i=1,\dots,n$}; %
      \plate{platej} {(x) (beta) (gamma2) (phi) (lambdaf) (c) (plateg)}
      {$j=1,\dots,p$}; %
      \plate{plated} 
      {(y) (x) (sigma2) (beta) (gamma2) (tau2) (phi) (lambdaf)
      (chi) (lambdad) (c) (z) (platei) (platej) (plateg) (plateh)}
      {$d=1,\dots,D$}; %
    }
    \caption{Hierarchical representation of the drug sensitivity prediction 
    model. Grey circles 
    represent observed variables, white circles represent unobserved variables,
    squares represent fixed data, and unenclosed letters are parameters to be
    estimated. Cell lines are indexed by $i$, features
    by $j$, drugs by $d$, drug covariates by $h$, and feature covariates by 
    $g$. The $y_{id}$ are the drug sensitivities, $x_{ij}$ the 
    molecular features, 
    $c_{jdg}$ the external feature covariates, $z_{dh}$ the external drug
    covariates, $\beta_{jd}$ the regression coefficients, $\sigma_d^2$ the 
    error variances, $\tau_d^2$ and $\gamma_{jd}^2$ the drug and feature
    specific variance components, respectively, and $\phi_{jd}$, 
    $\lambda_{\text{feat}}$, $\chi_{d}$, and $\lambda_{\text{drug}}$ the 
    hyperparameters.}
    \label{fig:dag}
  \end{figure}

	\section{Estimation}\label{sec:estimation}
	\subsection{Variational Bayes}\label{sec:variationalbayes}
	The posterior corresponding to the model described in (\ref{eq:likelihood})
	and (\ref{eq:prior}) is not available in closed form. 
	To avoid computationally intensive markov chain Monte Carlo (MCMC) 
	algorithms, we approximate the joint posterior by variational Bayes (see 
	\cite{blei_variational_2017} for a review), where the approximate
	posterior density factorises as: $p(\bbeta_d, \bm{\gamma}^2_d, \tau_{d}^2, 
	\sigma_d^2 | \y_d) \approx 
	Q_d(\cdot) = q(\bbeta_d) \cdot q(\bm{\gamma}_{d}^2) \cdot q(\tau_{d}^2) 
	\cdot q(\sigma_d^2)$, where $\bm{\gamma}_{d}^2 = 
	\begin{bmatrix} \gamma_{1d}^2 & \cdots & \gamma_{pd}^2 \end{bmatrix} \tr$. For
  notational convenience, we slightly abuse notation and let $q(\cdot)$ denote
	different densities for different inputs.
	Under such a factorisation, the marginal variational posteriors that 
	minimise the Kullback-Leibler divergence of the true posterior to the 
	variational Bayes approximation \cite[]{neal_view_1998} are given by:
	\begin{align*}
    q(\bm{\beta}_d) & \overset{D}{=} \mathcal{N}_{p} 
    (\bm{\mu}_d, \bm{\Sigma}_d), \\
    q(\bm{\gamma}_{d}^2) & \overset{D}{=} \prod_{j=1}^p \mathcal{GIG}
    \(-1, \lambda_{\text{feat}}/\phi_{jd}^2, \delta_{jd}\), \\
    q(\tau^2_d) & \overset{D}{=} \mathcal{GIG} \(-\frac{p + 1}{2}, 
    \lambda_{\text{drug}}/\chi_d^2, \eta_d\), \\
    q(\sigma^2_d) & \overset{D}{=} \Gamma^{-1} \(\frac{n + p + 1}{2}, \zeta_d\),
  \end{align*}
	where $\mathcal{GIG} \(p, \nu, \eta\)$ denotes the generalized inverse Gaussian
	distribution with index $p \in \mathbb{R}$, and scales $\nu > 0$ and 
	$\eta > 0$ \cite[]{jorgensen_statistical_1982}. See SM Section 
	\ref{sm-sec:vbderivations} for the derivations. The variational
	parameters $\bm{\mu}_d$, $\bm{\Sigma}_d$, $\delta_{jd}$, $\eta_d$, and
	$\zeta_d$ contain cyclic dependencies and are iteratively 
	updated by:
	\begin{subequations}\label{eq:vbequations}
    \begin{align}
      \bm{\Sigma}_d^{(h+1)} & = (a_d^{(h)})^{-1} 
      \[\X \tr \X + g_{d}^{(h)} \diag(b_{jd}^{(h)})\]^{-1}, \\
      \bm{\mu}_d^{(h+1)} & = \[\X \tr \X + g_{d}^{(h)} 
      \diag(b_{jd}^{(h)})\]^{-1} 
      \X \tr \y_d, \\
      \delta_{jd}^{(h+1)} & = a_d^{(h)} g_{d}^{(h)} \[(\bmu^{(h+1)}_{jd})^2 + 
      (\bSigma^{(h + 1)}_d)_{jj}\] + \lambda_{\text{feat}}, \\ 
      \eta_d^{(h+1)} & = a_d^{(h)} \sum_{j=1}^{p} b_{jd}^{(h + 1)}
      \[ (\bmu^{(h+1)}_{jd})^2 + (\bSigma^{(h + 1)}_d)_{jj} \] + 
      \lambda_{\text{drug}}, \\
      \zeta_d^{(h+1)} & = \frac{1}{2} \bigg[\mathbf{y}_d \tr \mathbf{y}_d -
      2 \mathbf{y}_d \tr \X \bm{\mu}_d^{(h+1)} + 
      \trace ( \X \tr \X \bm{\Sigma}_d^{(h+1)}) + 
      (\bm{\mu}_d^{(h+1)}) \tr \X \tr \X \bm{\mu}_d^{(h+1)} \\
      & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + 
      g_{d}^{(h + 1)} \trace \[ \diag (b_{jd}^{(h+1)}) \bm{\Sigma}_d^{(h+1)}\] + 
      g_{d}^{(h + 1)} (\bm{\mu}_d^{(h+1)}) \tr \diag (b_{jd}^{(h+1)}) 
      \bm{\mu}_d^{(h+1)}\bigg],
    \end{align}
  \end{subequations}
  until convergence, where $\y_d = \begin{bmatrix} y_{1d} & \cdots & y_{nd}
  \end{bmatrix} \tr$. Here, we set
  \begin{align}
    a_d^{(h)} & =\E_{Q^{(h)}}(\sigma_d^{-2})=(n + p + 1)/(2 \zeta_d^{(h)}), 
    \nonumber\\
    b_{jd}^{(h)} & = \E_{Q^{(h)}}(\gamma_{jd}^{-2}) = 
    \sqrt{\frac{\lambda_{\text{feat}}}{\phi_{jd}^2 \delta_{jd}^{(h)}}}
    \frac{K_0\(\sqrt{\delta_{jd}^{(h)}\lambda_{\text{feat}}/\phi_{jd}^2}\)}
    {K_1\(\sqrt{\delta_{jd}^{(h)}\lambda_{\text{feat}}/\phi_{jd}^2}\)} + 
    \frac{2}{\delta_{jd}^{(h)}}, \label{eq:ratiomodifiedbessel} \\
    g_d^{(h)} & =\E_{Q^{(h)}}(\tau_d^{-2})=
    \sqrt{\frac{\lambda_{\text{drug}}}{\chi_{d}^2 \eta_{d}^{(h)}}}
    \frac{K_{(p-1)/2}\(\sqrt{\eta_{d}^{(h)}\lambda_{\text{drug}}/\chi_{d}^2}\)}
    {K_{(p+1)/2}\(\sqrt{\eta_{d}^{(h)}\lambda_{\text{drug}}/\chi_{d}^2}\)} + 
    \frac{p + 1}{\eta_{d}^{(h)}}.
    \nonumber
  \end{align}
  where $K_{\nu}(x)$ denotes the modified Bessel function of the second kind.
  A method for fast and numerically stable calculation of ratios of
  modified Bessel functions of the second kind, as in 
  (\ref{eq:ratiomodifiedbessel}), is given in SM Section 
  \ref{sm-sec:ratiosmodifiedbessels}.
  
  \subsection{Empirical Bayes}\label{sec:empiricalbayes}
  We parametrised the prior 
  mean of the $\gamma_{jd}^2$ as $\phi_{jd}=(\mathbf{c}_{jd} \tr 
  \bm{\alpha}_{\text{feat}})^{-1}$ and the prior mean of $\tau_d^2$ as
  $\chi_{d}=(\mathbf{z}_{d} \tr \bm{\alpha}_{\text{drug}})^{-1}$.
  This parametrisation allows us to include feature and drug covariates,
  both continuous and discrete, into the 
  model. Additionally, it reduces the number of hyperparameters from 
  $pD$ to $|\bm{\alpha}_{\text{feat}}| + |\bm{\alpha}_{\text{drug}}| + 2$. The
  Bayesian model
  then requires the specification of the hyperparameters 
  $\bm{\alpha}=\begin{bmatrix} \bm{\alpha}_{\text{feat}} \tr & 
  \bm{\alpha}_{\text{drug}} \tr 
  \end{bmatrix} \tr$ and $\bm{\lambda} = \begin{bmatrix} \lambda_{\text{feat}} &
  \lambda_{\text{drug}}
  \end{bmatrix} \tr$. These are
  abstract and hard to interpret parameters 
  for which we generally lack expert knowledge. They do, however, have a 
  significant influence on the shape of the posterior distribution. We 
  therefore propose to estimate these hyperparameters by empirical Bayes.
  In our case, this results in an objective and data-driven inclusion of the
  external feature and drug covariates.
	The canonical method for empirical Bayes is to maximise the marginal 
	likelihood with respect to the hyperparameters. In 
	\cite{casella_empirical_2001} the marginal likelihood is maximised by an EM 
	algorithm:
	\begin{align*}
	  \bm{\alpha}^{(l+1)},\bm{\lambda}^{(l+1)} & = \underset{\bm{\alpha},
	  \bm{\lambda}}{\argmax}\E_{\cdot | \Y} 
	  [\log p(\Y, \B, \bm{\Gamma}^2, \bm{\tau}^2, \bsigma^2) | \bm{\alpha}^{(l)},
	  \bm{\lambda}^{(l)}] \\
	  & = \underset{\bm{\alpha},\bm{\lambda}}{\argmax} 
	  \E_{\cdot | \Y} [\log \pi (\bm{\Gamma}^2) | \bm{\alpha}_{\text{feat}}^{(l)},
	  \bm{\lambda}_{\text{feat}}^{(l)}] + 
	  \E_{\cdot | \Y} [\log \pi (\bm{\tau}^2) | 
	  \bm{\alpha}_{\text{drug}}^{(l)}, \bm{\lambda}_{\text{drug}}^{(l)}],
	\end{align*}
	where $\Y = \begin{bmatrix} \y_1 & \cdots & \y_D \end{bmatrix}$,
	$\B = \begin{bmatrix} \bbeta_1, \dots, \bbeta_D \end{bmatrix}$, 
	$\bm{\tau}^2 = \begin{bmatrix} \tau^2_1 & \cdots & 
	\tau^2_D \end{bmatrix} \tr$,
	$\bsigma^2 = \begin{bmatrix} \sigma^2_1 & \cdots & 
	\sigma^2_D \end{bmatrix} \tr$,
	and $\bm{\Gamma}^2 = \begin{bmatrix} \bgamma^2_1 & \cdots & \bgamma^2_D 
	\end{bmatrix}$,
	and the expectation is with respect to the joint posterior. In our case, this 
	posterior is not available in closed form, which renders the expectation 
	difficult. While \cite{casella_empirical_2001} suggests to approximate the 
	expectation by a Monte Carlo sample, we propose to use the variational Bayes 
	approximation developed in Section \ref{sec:variationalbayes}:
	$$
	\bm{\alpha}^{(l+1)},\bm{\lambda}^{(l+1)} = 
	\underset{\bm{\alpha},\bm{\lambda}}{\argmax}\E_{Q^{(l)}} 
	[\log \pi(\bm{\Gamma}^2)| \bm{\alpha}_{\text{feat}}^{(l)}, 
	\bm{\lambda}_{\text{feat}}^{(l)}] + 
	\E_{Q^{(l)}} [\log \pi(\bm{\tau}^2)| 
	\bm{\alpha}_{\text{drug}}^{(l)}, \bm{\lambda}_{\text{drug}}^{(l)}],
	$$
	where now the expectation is with respect to the converged variational 
	posterior $Q^{(l)}=\prod_{d=1}^D Q_d^{(l)}$. 
	
	If we stack the drug and feature covariates:
	\begin{align*}
	  \mathbf{C} = 
	  \begin{bmatrix} \mathbf{c}_{11} \tr \\ 
	    \vdots \\
	    \mathbf{c}_{p1} \tr \\
	    \vdots \\
	    \mathbf{c}_{1D} \tr \\
	    \vdots \\
	    \mathbf{c}_{pD} \tr
	  \end{bmatrix} \text{ and }
	  \mathbf{Z} =
	  \begin{bmatrix} \mathbf{z}_{1} \tr \\ 
	    \vdots \\
	    \mathbf{z}_D \tr
	  \end{bmatrix},
	\end{align*}
	the empirical Bayes updates are given by:
  \begin{align*}
    \bm{\alpha}^{(l+1)}_{\text{feat}} & = \[ \mathbf{C} \tr \diag 
    (e_{jd}^{(l)}) \mathbf{C} \]^{-1} \mathbf{C} \tr 
    \mathbf{1}_{pD \times 1}, \\
    \lambda^{(l+1)}_{\text{feat}} & = pD \[ \sum_{d=1}^D \sum_{j=1}^{p} 
    b_{jd}^{(l)} + (\bm{\alpha}^{(l+1)}_{\text{feat}}) \tr \mathbf{C} \tr 
    \diag (e_{jd}^{(l)}) \mathbf{C} \bm{\alpha}^{(l+1)}_{\text{feat}} - 
    2 (\bm{\alpha}^{(l+1)}_{\text{feat}}) \tr \mathbf{C} \tr 
    \mathbf{1}_{pD \times 1} \]^{-1},\\
    \bm{\alpha}^{(l+1)}_{\text{drug}} & = \[ \mathbf{Z} \tr \diag 
    (f_{d}^{(l)}) \mathbf{Z} \]^{-1} \mathbf{Z} \tr 
    \mathbf{1}_{D \times 1}, \\
    \lambda^{(l+1)}_{\text{drug}} & = D \[ \sum_{d=1}^D g_{d}^{(l)} + 
    (\bm{\alpha}^{(l+1)}_{\text{drug}}) \tr \mathbf{Z} \tr \diag (f_{d}^{(l)})
    \mathbf{Z} \bm{\alpha}^{(l+1)}_{\text{drug}} - 
    2 (\bm{\alpha}^{(l+1)}_{\text{drug}}) \tr \mathbf{Z} \tr 
    \mathbf{1}_{D \times 1} \]^{-1},
  \end{align*}
  where 
  \begin{align*}
    e_{jd}^{(l)} & = \E_{Q^{(l)}}(\gamma_{jd}^2| 
    \bm{\alpha}_{\text{feat}}^{(l)}, 
    \lambda_{\text{feat}}^{(l)}) = (b_{jd}^{(l)} - 
    2/\delta_{jd}^{(l)}) \cdot \delta_{jd}^{(l)} (\phi_{jd}^{(l)})^2/
    \lambda_{\text{feat}}^{(l)}, \\
    f_{d}^{(l)} & = \E_{Q^{(l)}}(\tau_{d}^2| \bm{\alpha}_{\text{drug}}^{(l)}, 
    \lambda_{\text{drug}}^{(l)}) = (g_{d}^{(l)} - 
    (p + 1)/\eta_{d}^{(l)}) \cdot \eta_{d}^{(l)} (\chi_{d}^{(l)})^2/
    \lambda_{\text{drug}}^{(l)}.
  \end{align*}
  
  To ensure proper and unbiased shrinkage, intercepts are included in 
  $\bm{\alpha}_{\text{feat}}$ and $\bm{\alpha}_{\text{drug}}$. This is achieved 
  by appending both
  $\mathbf{C}$ and $\mathbf{Z}$ with a column of 
  ones. These intercepts are roughly interpreted as the
  expected prior precisions $\E(\gamma_{jd}^{-2})$ and $\E(\tau_d^{-2})$
  if the feature and drug covariates are all zero. 
  Likewise, an $\alpha$ corresponding to an external covariate may be
  interpreted as an additive effect of the external covariate on the prior
  expected precision. So an $\alpha=1$ translates to an increase in expected 
  prior precision of
  1 for every increase in the external covariate of 1,
	keeping all the other external covariates fixed.
  
  Variational Bayes approximations are known to underestimate posterior 
  variances \cite[]{rue_approximate_2009,consonni_mean-field_2007,
  bishop_pattern_2006,wang_inadequacy_2005}. If posterior variances are 
  required, 
  we suggest generating samples from the posterior with fixed hyperparameters
  estimates (after the procedure described in Section \ref{sec:empiricalbayes}
  has converged). A Gibbs sampler is described in 
  SM Section \ref{sm-sec:gibbssampler}. Alternatively, we provide an
  implementation of the proposed model in stan using the R package rstan 
  \cite[]{guo_rstan:_2018} at 
  \url{https://github.com/magnusmunch/cambridge}.
	
	\section{Simulations}\label{sec:simulations}
	\subsection{Setup}
	This section investigates the empirical 
	Bayes estimation properties of the model in a simulated setting; its main aim 
	is to assess hyperparameter estimation. It is a data based simulation,
	wherein the responses are simulated from a synthetic model, but the
	features are taken from the real GDSC data introducted in Section 
	\ref{sec:gdsc}. 
	The real GDSC features contain strong collinearities. Such strong
	collinearities in the design matrix impede correct parameter estimation. 
	We therefore replace the ambition of correctly estimating the $\beta_{jd}$
	with the more modest aim of approximately correct estimation of the 
	hyperparameters.
	
	A pre-processing step selects 100 features with largest variance, while the 
	168 drug sensitivities for 482 cell lines are 
	simulated from model (\ref{eq:likelihood}) and 
	(\ref{eq:prior}). We draw the error
	variances as $\forall d: \sigma_d^2 \sim \Gamma^{-1}(3,2)$,
	such that the prior $\sigma_d^2$ mean and variance are both one. 
	We consider the following three scenarios for the simulation of the 
	drug and feature variance components:
	\begin{itemize}
	  \item{Scenario 1 fixes $\forall d: \tau_d^2=1$ and draws the 
	    $\gamma_{jd}^2$ according to model (\ref{eq:prior}). We
	    create four external dummy feature covariates that code for four 
	    randomly assigned approximately equally sized groups of features. We set
	    $\bm{\alpha}_{\text{feat}}$ such that the $\gamma_{jd}^2$ of the four
	    groups of features have prior means $\phi_{jd} \in \{ 1, 1/2, 1/4, 1/8 \}$
	    ($\bm{\alpha}_{\text{feat}}=\begin{bmatrix} 1 & 1 & 3 & 7 \end{bmatrix} 
	    \tr$). The prior scale parameter is set to $\lambda_{\text{feat}}=1$}.
	  \item{Scenario 2 fixes $\forall j,d: \gamma_{jd}^2=1$ and draws the 
	    $\tau_d^2$ according to model (\ref{eq:prior}), following a procedure
	    similar to the procedure for the $\gamma_{jd}^2$ in Scenario 1: we 
	    randomly create four groups of drugs with corresponding external drug 
	    dummy variables and set $\bm{\alpha}_{\text{drug}}=\begin{bmatrix} 
	    1 & 1 & 3 & 7 \end{bmatrix} \tr$, such that we have $\chi_{d} \in 
	    \{ 1, 1/2, 1/4, 1/8 \}$. The scale is set to $\lambda_{\text{drug}}=1$.}
	  \item{Scenario 3 combines the procedures from Scenarios 1 and 2 to draw both
	    the $\gamma_{jd}^2$ and $\tau_d^2$ according to (\ref{eq:prior}).}
	\end{itemize}
	We estimate four models: (i) the NIG model that only includes an intercept in
	the external covariates, called NIG$_{\text{f}}^-$, NIG$_{\text{d}}^-$, or
	NIG$_{\text{f}+\text{d}}^-$, depending on which variance components are 
	estimated (feature, drug, or both in Scenarios 1, 2, and 3, respectively),
	(ii) the NIG model estimated as in Section \ref{sec:estimation} that includes
	all external covariates, called NIG$_{\text{f}}$, NIG$_{\text{d}}$, or
	NIG$_{\text{f}+\text{d}}$, again depending on which variance components are 
	estimated, and frequentist (iii) lasso and (iv) ridge models.
	Exclusion of the external covariates as in the NIG$_{\text{f}}^-$, 
	NIG$_{\text{d}}^-$, and NIG$_{\text{f}+\text{d}}^-$ models
	amounts to direct estimation of common expected 
	prior means $\phi$ and/or $\chi$, instead of regression estimates for the
	$\phi_{jd}$ and/or $\chi_d$ as in the 
	NIG$_{\text{f}}$, NIG$_{\text{d}}$, and NIG$_{\text{f}+\text{d}}$ models. 
	The lasso and ridge models were fit using the \texttt{R} package 
	\texttt{glmnet} \cite[]{friedman_regularization_2010} with cross validated
	penalty parameters.
	All models are fit on 241 randomly selected cell lines, while performance
	measures are estimated from the remaining 241 cell lines. We repeat every
	simulation Scenario 50 times.
	
	\subsection{Results}
<<simulation_gdsc_res1>>=
res <- read.table("results/simulations_gdsc_res1.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp
true.phi <- mean(1/(c(1, 1, 3, 7) %*% t(unname(model.matrix(~ factor(1:4))))))
est.phi <- median(1/res[rownames(res)=="alphaf0", 1])
est.lambdaf <- apply(res[rownames(res)=="lambdaf", c(1, 2)], 2, median)


tab <- cbind(apply(res[rownames(res)=="emsel", ], 2, median),
             apply(res[rownames(res)=="emseh", ], 2, median),
             apply(res[rownames(res)=="emse", ], 2, median),
             apply(res[rownames(res)=="pmse", ], 2, median))
tab <- round(tab, 3)
tab[which(tab[, 1]==min(tab[, 1], na.rm=TRUE)), 1] <- 
  paste0("\\textbf{", tab[which(tab[, 1]==min(tab[, 1], na.rm=TRUE)), 1], "}")
tab[which(tab[, 2]==min(tab[, 2], na.rm=TRUE)), 2] <- 
  paste0("\\textbf{", tab[which(tab[, 2]==min(tab[, 2], na.rm=TRUE)), 2], "}")
tab[which(tab[, 3]==min(tab[, 3], na.rm=TRUE)), 3] <- 
  paste0("\\textbf{", tab[which(tab[, 3]==min(tab[, 3], na.rm=TRUE)), 3], "}")
tab[which(tab[, 4]==min(tab[, 4], na.rm=TRUE)), 4] <- 
  paste0("\\textbf{", tab[which(tab[, 4]==min(tab[, 4], na.rm=TRUE)), 4], "}")
colnames(tab) <- c("$\\text{EMSE}_{\\text{bottom}}$",
                   "$\\text{EMSE}_{\\text{top}}$",
                   "$\\text{EMSE}_{\\text{total}}$",
                   "$\\text{PMSE}$")     
rownames(tab)[c(1, 2)] <- c("NIG$_{\\text{f}}^-$",
                            "NIG$_{\\text{f}}$")
@	
  Figure \ref{fig:simulations_gdsc_est1} shows the estimated
	$\bm{\alpha}_{\text{feat}}$ together with its true value for NIG$_{\text{f}}$ 
	in Scenario 1 of the simulation study (fixed $\tau_d^2$). Figure 
	\ref{fig:simulations_gdsc_est1}a shows that 
	$\bm{\alpha}_{\text{feat}}$ is slightly but 
	consistently underestimated. On the $\phi_{jd}$ scale this underestimation is
	almost undetectable as depicted in Figure
	\ref{fig:simulations_gdsc_est1}b. $\lambda_{\text{feat}}$ is slightly 
	overestimated with median estimate \Sexpr{round(est.lambdaf[2], 2)} (the
	true value is 1). The overestimated $\lambda_{\text{feat}}$ translates 
	into underestimated $\gamma_{jd}^2$ variances. Underestimation of variances is 
	a common phenomenon in variational Bayes approximations. In addition, 
	$\lambda_{\text{feat}}$ is a higher level scale
	parameter, which are, in general, hard to estimate. 
<<simulations_gdsc_est1, fig.cap="Simulation results for Scenario 1 ($\\tau^2_d$ fixed): (a) estimated and true values for $\\alpha_{\\text{feat}}$ and, (b) estimated and true values for prior means $\\phi_{jd}$.", out.width="100%", fig.asp=1/2>>= 
@
	
	Model NIG$_{\text{f}}^-$ (that excludes the external covariates) gives a 
	median $\phi$ estimate of \Sexpr{round(est.phi, 2)},  
	equal to the true mean of the $\phi_{jd}$, \Sexpr{round(true.phi, 2)}. 
	$\lambda_{\text{feat}}$ is slightly underestimated with median estimate
	\Sexpr{round(est.lambdaf[1], 2)} (the true value is one) and thus 
	overestimates the 
	prior $\gamma_{jd}^2$ variance. This is likely to compensate for the common
	prior mean $\phi$, that is not able to capture part of the variation in the 
	true feature specific $\phi_{jd}$. 

	Table \ref{tab:simulations_gdsc_tab1} displays median estimation MSE 
	(EMSE) and prediction MSE (PMSE) for simulation Scenario 1, calculated on an 
	independent test data set. EMSE is further split into the contribution of the
	bottom 90\% of the $\beta_{jd}$ in terms of size (in absolute value) and the 
	top 10\% of the $\beta_{jd}$ in size. The table includes as a reference the 
	null model, which is an empty model that only fits the means of the $\y_{d}$.
	Estimation MSE (EMSE) and prediction MSE (PMSE) give measures of predictive 
	performance. Focussing on MSE, we see that the NIG$_{\text{f}}$ model, 
	that includes the external covariates, performs best, both in terms of 
	estimation and prediction. The lower MSEs confirm that the model that
	includes the external data (NIG$_{\text{f}}$) learns the underlying structure 
	in the data better than model NIG$_{\text{f}}^-$ that does not include the
	external data. Furthermore, we see that estimation of the larger $\beta_{jd}$ 
	is where ridge loses the most compared to the NIG and lasso models. 
	The heavier tails of the NIG and lasso priors allow for larger $\beta_{jd}$
	estimates and thus lower EMSE for these $\beta_{jd}$.
<<simulations_gdsc_tab1>>=
library(knitr, quietly=TRUE, verbose=FALSE)
suppressWarnings(library(kableExtra))
options(knitr.kable.NA="", digits=3)
kable(tab, col.names=colnames(tab), align="r",
      caption="Median performance measure for simulation Scenario 1, estimated on the test data (lowest in bold).", 
      format="latex", booktabs=TRUE, escape=FALSE) %>%
  kable_styling(latex_options=c("HOLD_position"))
@

<<simulation_gdsc_res2>>=
res <- read.table("results/simulations_gdsc_res2.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp
true.chi <- mean(1/(c(1, 1, 3, 7) %*% t(unname(model.matrix(~ factor(1:4))))))
est.chi <- median(1/res[rownames(res)=="alphad0", 1])
est.lambdad <- apply(res[rownames(res)=="lambdad", c(1, 2)], 2, median)
tab <- cbind(apply(res[rownames(res)=="emsel", ], 2, median),
             apply(res[rownames(res)=="emseh", ], 2, median),
             apply(res[rownames(res)=="emse", ], 2, median),
             apply(res[rownames(res)=="pmse", ], 2, median))
tab <- round(tab, 3)
tab[which(tab[, 1]==min(tab[, 1], na.rm=TRUE)), 1] <- 
  paste0("\\textbf{", tab[which(tab[, 1]==min(tab[, 1], na.rm=TRUE)), 1], "}")
tab[which(tab[, 2]==min(tab[, 2], na.rm=TRUE)), 2] <- 
  paste0("\\textbf{", tab[which(tab[, 2]==min(tab[, 2], na.rm=TRUE)), 2], "}")
tab[which(tab[, 3]==min(tab[, 3], na.rm=TRUE)), 3] <- 
  paste0("\\textbf{", tab[which(tab[, 3]==min(tab[, 3], na.rm=TRUE)), 3], "}")
tab[which(tab[, 4]==min(tab[, 4], na.rm=TRUE)), 4] <- 
  paste0("\\textbf{", tab[which(tab[, 4]==min(tab[, 4], na.rm=TRUE)), 4], "}")
colnames(tab) <- c("$\\text{EMSE}_{\\text{bottom}}$",
                   "$\\text{EMSE}_{\\text{top}}$",
                   "$\\text{EMSE}_{\\text{total}}$",
                   "$\\text{PMSE}$")      
rownames(tab)[c(1, 2)] <- c("NIG$_{\\text{d}}^-$",
                            "NIG$_{\\text{d}}$")
@	
  Figure \ref{fig:simulations_gdsc_est2} shows the estimated
	$\bm{\alpha}_{\text{drug}}$ together with its true value for NIG$_{\text{d}}$ 
	in Scenario 2 of the simulation study (fixed $\gamma_{jd}^2$). They are
	estimated without bias.
	Median $\lambda_{\text{drug}}=\Sexpr{round(est.lambdad[2], 2)}$ is slightly 
	overestimated compared to true value one. 
	The median $\chi$ estimate in the NIG$_{\text{d}}^-$ model is 
	\Sexpr{round(est.chi, 2)},  
	which is almost equal to the true mean \Sexpr{round(true.chi, 2)}. 
	Scale $\lambda_{\text{drug}}$ is underestimated with median estimate
	\Sexpr{round(est.lambdad[1], 2)} compared to the true value one.
	Median performance measures for simulation Scenario 2 are displayed in Table
	\ref{tab:simulations_gdsc_tab2}. Again, we see that NIG$_{\text{d}}$ performs
	best in terms of MSE. However, NIG$_{\text{d}}^-$ performance is the same 
	here, indicating that the external covariates do not add much here.
	Similarly as in Scenario 1, most of the estimation performance gain over ridge
	is through the top 10\% of the $\beta_{jd}$.
<<simulations_gdsc_est2, fig.cap="Simulation results for Scenario 2 ($\\gamma^2_{jd}$ fixed): (a) estimated and true values for $\\alpha_{\\text{feat}}$ and, (b) estimated and true values for prior means $\\phi_{jd}$.", out.width="100%", fig.asp=1/2>>= 
@
	
<<simulations_gdsc_tab2>>=
library(knitr, quietly=TRUE, verbose=FALSE)
suppressWarnings(library(kableExtra))
options(knitr.kable.NA="", digits=3)
kable(tab, col.names=colnames(tab), align="r",
      caption="Median performance measure for simulation Scenario 2, estimated on the test data (lowest in bold).", 
      format="latex", booktabs=TRUE, escape=FALSE) %>%
  kable_styling(latex_options=c("HOLD_position"))
@

<<simulation_gdsc_res3>>=
res <- read.table("results/simulations_gdsc_res3.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp
true.chi <- mean(1/(c(1, 1, 3, 7) %*% t(unname(model.matrix(~ factor(1:4))))))
est.chi <- median(1/res[rownames(res)=="alphad0", 1])
est.lambdad <- apply(res[rownames(res)=="lambdad", c(1, 2)], 2, median)
true.phi <- mean(1/(c(1, 1, 3, 7) %*% t(unname(model.matrix(~ factor(1:4))))))
est.phi <- median(1/res[rownames(res)=="alphaf0", 1])
est.lambdaf <- apply(res[rownames(res)=="lambdaf", c(1, 2)], 2, median)

phi.ratio <- apply(apply(sapply(0:3, function(s) {
  res[rownames(res)==paste0("alphaf" ,s), 2]}) %*% 
    t(cbind(1, rbind(0, diag(3)))), 2, function(m) {
      res[rownames(res)=="alphaf0", 2]/m}), 2, median)
chi.ratio <- apply(apply(sapply(0:3, function(s) {
  res[rownames(res)==paste0("alphad" ,s), 2]}) %*% 
    t(cbind(1, rbind(0, diag(3)))), 2, function(m) {
      res[rownames(res)=="alphad0", 2]/m}), 2, median)
true.ratio <- as.numeric(1/(c(1, 1, 3, 7) %*% 
                              t(unname(model.matrix(~ factor(1:4))))))

tab <- cbind(apply(res[rownames(res)=="emsel", ], 2, median),
             apply(res[rownames(res)=="emseh", ], 2, median),
             apply(res[rownames(res)=="emse", ], 2, median),
             apply(res[rownames(res)=="pmse", ], 2, median))
tab <- round(tab, 3)
tab[which(tab[, 1]==min(tab[, 1], na.rm=TRUE)), 1] <- 
  paste0("\\textbf{", tab[which(tab[, 1]==min(tab[, 1], na.rm=TRUE)), 1], "}")
tab[which(tab[, 2]==min(tab[, 2], na.rm=TRUE)), 2] <- 
  paste0("\\textbf{", tab[which(tab[, 2]==min(tab[, 2], na.rm=TRUE)), 2], "}")
tab[which(tab[, 3]==min(tab[, 3], na.rm=TRUE)), 3] <- 
  paste0("\\textbf{", tab[which(tab[, 3]==min(tab[, 3], na.rm=TRUE)), 3], "}")
tab[which(tab[, 4]==min(tab[, 4], na.rm=TRUE)), 4] <- 
  paste0("\\textbf{", tab[which(tab[, 4]==min(tab[, 4], na.rm=TRUE)), 4], "}")
colnames(tab) <- c("$\\text{EMSE}_{\\text{bottom}}$",
                   "$\\text{EMSE}_{\\text{top}}$",
                   "$\\text{EMSE}_{\\text{total}}$",
                   "$\\text{PMSE}$")     
rownames(tab)[c(1, 2)] <- c("NIG$_{\\text{f}+\\text{d}}^-$",
                            "NIG$_{\\text{f}+\\text{d}}$")
@	
  In Figure \ref{fig:simulations_gdsc_est3} the 
	$\bm{\alpha}_{\text{feat}}$ and $\bm{\alpha}_{\text{drug}}$ estimated by 
	NIG$_{\text{f}+\text{d}}$
	are displayed together with their true values for
	simulation Scenario 3. $\bm{\alpha}_{\text{feat}}$ are underestimated, while 
	$\bm{\alpha}_{\text{drug}}$ are overestimated. These biases seem to 
	be consistent though. The median ratios $\phi_1$ to $\phi_2$, $\phi_3$, 
	$\phi_4$ and $\chi_1$ to $\chi_2$, $\chi_3$, 
	$\chi_4$ are \Sexpr{round(phi.ratio, 2)[-1]} and 
	\Sexpr{round(chi.ratio, 2)[-1]}, respectively, compared to their true values
	\Sexpr{round(true.ratio, 2)[-1]}. So in a relative sense, the estimates are
	correct. Median $\lambda_{\text{feat}}$ and $\lambda_{\text{drug}}$ are
	are over- and underestimated with \Sexpr{round(est.lambdaf[2], 2)}
	and \Sexpr{round(est.lambdad[2], 2)}, respectively, compared to their true
	value 1. The NIG$_{\text{f}+\text{d}}^-$ model is also consistently over- 
	and underestimating $\phi$
	and $\chi$ with estimates \Sexpr{round(est.phi, 2)} and 
	\Sexpr{round(est.chi, 2)}, respectively (compared to the true mean 
	\Sexpr{round(true.chi, 2)}). NIG$_{\text{f}+\text{d}}^-$ also over- and
	underestimates 
	$\lambda_{\text{feat}}$ and $\lambda_{\text{drug}}$ with median estimates
	\Sexpr{round(est.lambdaf[1], 2)}
	and \Sexpr{round(est.lambdad[1], 2)}, respectively. Table 
	\ref{tab:simulations_gdsc_tab3}, with median performance measures, shows
	that NIG$_{\text{f}+\text{d}}$ outperforms the other models in terms of
	prediction and estimation MSE, with again most of the edge over ridge in the
	top 10\% of the $\beta_{jd}$.
<<simulations_gdsc_est3, fig.cap="Simulation results for Scenario 3: (a) estimated and true values for $\\alpha_{\\text{feat}}$ and, (b) estimated and true values for prior means $\\phi_{jd}$.", out.width="100%", fig.asp=1>>= 
@
	
<<simulations_gdsc_tab3>>=
library(knitr, quietly=TRUE, verbose=FALSE)
suppressWarnings(library(kableExtra))
options(knitr.kable.NA="", digits=3)
kable(tab, col.names=colnames(tab), align="r",
      caption="Median performance measure for simulation Scenario 3, estimated on the test data (lowest in bold).", 
      format="latex", booktabs=TRUE, escape=FALSE) %>%
  kable_styling(latex_options=c("HOLD_position"))
@

	\section{GDSC data}\label{sec:gdsc}
	\subsection{Primary data}
	The GDSC project's \cite[]{yang_genomics_2013} aim is ``to improve cancer 
	treatments by discovering therapeutic biomarkers that can be used to identify 
	patients most likely to respond to anticancer drug". Part of the project is to
	screen $>1000$ human cancer cell lines for drug sensitivities. The cell lines
	have been genetically characterised and several drug sensitivity measures are
	recorded.
	The data is freely available from \cite{garnett_systematic_2012} and consist 
	of: (i) the sensitivity measures of the cell lines 
	to the drugs, (ii) annotation of the 
	screened compounds, and (iii) the cell lines' genomic profile (mutations, 
	copy numbers, methylation profiles, and gene expression). We will attempt to 
	predict drug sensitivities of the cell lines, as quantified by half maximal
	inhibitory concentration (IC50), using the
	gene expression data. Other choices of sensitivity measures than IC50 are 
	possible, but a discussion on the pros and cons of different sensitivity
	measures is beyond the aim of this paper. We have used the version of the 
	data that is presented in \cite{iorio_landscape_2016}.
	We average repeated measures over cell line-drug combinations and model the
	logarithm of the IC50 values. In the following, IC50 refers 
	to the log-transformed values. 
	After iteratively removing the cell line or drug with the highest proportion 
	of missings until no more missing values remain, we end up with IC50 
	estimates for 482 cell lines on 168 drugs. 
	
	A comprehensive penalized regression
	approach such as in 
	{\cite{mai_composite_2019} uses most of the available molecular markers 
	instead of just gene expression (68 mutations, 426 copy numbers, 
	and 2602 gene expression levels for 498 cell lines and 97 drugs). The 
	current version of our software is not able to handle such problem sizes,
	due to the repeated calculation of posterior parameters in the empirical
	Bayes iterations. Currently, problems with number of outcomes in the hundreds
	and number of features and samples up to a thousand are computationally 
	feasible.

	\subsection{External data}
	Two ternary drug covariates are available: the developmental stage
	(experimental, in clinical development, or clinically approved) of the
	drugs and the action (unkown, cytotoxic, or targeted) of the drugs. 
	Furthermore, we have a binary feature covariate available that indicates
	whether a gene belongs to the drug target pathway. The drug covariates are 
	taken
	directly from the GDSC database's annotation file. The feature covariate
	was created by comparing the target pathways in the GDSC annotation to
	the KEGG \cite[]{kanehisa_kegg:_2000} and reactome 
	\cite[]{fabregat_reactome_2018} repositories. The external covariates are 
	dummy-coded with
	reference categories clinically approved drugs, cytotoxic drugs, and 
	features that are not in the target pathway.
	
	We expect that drugs that have been clinically approved are easiest to 
	predict and hence yield the largest prior $\beta_{jd}$ variances, followed by
	the drugs in clinical development, and the experimental drugs. Likewise we 
	expect the targeted drugs to yield the largest prior $\beta_{jd}$ variances, 
	followed by the cytotoxic drugs, and the unkown target drugs. For the
	feature covariate, we expect that genes that are in the pathway of the drug
	are more predictive than genes that are not, i.e., 
	they have larger prior $\beta_{jd}$ variances than drugs that are not in 
	the pathway. Note that large $\beta_{jd}$ variances translate to large
	prior $\gamma_{jd}^2$ and $\tau_d^2$ means.
	
	\subsection{Results}
	A preliminary step selects, for each
	drug separately, 50 features that are not in the pathway of the drug with
	maximum variance over the cell lines and 50 features that are in the pathway 
	of the drug with maximum variance over the cell lines. If less than 50 
	features in the pathway are available,
	we select all available features in the pathway. This strategy ensures
	that the features are somewhat balanced with respect to the external feature
	covariate, i.e., pathway membership. 
	
	To compute performance measures, we randomly split the data into training and
	test sets, containing 
	241 cell lines each, estimate the models on the training data and compute 
	performance measures on the test data. We repeated this procedure 50 times
	and average performance measures over the splits.
	The estimated 
	models are: four models like in (\ref{eq:prior}) but with different external 
	variables, named NIG$_{\text{f}+\text{d}}^-$, NIG$_{\text{f}+\text{d}}$,
	NIG$_{\text{f}}$, and NIG$_{\text{d}}$ in the following. The nomenclature is
	the same as in Section \ref{sec:simulations}, i.e., 
	NIG$_{\text{f}+\text{d}}^-$ excludes the external covariates and estimates
	one overall $\phi$ and one overall $\chi$, while NIG$_{\text{f}+\text{d}}$ 
	uses the external covariates to estimate both $\bm{\alpha}_{\text{feat}}$ and
	$\bm{\alpha}_{\text{drug}}$. NIG$_{\text{f}}$ and NIG$_{\text{d}}$ estimate 
	only $\bm{\alpha}_{\text{feat}}$ or $\bm{\alpha}_{\text{drug}}$, respectively,
	keeping either $\forall d: \tau_d^2=1$ or $\forall j,d: \gamma_{jd}^2=1$
	fixed. In the language of \cite{bernardo_shrink_2011} as introduced in 
	Section \ref{sec:introduction}, models NIG$_{\text{f}}$ and NIG$_{\text{d}}$
	may be described as local and global shrinkage rules, respectively, as 
	opposed to the global-local shrinkage models NIG$_{\text{f}+\text{d}}$ and
	NIG$_{\text{f}+\text{d}}^-$.
	
	In addition, we estimate regular lasso and ridge models, and a
	bSEM model \cite[]{leday_gene_2017,kpogbezan_empirical_2017}. The bSEM model 
	is similar to the NIG model in that it
	draws the $\beta_{jd}$ from a conditionally normal distribution, but
	fixes $\tau_d^2=1$ and draws
	$\gamma_{jd}^2 \sim \Gamma^{-1} (\phi_{jd}, \lambda_{jd})$, instead of
	$\gamma_{jd}^2 \sim \mathcal{IG} (\phi_{jd}, \lambda_{jd})$, where 
	$\Gamma^{-1} (\phi, \lambda)$ denotes the inverse gamma distribution with 
	shape $\phi$ and scale $\lambda$. Additionally, the bSEM model can
	only incorporate one binary external covariate that divides the features into
	two groups, i.e., $\bm{\alpha}_{\text{feat}} = \begin{bmatrix} \alpha_0 &
	\alpha_1
	\end{bmatrix} \tr$ and $\bm{c}_{jd} = \begin{bmatrix} 1 & 0 \end{bmatrix}
	\tr$ if feature $jd$ belongs to group one and $\bm{c}_{jd} = \begin{bmatrix}
	1 & 1 \end{bmatrix} \tr$ if feature $jd$ belongs to group two. Both
	$\bm{\alpha}_{\text{feat}}$ and $\lambda_{\text{feat}}$ are
	estimated in a similar fashion as in Section \ref{sec:estimation}.
	The null model that fits the means of the $\y_d$ is included as a reference.

<<analysis_gdsc_fit1>>=
fit <- read.table("results/analysis_gdsc_fit1.txt", row.names=NULL)
temp <- fit[, 1]
fit <- as.matrix(fit[, -1])
rownames(fit) <- temp

res <- read.table("results/analysis_gdsc_res1.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

est1 <- cbind(fit[, 1], c(NA, 0, 0, NA, 0), fit[, 2])
est1 <- round(est1, 2)
est1[is.na(est1)] <- "-"
colnames(est1) <- c("intercept", "not in pathway", "in pathway")
est2 <- cbind(fit[, 3:5], c(NA, 0, NA, 0, NA), fit[, c(7:6)], 
              c(NA, 0, NA, 0, NA))
est2 <- round(est2, 2)
est2[is.na(est2)] <- "-"
colnames(est2) <- c("intercept", "experimental", "development", "approved",
                    "unkown", "targeted", "cytotoxic")

rownames(est1)[c(1:4)] <- rownames(est2)[c(1:4)] <-
  c("NIG$_{\\text{f}+\\text{d}}^-$", "NIG$_{\\text{f}+\\text{d}}$",
    "NIG$_{\\text{f}}$", "NIG$_{\\text{d}}$")

tab <- cbind(apply(res[rownames(res)=="pmse", ], 2, median))
tabm <- tab
tabm <- round(tabm, 3)
tabm[which(tabm[, 1]==min(tabm[, 1], na.rm=TRUE)), 1] <- 
  paste0("\\textbf{", tabm[which(tabm[, 1]==min(tabm[, 1], na.rm=TRUE)), 1], "}")
colnames(tabm) <- c("$\\text{PMSE}_{\\text{test}}$")  
rownames(tabm)[c(1:4)] <- rownames(est2)[c(1:4)] <-
  c("NIG$_{\\text{f}+\\text{d}}^-$", "NIG$_{\\text{f}+\\text{d}}$",
    "NIG$_{\\text{f}}$", "NIG$_{\\text{d}}$")

library(glmnet)
kurtosis <- function(x) {
  n <- length(x) 
  kurt <- (n + 1)*n*sum((x - mean(x))^4)/((n - 1)*(n - 2)*(n - 3)*var(x)^2) - 
    3*(n - 1)^2/((n - 2)*(n - 3))
  replace(kurt, !is.finite(kurt), NA)
}
load("results/analysis_gdsc_fit1.Rdata")
best2.semnig <- fit2.semnig$vb$mu
best1.lasso <- lapply(fit1.lasso, function(s) {
  unname(coef(s, "lambda.min")[-1, ])})
best1.ridge <- lapply(fit1.ridge, function(s) {
  unname(coef(s, "lambda.min")[-1, ])})
best1.bSEM <- lapply(fit1.bSEM$vb$beta, function(s) {s[, 2]})
kurts <- c(mean(sapply(best2.semnig, kurtosis), na.rm=TRUE), 
           mean(sapply(best1.lasso, kurtosis), na.rm=TRUE),
           mean(sapply(best1.ridge, kurtosis), na.rm=TRUE), 
           mean(sapply(best1.bSEM, kurtosis), na.rm=TRUE))
ranges <- rbind(range(unlist(lapply(1:length(
  fit2.semnig$eb$mpriord), function(d) {
    fit2.semnig$eb$mpriord[d]*fit2.semnig$eb$mpriorf[[d]]}))),
  fit1.bSEM$eb$b[nrow(fit1.bSEM$eb$b), ]/
    (fit1.bSEM$eb$a[nrow(fit1.bSEM$eb$a), ] - 1))
@
  The non-zero $\bm{\alpha}$ estimates in Tables 
  \ref{tab:analysis_gdsc_est1_feat} and \ref{tab:analysis_gdsc_est1_drug} show 
  that there is an effect of the
  external covariates. Models NIG$_{\text{f}+\text{d}}$, NIG$_{\text{f}}$, and
  bSEM estimate a negative effect for
  the external pathway covariate. This translates to a negative additive effect 
  on the expected prior precision of the $\bm{\beta}_d$, i.e., genes that are 
  found
  in the target pathway of a drug are more predictive of IC50 than genes that
  are not in the target pathway, according to expectation. Furthermore the 
  NIG$_{\text{f}+\text{d}}$
  model estimates positive effects for the experimental and developmental drugs,
  compared to the approved drugs. This translates to smaller expected 
  prior precisions for the regression parameters of approved drugs, i.e.,
  approved drugs are easiest to predict, followed by developmental and 
  experimental drugs, as expected. On the other hand, the NIG$_{\text{d}}$ 
  model, that only includes external
  drug covariates, estimates a negative effect for the developmental drugs.
  The NIG$_{\text{f}+\text{d}}$ model estimates the smallest prior 
  $\bm{\beta}_d$ precision for the
  targeted drugs, followed by cytotoxic drugs, while drugs with an unkown target
  are assigned the largest prior $\bm{\beta}_d$ precision, according
  to expectation. Somewhat surprisingly, the NIG$_{\text{d}}$ model
  estimates the smallest
  prior $\bm{\beta}_d$ precision for the cytotoxic drugs, compared to the 
  targeted and unkown drugs. We conjecture that, due to absence of a local
  feature-specific variance component, this model is not flexible 
  enough to correctly capture the full covariance structure of the 
  $\bm{\beta}_d$ and therefore incorreclty estimates $\bm{\alpha}$.
  To see the total effect of the external covariates on the $\bm{\beta}_d$ 
  prior, we calculated the range of the expected prior variances 
  (up to a multiplicative error variance) as 
  $\hat{E}(\gamma_{jd}^2 \cdot \tau_d^2) \in \[ \Sexpr{ranges[1, ]}\]$ and
  $\hat{E}(\gamma_{jd}^2 \cdot \tau_d^2) \in \[ \Sexpr{ranges[2, ]}\]$ for
  NIG$_{\text{f}+\text{d}}$ and bSEM, respectively. This confirms that the
  estimated prior 
  $\bm{\beta}_d$ variance components are in the same order of magnitude and that
  the external covariates have a detectable effect on the prior.
<<analysis_gdsc_est1_feat>>=
options(knitr.kable.NA="", digits=3)
kable(est1,
      col.names=colnames(est1), align="r",
      caption="$\\bm{\\alpha}_{\\text{feat}}$ estimates from GDSC data.",
      format="latex", booktabs=TRUE, escape=FALSE) %>%
  kable_styling(latex_options=c("HOLD_position"))
@

<<analysis_gdsc_est1_drug>>=
kable(est2,
      col.names=colnames(est2), align="r",
      caption="$\\bm{\\alpha}_{\\text{drug}}$ estimates from GDSC data.",
      format="latex", booktabs=TRUE, escape=FALSE) %>%
  kable_styling(latex_options=c("HOLD_position"))
@
  The median PMSEs and ELBOs, calculated on the test data
  are displayed in Table \ref{tab:analysis_gdsc_tab1}. In terms of prediction 
  error, ridge outperforms the other models 
  (PMSE of \Sexpr{round(tab[6, 1], 3)}), while bSEM 
  (PMSE of \Sexpr{round(tab[7, 1], 3)}) performs worse than 
  the null model (PMSE of \Sexpr{round(tab[8, 1], 3)}). Second and third best
  performing models are NIG$_{\text{f}+\text{d}}^-$ and 
  NIG$_{\text{f}+\text{d}}$ (PMSEs of \Sexpr{round(tab[1, 1], 3)} and
  \Sexpr{round(tab[2, 1], 3)}, respectively), both outperforming the lasso
  with a PMSE of \Sexpr{round(tab[5, 1], 3)}. The underperformance of the 
  NIG$_{\text{f}}$ and NIG$_{\text{d}}$ models shows the usefulness of a
  global-local shrinkage prior as opposed to only global or local shrinkage as 
  as in models NIG$_{\text{f}}$ and NIG$_{\text{d}}$.
<<analysis_gdsc_tab1>>=
kable(tabm,
      col.names=colnames(tabm), align="r",
      caption="Performance measures calculated on GDSC data (best performing model in bold).",
      format="latex", booktabs=TRUE, escape=FALSE) %>%
  kable_styling(latex_options=c("HOLD_position"))
@	
  
  Part of the differences in performance between
  NIG, lasso, and ridge may be explained with the different levels of 
  ``sparsity'' in the solution. Although NIG and ridge do not give exactly 
  sparse solutions (none of the $\bm{\beta}_d$ coefficients are set to zero), we
  may use the ``heavy-tailedness'' of the distribution of the estimated 
  $\bm{\beta}_d$ as a proxy for sparsity. A common measure of heavy-tailedness
  is excess kurtosis, calculated as \Sexpr{round(kurts[1], 3)}, 
  \Sexpr{round(kurts[2], 3)}, and \Sexpr{round(kurts[3], 3)} for the 
  NIG$_{\text{f}+\text{d}}$, 
  lasso, and ridge estimates, respectively. Evidently, lasso gives the 
  ``sparsest'' solution, followed by NIG and ridge. In some sense, this
  means that the NIG uses less of the $\bm{\beta}_d$ to predict than the ridge
  model. In addition, the penalized regression methods estimate penalty 
  parameters by cross-validation. Cross-validation directly minimises the 
  (approximate) PMSE, as opposed to empirical Bayes in the NIG that maximises 
  the (approximate) marginal likelihood, a measure of model fit. The superior 
  PMSE of ridge regression is therefore not surprising. A caveat with penalized
  regression methods is that they do not give measures of parameter uncertainty.
  NIG, on the other hand, gives the full posterior of the parameters, either
  through a variational Bayes approximation or with the Gibbs sampler from
  SM Section \ref{sm-sec:gibbssampler}. The full posterior gives direct access
  to the parameter uncertainties for a better interpretable model.
  
	\section{Discussion}\label{sec:discussion}
<<time>>=
load("results/analysis_gdsc_fit1.Rdata")
time2.semnig <- sum(fit2.semnig$time)
@	
	The preceding presents a novel model for drug sensitivity
	prediction from a set of high dimensional molecular features. The model allows
	for the inclusion of discrete and continuous external covariates on both the 
	drugs and features. Inclusion of the external information is through 
	data-driven and adaptive empirical Bayes estimation of the hyperparameters in 
	the normal inverse Gaussian prior model (\ref{eq:prior}). Estimation is 
	efficient and scales well with the number of features and samples.
	Estimation of the NIG model that includes both external drugs and feature
	covariates (NIG$_{\text{f}+\text{d}}$ in Section \ref{sec:gdsc}) on the GDSC
	data from Section
	\ref{sec:gdsc} took \Sexpr{round(time2.semnig, 2)} seconds on a 2016 
	MacBook Pro with a 2 GHz Dual-Core Intel Core i5 processor and
	8 GB of 1867 MHz LPDDR3 memory, running macOS  10.15.1.
	
	Simulations in Section \ref{sec:simulations} show that the method is 
	reasonably able to estimate the hyperparameters. Simulation Scenarios 1 and 2
	show
	that estimation of drug- and feature-specific hyperparameters is, in 
	principle, fairly accurate. However, when estimated jointly, biases may occur
	due to the interplay between the two sources of information. Nevertheless,
	the simulations show that inclusion of informative external covariates is 
	still beneficial to predictive performance. 

	The model is put into practice on the GDSC data. 
	The NIG model is competitive with convential models like lasso in 
	terms of predictive performance, but is slightly outperformed by ridge.
	We note that estimation of the penalty parameters in the latter two models
	is geared towards predictive performance, because cross-validation is used
	to train these parameters, as opposed to empirical Bayes estimation of the
	hyperparameters in the NIG model, which aims to improve model fit. In that 
	sense, the comparison metric (PMSE) is not to the benefit of the NIG model. 
	
	The results in Section
	\ref{sec:gdsc} show that the inclusion of the external covariates covariates 
	indeed substantially modifies the hyperparameters. Although, PMSE of the NIG
	model is not necessarily lower if external covariates are included, the
	resulting hyperparameter 
	estimates are still informative and may guide future research efforts.
	
	In addition, the comparison to bSEM, a Bayesian counterpart model that allows
	for the inclusion of one binary external feature covariate favours the 
	proposed NIG model. PMSEs are almost uniformly lower over the drugs for the 
	NIG model compared to bSEM. Consequently, the total PMSE is significantly
	lower for the NIG model. 
	
	A possible direction of future research to attempt
	to lower PMSE of the NIG model is the introduction of a cross-validated global
	variance component in the prior of the $\bm{\beta}_d$. If we add the 
	additional constraint that the drug and feature-specific variance components
	average to one, this cross-validated variance component may be interpreted as
	the overall level of shrinkage. This overall shrinkage level is then tailored 
	to lower PMSE, while the feature- and drug-specific variance component 
	include the external information. Other directions of future research are
	applications of the NIG
	model to different data types. One possibility is to apply the model in an 
	eQTL study, 
	where gene expressions are regressed on SNPs. Several interesting external
	covariates are available, both on the genes as well as the SNPs. An example of
	an external
	covariate for the genes is genelength, where we suspect that longer genes are
	harder to predict. The distance of the SNP to the gene is an example of an
	external SNP covariate, where the expectation is that SNPs further from the 
	gene are less predictive of that gene's expression.
	
	\section*{Supplementary Material}
	Supplementary Material is available online from 
	\url{https://github.com/magnusmunch/cambridge}. 
	
	\section*{Reproducible Research}
	All results and documents may be recreated from 
	\url{https://github.com/magnusmunch/cambridge}.
	
	\section*{Acknowledgements}
	% We thank Zhi Zhao and Leiv Tore Salte R{\o}nneberg from the Oslo Centre for 
	% Biostatistics and Epidemiology for their suggestions on the pre-processing of
	% the GDSC data. 
	MM visited SR and GL on a travel grant funded the Amsterdam Public Health 
	institute's Methodology program.
	\textit{Conflict of Interest}: None declared.
	
	\bibliographystyle{author_short3} 
	\bibliography{refs}
	
	\section*{Session info}

<<r session-info, eval=TRUE, echo=TRUE, tidy=TRUE>>=
devtools::session_info()
@

\end{document}