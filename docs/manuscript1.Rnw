% set options
<<settings, include=FALSE, echo=FALSE>>=
opts_knit$set(root.dir="..", base.dir="../figs")
opts_chunk$set(fig.align='center', fig.path="../figs/", 
               echo=FALSE, cache=FALSE, message=FALSE, fig.pos='!ht')
knit_hooks$set(document=function(x) {
  sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed=TRUE)})
options(knitr.kable.NA="")
@

% read figure code
<<figures, include=FALSE>>=
read_chunk("code/figures.R")
@

\documentclass[a4paper,hidelinks]{article}
\usepackage{bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,
rotating,caption,subcaption,natbib,appendix,titlesec,multicol,hyperref,verbatim,
bbm,algorithm,algpseudocode,pgfplotstable,threeparttable,booktabs,mathtools,
dsfont,parskip,xr,tikz}
\externaldocument[sm-]{supplement1}
\usetikzlibrary{bayesnet}

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Em}{\mathbf{E}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\Lm}{\mathbf{L}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}

% settings
\pgfplotsset{compat=1.16}
\graphicspath{{../figs/}}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Drug sensitivity prediction with normal inverse Gaussian shrinkage 
informed by external data}
\date{\today}
\author{Magnus M. M\"unch$^{1,2,3}$\footnote{Correspondence to: 
\href{mailto:m.munch@amsterdamumc.nl}{m.munch@amsterdamumc.nl}}, Mark A. van de 
Wiel$^{1,3}$, Sylvia Richardson$^{3}$, \\ and Gwena{\"e}l G. R. Leday$^{3}$}

\begin{document}

	\maketitle
	
	\noindent
	1. Department of Epidemiology \& Biostatistics, Amsterdam UMC, VU University, 
	PO Box 7057, 1007 MB Amsterdam, The Netherlands \\
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\
	3. MRC Biostatistics Unit, Cambridge Institute of Public Health, Cambridge,
	United Kingdom
	
	\begin{abstract}
		{In precision medicine, a common problem is drug sensitivity prediction from
		cancer tissue cell lines. These types of problems entail modelling 
		multivariate drug responses on high dimensional molecular feature sets in 
		typically $>1000$ cell lines. The dimensions of the problem require 
		specialised models and estimation methods. In addition, external information
		on both the drugs and the features is often available. We propose to 
		model the drug responses through a linear regression with shrinkage enforced
		through a normal
		inverse Gaussian prior. We let the prior depend on the external information,
		and estimate the model and external information dependence in an 
		empirical-variational Bayes framework. We demonstrate the usefullness of
		this model in both a simulated setting and in the publicly available
		Genomics of Drug Sensitivity in Cancer data.}
	\end{abstract}
	
	\noindent\textbf{Keywords}: Drug sensitivity; Empirical Bayes; 
	Genomics of Drug Sensitivity in Cancer (GDSC); Variational Bayes
	
	\noindent\textbf{Software available from}: 
	\url{https://github.com/magnusmunch/cambridge}
	
	\section{Introduction}\label{sec:introduction}
	Recently, promising results in precision medicine 
	have sparked an interest in cancer drug sensitivity prediction models
	\cite[]{iorio_landscape_2016}. Typically, these models 
	predict the drug sensitivity for new patients from a set of molecular 
	features.
	Development of such models is often done in well-characterised human cancer
	tissue cell lines. The current paper presents a novel drug sensitivity
	prediction model and an application to a real drug
	sensitivity data set.
	
	Development of such models from cell lines has proven to be difficult
	(see e.g., the DREAM 7 challenge in \cite{costello_community_2014}). 
	Difficulties arise, among others, from the dimensions of the problem. 
	Typically, the data contains
	hundreds of drugs, thousands of cell lines, and thousands of molecular 
	features. An example of a large database of drug responses and molecular
	features is the Genomics of Drug Sensitivity in Cancer (GDSC) data 
	\cite[]{yang_genomics_2013}, which we will further 
	investigate in Section \ref{sec:gdsc}. Other examples of such databases 
	include
	the Cancer Cell Line Encyclopedia (CCLE) \cite[]{li_landscape_2019} and 
	the US National Cancer Institute 60 human tumour cell line anticancer drug
	screen (NCI60) \cite[]{shoemaker_NCI60_2006}. The dimensions of these data
	prohibit the estimation of standard regression models and
	typically require some form of regularisation.
	
	The GDSC database contains additional information on both the drugs and 
	molecular features, such as the target 
	pathways and developmental stages of the drugs. Additional online
	repositories may provide extra information such as the molecular weight of
	the compounds or the publication signatures of the molecular features. In some
	cases, prior knowledge on the drug efficacies may be available,
	from previous experiments. We propose to include these possibly beneficial 
	information sources in the estimation of the sensitivity prediction models in 
	a data-driven manner. More specifically, we estimate a normal 
	inverse Gaussian model, where the extent of regularisation is estimated by an 
	adaptive empirical Bayes procedure, guided by the external information.
	
	We are not the first to work on drug sensitivity prediction models. Reviews on 
	the topic are \cite{azuaje_computational_2017} and \cite{ali_machine_2019}.
	\cite{zhao_structured_2019} and \cite{mai_composite_2019} consider a
	structured penalized multivariate regression approach. 
	\cite{aben_tandem:_2016} introduce a two-stage penalized regression 
	model that includes two different types of molecular features.
	\cite{ammad-ud-din_drug_2016} and \cite{costello_community_2014} tackle the
	problem through a multiple kernel learning approach. Our solution allows for
	the adaptive incorporation of the external 
  information on drugs and features. This is done by
  pooling information, both across drugs and features. Estimation of the model
  is through computational feasible variational bayes approximations, while 
  empirical Bayes estimation of tuning parameters pools information across drugs
  and features in a data-driven manner.
  
	The rest of the paper is structured as follows. In Section 
	\ref{sec:model} we introduce our model, the estimation of which is detailed
	in Section \ref{sec:estimation}. Section \ref{sec:simulations} describes a 
	simulation study that investigates the estimation of hyperparameters by the
	proposed method. In Section \ref{sec:gdsc} we analyse the
	GDSC data, and we end with a discussion in Section \ref{sec:discussion} on the 
	pros and cons of the proposed method.
	
	\section{Model}\label{sec:model}
	\subsection{Simultaneous equations model}\label{sec:SEM}
	Let $y_{id}$ be the continuous sensitivity measures 
	for cell lines $i=1,\dots, n$, and drug $d=1, \dots, D$. 
	We predict sensitivity from molecular features 
	$x_{ij}$, $j=1,\dots, p$, collected in 
	$\x_i = \begin{bmatrix} x_{i1} & \dots & x_{ip} \end{bmatrix} \tr$. 
	We assume that both covariates and responses have been centred per drug
	and regress the drug sensitivities on the molecular features:
	\begin{equation}\label{eq:likelihood}
	  y_{id} = \x_i \tr \bbeta_d + \epsilon_{id}, 
	  \text{ with } \epsilon_{id} \sim \mathcal{N}(0, \sigma_d^2),
	\end{equation}
	where the $p$-dimensional 
	$\bbeta_d = \begin{bmatrix} \beta_{1d} & \cdots & \beta_{pd} \end{bmatrix} 
	\tr$ are the drug-specific omics feature 
	effects. Note that (\ref{eq:likelihood}) gives rise to a system of 
	D linear regression equations.
	
	The cell lines used in drug response models are often taken from different
	tissues. In addition, other clinical covariates might be available. 
	To obtain unbiased feature effects, one may wish to account for these. 
	We do so by introducing unpenalized covariates, the $\beta_{jd}$ coefficients 
	of which are endowed with a flat prior. For the sake of clarity, in the
	following, such unpenalized covariates are omitted. However, the available
	software allows for their inclusion.
	
	\subsection{Bayesian prior model}\label{sec:prior}
	We carry out inference by endowing the parameters with the following priors:
	\begin{subequations}\label{eq:prior}
		\begin{align}
		  \beta_{jd} | \gamma^2_{jd}, \tau_d^2, \sigma^2_d & \sim \mathcal{N}_{p} 
		  (0, \gamma_{jd}^2 \tau_d^2 \sigma_d^2), 
		  \label{eq:betaprior}\\
		  \gamma_{jd}^{2} & \sim \mathcal{IG}(\phi_{jd}, \lambda_{\text{feat}}), 
		  \label{eq:gammaprior}\\
		  \tau_d^{2} & \sim \mathcal{IG}(\chi_d, \lambda_{\text{drug}}), 
		  \label{eq:tauprior}\\
		  \sigma_d^{2} & \sim 1/\sigma_d^{3}, \label{eq:sigmaprior}
		\end{align}
	\end{subequations}
	where $\mathcal{IG}(\phi, \lambda)$ denotes an inverse Gaussian distribution 
	with mean $\phi$ and shape $\lambda$. 
	
	In model (\ref{eq:prior}), $\gamma_{jd}^2$ in (\ref{eq:gammaprior}) denotes a 
	local variance 
	component that is supposed to capture local, feature-specific variation in the
	model parameters $\beta_{jd}$ in (\ref{eq:betaprior}), 
	while the global variance components $\tau_d^2$ in (\ref{eq:tauprior})
	capture the drug-specific, general trend in $\bm{\beta}_{d}$. Each drug 
	response is endowed with a random error variance $\sigma_d^2$, distributed
	according to (\ref{eq:sigmaprior}).
	
	Prior distributions of the form
	(\ref{eq:prior}) are often referred to as global-local shrinkage rules 
	\cite[]{bernardo_shrink_2011}, due to the multiplicative separation of the 
	prior variance into a local component $\gamma_{jd}^2$ and a global component
	$\tau_d^2$. For appropriate local shrinkage in global-local 
	shrinkage models it is important  to account for 
  different noise levels $\sigma_d^2$ by scaling the $\beta_{jd}$ variances 
  accordingly.

	The normal inverse Gaussian (NIG) prior model was introduced in 
  \cite{barndorff-nielsen_hyperbolic_1978} and since 
  \cite{barndorff-nielsen_normal_1997} it is routinely applied in mathematical 
  finance (see, e.g., \cite{kalemanova_normal_2007}). Here we extend it with an
  additional global variance component $\tau_d^2$. Supplementary Material
  (SM) Section \ref{sm-sec:prior} contains more details on the NIG prior.
  To illustrate the effect of the NIG prior on the posterior mean, we consider 
  the prior reparametrised as in 
  \cite{carvalho_handling_2009}, i.e., in terms of shrinkage weights 
  $\kappa_{jd}=1/(1 + \gamma_{jd}^2) \in (0, 1)$.
  Under the (simplified) normal means model, i.e., 
  $\X= \begin{bmatrix} \x_1 & \cdots \x_n \end{bmatrix} \tr =\I_p$, with fixed 
  $\tau_d^2=\sigma_d^2=1$, the resulting conditional
  posterior mean for the $\beta_{jd}$ is
  $\E(\beta_{jd} | y_{jd}, \kappa_{jd}) = (1 - \kappa_{jd}) y_{jd}$. 
  Thus, $\kappa_{jd}=0$ implies no shrinkage of $\beta_{jd}$ and $\kappa_{jd}=1$
  implies full shrinkage towards zero. Figure 
  \ref{fig:dens_kappa} depicts the prior on $\kappa_{jd}$ implied by 
  several choices of $\beta_{jd}$ prior.
<<dens_kappa, fig.cap="Implied prior densities $\\pi(\\kappa_{jd})$ for the (a) NIG, (b) Student's $t$, and (c) lasso priors. Different line types correspond to different hyperparameter settings. The hyperparameter settings (given in Section \\ref{sm-sec:hyperparameters} of the SM) were chosen to show some possible, distinct shapes that each of the priors can take.", out.width="100%", fig.asp=1/3>>=
@
  
  Figure \ref{fig:dens_kappa} shows that, depending on the choice
  of hyperparameters, the NIG prior can behave similarly to the Student's $t$ 
  prior (decreasing form zero, with substantial mass close to zero and 
  little mass close to one, like the
  solid lines in Figure \ref{fig:dens_kappa}a-\ref{fig:dens_kappa}b), 
  but also rather differently (dashed and dotted lines in 
  Figure \ref{fig:dens_kappa}a-\ref{fig:dens_kappa}b). Our argumentation to 
  model the $\gamma^2_{jd}$ by an inverse Gaussian 
  distribution, as has been suggested in \cite{fabrizi_specification_2016} and 
  \cite{caron_sparse_2008}, is three-fold: (i) the NIG model is more flexible 
  than the lasso prior (as seen from Figure 
  \ref{fig:dens_kappa}), (ii) the NIG prior allows to model
  the means of the $\gamma_{jd}^2$ ($\phi_{jd}$) and $\tau_{jd}^2$ 
  ($\chi_{d}$) as a function of external data 
  more conveniently than the Student's $t$ prior, as explained in Section 
  \ref{sec:empiricalbayes}, and (iii) like the horseshoe 
  \cite[]{carvalho_handling_2009}, the NIG shrinkage
  weights prior can put mass both near zero and one, a desirable property of 
  shrinkage priors \cite[]{bernardo_shrink_2011}.

  A few remarks on the choice of error variance prior are justified here: 
  many authors endow error variance components with vague gamma priors. 
  \cite{gelman_prior_2006}, 
  among others, advises against this practice. The degree of `vagueness' has a 
  large influence on the posterior, while degree of `vagueness' is a difficult 
  parameter to set. This influence is especially pronounced if the likelihood is
  relatively flat, as may be reasonably expected in the large $p$, 
  small $n$ setting. We therefore model the error variance with Jeffrey's 
  objective prior 
  \cite[]{jeffreys_invariant_1946} that does not depend on any 
  subjective specification of hyperparameters. In the derivation of our 
  Jeffrey's prior
  for the error variance, we jointly consider an unknown data mean and variance 
  \cite[]{kass_selection_1996}. This
  joint consideration results in the somewhat unorthodox $1/\sigma^3$ Jeffrey's
  prior.
	
	\subsection{External information}\label{sec:external}
	In drug sensitivity prediction models, external information on both the drugs
	and features is often available. Here, we assume this information to be 
	available as
	external feature `covariates' $\mathbf{c}_{jdg}$, for $g=1, \dots, G$, and 
	drug `covariates' $\mathbf{z}_{dh}$, for $h=1,\dots, H$. An example of a 
	(binary) feature covariate is target pathway presence, with $c_{jdg}=0$ if 
	gene $j$ is present in the 
	target pathway of drug $d$ and $c_{jdg}=1$ if it is not. An example of a 
	(ternary) drug
	covariate is developmental phase, with levels experimental phase, 
	clinical development, and approved by a governing agency.
	
	The external covariates come in through our mean models for 
	the $\gamma_{jd}^2$ and $\tau_d^2$ hyperpriors:
	$\phi_{jd} = (\mathbf{c}_{jd} \tr \bm{\alpha}_{\text{feat}})^{-1}$ and
	$\chi_d = (\mathbf{z}_{d} \tr \bm{\alpha}_{\text{drug}})^{-1}$, with 
	$\mathbf{c}_{jd} = \begin{bmatrix} c_{jd1} & \cdots & c_{jdG} \end{bmatrix}$
	and $\mathbf{z}_{d} = \begin{bmatrix} z_{d1} & \cdots & z_{dH} \end{bmatrix}$,
	where categorical external covariates are dummy coded.
	The model now requires hyperparameters $\bm{\alpha}_{\text{feat}}$, 
	$\lambda_{\text{feat}}$, $\bm{\alpha}_{\text{drug}}$, and $
	\lambda_{\text{drug}}$, which we estimate in a data-driven manner (see Section
	\ref{sec:empiricalbayes}).
  
  A representation of our model as a Bayesian DAG is given in Figure
  \ref{fig:dag}. We note that in many settings, the set of features might be
  different for different drugs. In that case the covariates are indexed by
  the drug $d$: $\X^d$, a trivial extension of model 
  (\ref{eq:likelihood}) and (\ref{eq:prior}). This extension is included in the
  available software, but for clarity it is omitted in the following. 
  
  \begin{figure}
    \centering
    \tikz{ %
      % Y
      \node[obs] (y) {$y_{id}$}; %
      
      % X
      \node[det, above=of y, yshift=1cm] (x) {$x_{ij}$}; %
      
      % sigma2
      \node[latent, right=of y] (sigma2) {$\sigma_d^2$}; %
      
      % beta
      \node[latent, right=of x] (beta) {$\beta_{jd}$}; %
      
      % gamma2
      \node[latent, right=of beta] (gamma2) {$\gamma_{jd}^2$}; %
      \node[const, right=of gamma2, yshift=0.5cm] (phi) {$\phi_{jd}$}; %
      \node[const, right=of gamma2, yshift=-0.5cm] (lambdaf) 
      {$\lambda_{\text{feat}}$}; %
      \node[det, right=of gamma2, xshift=2cm, yshift=0.5cm] (c) {$c_{jdg}$}; %
      
      % tau2
      \node[latent, right=of sigma2] (tau2) {$\tau_d^2$}; %
      \node[const, right=of tau2, yshift=0.5cm] (chi) {$\chi_{d}$}; %
      \node[const, right=of tau2, yshift=-0.5cm] (lambdad) 
      {$\lambda_{\text{drug}}$}; %
      \node[det, right=of tau2, xshift=2cm, yshift=0.5cm] (z) {$z_{dh}$}; %
      
      % edges
      \edge {x} {y};%
      \edge {sigma2} {y};%
      \edge {beta} {y};%
      \edge {sigma2} {beta};%
      \edge {gamma2} {beta};%
      \edge {tau2} {beta};%
      \edge {phi} {gamma2};%
      \edge {lambdaf} {gamma2};%
      \edge {chi} {tau2};%
      \edge {lambdad} {tau2};%
      \edge {c} {phi}; %
      \edge {z} {chi}; %
      
      % plates
      \plate{plateg} {(c)} {$g=1,\dots,G$}
      \plate{plateh} {(z)} {$h=1,\dots,H$}
      \plate{platei} {(y) (x)}{$i=1,\dots,n$}; %
      \plate{platej} {(x) (beta) (gamma2) (phi) (lambdaf) (c) (plateg)}
      {$j=1,\dots,p$}; %
      \plate{plated} 
      {(y) (x) (sigma2) (beta) (gamma2) (tau2) (phi) (lambdaf)
      (chi) (lambdad) (c) (z) (platei) (platej) (plateg) (plateh)}
      {$d=1,\dots,D$}; %
    }
    \caption{Hierarchical representation of the drug sensitivity prediction 
    model. Grey circles 
    represent observed variables, white circles represent unobserved variables,
    tilted squares represent fixed data, and unenclosed letters are parameters 
    to be estimated. Cell lines are indexed by $i$, features
    by $j$, drugs by $d$, drug covariates by $h$, and feature covariates by 
    $g$. The $y_{id}$ are the drug sensitivities, $x_{ij}$ the 
    molecular features, 
    $c_{jdg}$ the external feature covariates, $z_{dh}$ the external drug
    covariates, $\beta_{jd}$ the regression coefficients, $\sigma_d^2$ the 
    error variances, $\tau_d^2$ and $\gamma_{jd}^2$ the drug and feature
    specific variance components, respectively, and $\phi_{jd}$, 
    $\lambda_{\text{feat}}$, $\chi_{d}$, and $\lambda_{\text{drug}}$ the 
    hyperparameters.}
    \label{fig:dag}
  \end{figure}
	
	\section{Estimation}\label{sec:estimation}
	\subsection{Variational Bayes}\label{sec:variationalbayes}
	The posterior corresponding to the model described in (\ref{eq:likelihood})
	and (\ref{eq:prior}) is not available in closed form. 
	To avoid computationally intensive markov chain Monte Carlo (MCMC) 
	algorithms, we approximate the joint posterior by variational Bayes (see 
	\cite{blei_variational_2017} for a review), where the approximate
	posterior density factorises as: $p(\bbeta_d, \bm{\gamma}^2_d, \tau_{d}^2, 
	\sigma_d^2 | \y_d) \approx 
	Q_d(\cdot) = q(\bbeta_d) \cdot q(\bm{\gamma}_{d}^2) \cdot q(\tau_{d}^2) 
	\cdot q(\sigma_d^2)$, where $\bm{\gamma}_{d}^2 = 
	\begin{bmatrix} \gamma_{1d}^2 & \cdots & \gamma_{pd}^2 \end{bmatrix} \tr$. For
  notational convenience, we slightly abuse notation and let $q(\cdot)$ denote
	different densities for different inputs.
	Under such a factorisation, the marginal variational posteriors that 
	minimise the Kullback-Leibler divergence of the true posterior to the 
	variational Bayes approximation \cite[]{neal_view_1998} are given by:
	\begin{align*}
    q(\bm{\beta}_d) & \overset{D}{=} \mathcal{N}_{p} 
    (\bm{\mu}_d, \bm{\Sigma}_d), \\
    q(\bm{\gamma}_{d}^2) & \overset{D}{=} \prod_{j=1}^p \mathcal{GIG}
    \(-1, \lambda_{\text{feat}}/\phi_{jd}^2, \delta_{jd}\), \\
    q(\tau^2_d) & \overset{D}{=} \mathcal{GIG} \(-\frac{p + 1}{2}, 
    \lambda_{\text{drug}}/\chi_d^2, \eta_d\), \\
    q(\sigma^2_d) & \overset{D}{=} \Gamma^{-1} \(\frac{n + p + 1}{2}, \zeta_d\),
  \end{align*}
	where $\mathcal{GIG} \(p, \nu, \eta\)$ denotes the generalized inverse Gaussian
	distribution with index $p \in \mathbb{R}$, and scales $\nu > 0$ and 
	$\eta > 0$ \cite[]{jorgensen_statistical_1982}. See SM Section 
	\ref{sm-sec:vbderivations} for the derivations. The variational
	parameters $\bm{\mu}_d$, $\bm{\Sigma}_d$, $\delta_{jd}$, $\eta_d$, and
	$\zeta_d$ contain cyclic dependencies and are iteratively 
	updated by:
	\begin{subequations}\label{eq:vbequations}
    \begin{align}
      \bm{\Sigma}_d^{(h+1)} & = (a_d^{(h)})^{-1} 
      \[\X \tr \X + g_{d}^{(h)} \diag(b_{jd}^{(h)})\]^{-1}, \\
      \bm{\mu}_d^{(h+1)} & = \[\X \tr \X + g_{d}^{(h)} 
      \diag(b_{jd}^{(h)})\]^{-1} 
      \X \tr \y_d, \\
      \delta_{jd}^{(h+1)} & = a_d^{(h)} g_{d}^{(h)} \[(\bmu^{(h+1)}_{jd})^2 + 
      (\bSigma^{(h + 1)}_d)_{jj}\] + \lambda_{\text{feat}}, \\ 
      \eta_d^{(h+1)} & = a_d^{(h)} \sum_{j=1}^{p} b_{jd}^{(h + 1)}
      \[ (\bmu^{(h+1)}_{jd})^2 + (\bSigma^{(h + 1)}_d)_{jj} \] + 
      \lambda_{\text{drug}}, \\
      \zeta_d^{(h+1)} & = \frac{1}{2} \bigg[\mathbf{y}_d \tr \mathbf{y}_d -
      2 \mathbf{y}_d \tr \X \bm{\mu}_d^{(h+1)} + 
      \trace ( \X \tr \X \bm{\Sigma}_d^{(h+1)}) + 
      (\bm{\mu}_d^{(h+1)}) \tr \X \tr \X \bm{\mu}_d^{(h+1)} \\
      & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + 
      g_{d}^{(h + 1)} \trace \[ \diag (b_{jd}^{(h+1)}) \bm{\Sigma}_d^{(h+1)}\] + 
      g_{d}^{(h + 1)} (\bm{\mu}_d^{(h+1)}) \tr \diag (b_{jd}^{(h+1)}) 
      \bm{\mu}_d^{(h+1)}\bigg],
    \end{align}
  \end{subequations}
  until convergence, where $\y_d = \begin{bmatrix} y_{1d} & \cdots & y_{nd}
  \end{bmatrix} \tr$. Here, we set
  \begin{align}
    a_d^{(h)} & =\E_{Q^{(h)}}(\sigma_d^{-2})=(n + p + 1)/(2 \zeta_d^{(h)}), 
    \nonumber\\
    b_{jd}^{(h)} & = \E_{Q^{(h)}}(\gamma_{jd}^{-2}) = 
    \sqrt{\frac{\lambda_{\text{feat}}}{\phi_{jd}^2 \delta_{jd}^{(h)}}}
    \frac{K_0\(\sqrt{\delta_{jd}^{(h)}\lambda_{\text{feat}}/\phi_{jd}^2}\)}
    {K_1\(\sqrt{\delta_{jd}^{(h)}\lambda_{\text{feat}}/\phi_{jd}^2}\)} + 
    \frac{2}{\delta_{jd}^{(h)}}, \label{eq:ratiomodifiedbessel} \\
    g_d^{(h)} & =\E_{Q^{(h)}}(\tau_d^{-2})=
    \sqrt{\frac{\lambda_{\text{drug}}}{\chi_{d}^2 \eta_{d}^{(h)}}}
    \frac{K_{(p-1)/2}\(\sqrt{\eta_{d}^{(h)}\lambda_{\text{drug}}/\chi_{d}^2}\)}
    {K_{(p+1)/2}\(\sqrt{\eta_{d}^{(h)}\lambda_{\text{drug}}/\chi_{d}^2}\)} + 
    \frac{p + 1}{\eta_{d}^{(h)}}.
    \nonumber
  \end{align}
  where $K_{\nu}(x)$ denotes the modified Bessel function of the second kind.
  A method for fast and numerically stable calculation of ratios of
  modified Bessel functions of the second kind, as in 
  (\ref{eq:ratiomodifiedbessel}), is given in SM Section 
  \ref{sm-sec:ratiosmodifiedbessels}.
  
  \subsection{Empirical Bayes}\label{sec:empiricalbayes}
  We parametrised the prior 
  mean of the $\gamma_{jd}^2$ as $\phi_{jd}=(\mathbf{c}_{jd} \tr 
  \bm{\alpha}_{\text{feat}})^{-1}$ and the prior mean of $\tau_d^2$ as
  $\chi_{d}=(\mathbf{z}_{d} \tr \bm{\alpha}_{\text{drug}})^{-1}$.
  This parametrisation allows us to include feature and drug covariates,
  both continuous and discrete, into the 
  model. Additionally, it reduces the number of hyperparameters from 
  $pD$ to $|\bm{\alpha}_{\text{feat}}| + |\bm{\alpha}_{\text{drug}}| + 2$. The
  Bayesian model
  then requires the specification of the hyperparameters 
  $\bm{\alpha}=\begin{bmatrix} \bm{\alpha}_{\text{feat}} \tr & 
  \bm{\alpha}_{\text{drug}} \tr 
  \end{bmatrix} \tr$ and $\bm{\lambda} = \begin{bmatrix} \lambda_{\text{feat}} &
  \lambda_{\text{drug}}
  \end{bmatrix} \tr$. These are
  abstract and hard to interpret parameters 
  for which we generally lack expert knowledge. They do, however, have a 
  significant influence on the shape of the posterior distribution. We 
  therefore propose to estimate these hyperparameters by empirical Bayes.
  In our case, this results in an objective and data-driven inclusion of the
  external feature and drug covariates.
	
	The canonical method for empirical Bayes is to maximise the marginal 
	likelihood with respect to the hyperparameters. In 
	\cite{casella_empirical_2001} the marginal likelihood is maximised by an EM 
	algorithm:
	\begin{align*}
	  \bm{\alpha}^{(l+1)},\bm{\lambda}^{(l+1)} & = \underset{\bm{\alpha},
	  \bm{\lambda}}{\argmax}\E_{\cdot | \Y} 
	  [\log p(\Y, \B, \bm{\Gamma}^2, \bm{\tau}^2, \bsigma^2) | \bm{\alpha}^{(l)},
	  \bm{\lambda}^{(l)}] \\
	  & = \underset{\bm{\alpha},\bm{\lambda}}{\argmax} 
	  \E_{\cdot | \Y} [\log \pi (\bm{\Gamma}^2) | \bm{\alpha}_{\text{feat}}^{(l)},
	  \bm{\lambda}_{\text{feat}}^{(l)}] + 
	  \E_{\cdot | \Y} [\log \pi (\bm{\tau}^2) | 
	  \bm{\alpha}_{\text{drug}}^{(l)}, \bm{\lambda}_{\text{drug}}^{(l)}],
	\end{align*}
	where $\Y = \begin{bmatrix} \y_1 & \cdots & \y_D \end{bmatrix}$,
	$\B = \begin{bmatrix} \bbeta_1, \dots, \bbeta_D \end{bmatrix}$, 
	$\bm{\tau}^2 = \begin{bmatrix} \tau^2_1 & \cdots & 
	\tau^2_D \end{bmatrix} \tr$,
	$\bsigma^2 = \begin{bmatrix} \sigma^2_1 & \cdots & 
	\sigma^2_D \end{bmatrix} \tr$,
	and $\bm{\Gamma}^2 = \begin{bmatrix} \bgamma^2_1 & \cdots & \bgamma^2_D 
	\end{bmatrix}$,
	and the expectation is with respect to the joint posterior. 
	In our case, this 
	posterior is not available in closed form, which renders the expectation 
	difficult. While \cite{casella_empirical_2001} suggests to approximate the 
	expectation by a Monte Carlo sample, we propose to use the variational Bayes 
	approximation developed in Section \ref{sec:variationalbayes}:
	$$
	\bm{\alpha}^{(l+1)},\bm{\lambda}^{(l+1)} = 
	\underset{\bm{\alpha},\bm{\lambda}}{\argmax}\E_{Q^{(l)}} 
	[\log \pi(\bm{\Gamma}^2)| \bm{\alpha}_{\text{feat}}^{(l)}, 
	\bm{\lambda}_{\text{feat}}^{(l)}] + 
	\E_{Q^{(l)}} [\log \pi(\bm{\tau}^2)| 
	\bm{\alpha}_{\text{drug}}^{(l)}, \bm{\lambda}_{\text{drug}}^{(l)}],
	$$
	where now the expectation is with respect to the converged variational 
	posterior $Q^{(l)}=\prod_{d=1}^D Q_d^{(l)}$. Note that the prior 
	$\bgamma^2_d$ and $\tau_d^2$ independence assumption 
	results in separate optimisation problems for the 
	feature hyperparameters ($\bm{\alpha}_{\text{feat}}$ and 
	$\bm{\lambda}_{\text{feat}}$), and the drug hyperparameters 
	($\bm{\alpha}_{\text{drug}}$ and $\bm{\lambda}_{\text{drug}}$).
	If we stack the drug and feature covariates:
	\begin{align*}
	  \mathbf{C} = 
	  \begin{bmatrix} \mathbf{c}_{11} \tr \\ 
	    \vdots \\
	    \mathbf{c}_{p1} \tr \\
	    \vdots \\
	    \mathbf{c}_{1D} \tr \\
	    \vdots \\
	    \mathbf{c}_{pD} \tr
	  \end{bmatrix} \text{ and }
	  \mathbf{Z} =
	  \begin{bmatrix} \mathbf{z}_{1} \tr \\ 
	    \vdots \\
	    \mathbf{z}_D \tr
	  \end{bmatrix},
	\end{align*}
	the empirical Bayes updates are given by:
  \begin{align*}
    \bm{\alpha}^{(l+1)}_{\text{feat}} & = \[ \mathbf{C} \tr \diag 
    (e_{jd}^{(l)}) \mathbf{C} \]^{-1} \mathbf{C} \tr 
    \mathbf{1}_{pD \times 1}, \\
    \lambda^{(l+1)}_{\text{feat}} & = pD \[ \sum_{d=1}^D \sum_{j=1}^{p} 
    b_{jd}^{(l)} + (\bm{\alpha}^{(l+1)}_{\text{feat}}) \tr \mathbf{C} \tr 
    \diag (e_{jd}^{(l)}) \mathbf{C} \bm{\alpha}^{(l+1)}_{\text{feat}} - 
    2 (\bm{\alpha}^{(l+1)}_{\text{feat}}) \tr \mathbf{C} \tr 
    \mathbf{1}_{pD \times 1} \]^{-1},\\
    \bm{\alpha}^{(l+1)}_{\text{drug}} & = \[ \mathbf{Z} \tr \diag 
    (f_{d}^{(l)}) \mathbf{Z} \]^{-1} \mathbf{Z} \tr 
    \mathbf{1}_{D \times 1}, \\
    \lambda^{(l+1)}_{\text{drug}} & = D \[ \sum_{d=1}^D g_{d}^{(l)} + 
    (\bm{\alpha}^{(l+1)}_{\text{drug}}) \tr \mathbf{Z} \tr \diag (f_{d}^{(l)})
    \mathbf{Z} \bm{\alpha}^{(l+1)}_{\text{drug}} - 
    2 (\bm{\alpha}^{(l+1)}_{\text{drug}}) \tr \mathbf{Z} \tr 
    \mathbf{1}_{D \times 1} \]^{-1},
  \end{align*}
  where 
  \begin{align*}
    e_{jd}^{(l)} & = \E_{Q^{(l)}}(\gamma_{jd}^2| 
    \bm{\alpha}_{\text{feat}}^{(l)}, 
    \lambda_{\text{feat}}^{(l)}) = (b_{jd}^{(l)} - 
    2/\delta_{jd}^{(l)}) \cdot \delta_{jd}^{(l)} (\phi_{jd}^{(l)})^2/
    \lambda_{\text{feat}}^{(l)}, \\
    f_{d}^{(l)} & = \E_{Q^{(l)}}(\tau_{d}^2| \bm{\alpha}_{\text{drug}}^{(l)}, 
    \lambda_{\text{drug}}^{(l)}) = (g_{d}^{(l)} - 
    (p + 1)/\eta_{d}^{(l)}) \cdot \eta_{d}^{(l)} (\chi_{d}^{(l)})^2/
    \lambda_{\text{drug}}^{(l)}.
  \end{align*}
  
  To ensure proper and unbiased shrinkage, intercepts are included in 
  $\bm{\alpha}_{\text{feat}}$ and $\bm{\alpha}_{\text{drug}}$. This is achieved 
  by appending both
  $\mathbf{C}$ and $\mathbf{Z}$ with a column of 
  ones. These intercepts are roughly interpreted as the
  expected prior precisions $\E(\gamma_{jd}^{-2})$ and $\E(\tau_d^{-2})$
  if the feature and drug covariates are all zero. 
  Likewise, an $\alpha$ corresponding to an external covariate may be
  interpreted as an additive effect of the external covariate on the prior
  expected precision. So an $\alpha=1$ translates to an increase in expected 
  prior precision of
  1 for every increase in the external covariate of 1,
	keeping all the other external covariates fixed.
  
  Variational Bayes approximations are known to underestimate posterior 
  variances \cite[]{rue_approximate_2009,consonni_mean-field_2007,
  bishop_pattern_2006,wang_inadequacy_2005}. In simulation Scenario 5 in Section 
  \ref{md-sec:simulations} of the SM, we compare the variational posterior to 
  MCMC samples from the posterior with fixed hyperparameters
  estimates (after the procedure described in Section \ref{sec:empiricalbayes}
  has converged). In this simulation Scenario and other settings (not 
  shown), the variational approximation to the posterior is accurate. If 
  however, the
  user is reluctant to trust the variational posterior variances, samples
  from the posterior may be generated with the
  Gibbs sampler in SM Section \ref{sm-sec:gibbssampler}. Alternatively, 
  we provide an
  implementation of the proposed model in stan using the R package rstan 
  \cite[]{guo_rstan:_2018} at 
  \url{https://github.com/magnusmunch/cambridge}.
	
	\section{Simulations}\label{sec:simulations}
	\subsection{Setup}
	This section investigates the empirical 
	Bayes estimation properties of the model in a simulated setting; its main aim 
	is to assess hyperparameter estimation. It is a data based simulation,
	wherein the responses are simulated from a synthetic model, but the
	features are taken from the real GDSC expression data introducted in Section 
	\ref{sec:gdsc}. The real GDSC features contain strong collinearities 
	(correlations ranging from -0.89 to 0.99). Such
	strong collinearities in the design matrix impede correct parameter estimation
	with small sample sizes. 
	We therefore replace the ambition of correctly estimating the $\beta_{jd}$
	with the more modest aim of approximately correct estimation of the 
	hyperparameters.
	
	A pre-processing step selects 100 features with largest variance, while
	100 drug sensitivities for 100 cell lines are 
	simulated from model (\ref{eq:likelihood}) and 
	(\ref{eq:prior}). We draw the error
	variances as $\forall d: \sigma_d^2 \sim \Gamma^{-1}(3,2)$,
	such that the prior $\sigma_d^2$ mean and variance are both one. 
	We consider the following four scenarios for the simulation of the 
	drug and feature variance components:
	\begin{itemize}
	  \item{Scenario 1 fixes $\forall d: \tau_d^2=1$ and draws the 
	    $\gamma_{jd}^2$ according to model (\ref{eq:prior}). We
	    create four external dummy feature covariates that code for four 
	    approximately equally sized groups of features. We set
	    $\bm{\alpha}_{\text{feat}}$ such that the $\gamma_{jd}^2$ of the four
	    groups of features have prior means $\phi_{jd} \in \{ 1, 1/2, 1/4, 1/8 \}$
	    ($\bm{\alpha}_{\text{feat}}=\begin{bmatrix} 1 & 1 & 3 & 7 \end{bmatrix} 
	    \tr$). The prior scale parameter is set to $\lambda_{\text{feat}}=1$}.
	  \item{Scenario 2 fixes $\forall j,d: \gamma_{jd}^2=1$ and draws the 
	    $\tau_d^2$ according to model (\ref{eq:prior}), following a procedure
	    similar to the procedure for the $\gamma_{jd}^2$ in Scenario 1: we 
	    create four groups of drugs with corresponding external drug 
	    dummy variables and set $\bm{\alpha}_{\text{drug}}=\begin{bmatrix} 
	    1 & 1 & 3 & 7 \end{bmatrix} \tr$, such that we have $\chi_{d} \in 
	    \{ 1, 1/2, 1/4, 1/8 \}$. The scale is set to $\lambda_{\text{drug}}=1$.}
	  \item{Scenario 3 combines the procedures from Scenarios 1 and 2 to draw both
	    the $\gamma_{jd}^2$ and $\tau_d^2$ according to (\ref{eq:prior}).}
	  \item{Scenario 4 is equal to Scenario three, except that we add noise to
	    the external covariates. Noise is supposed to mimic a low external 
	    covariate signal and is added by permutation of fractions 
	    $q \in \{0.1, 0.2, 0.33, 0.5, 0.67, 0.8, 1\}$ of the 
	    rows of the external covariates.}
	\end{itemize}
	We estimate two models: (i) the NIG model that only includes an intercept in
	the external covariates, called NIG$_{\text{f}}^-$, NIG$_{\text{d}}^-$, or
	NIG$_{\text{f}+\text{d}}^-$, depending on which variance components are 
	estimated (feature, drug, or both in Scenarios 1, 2, and 3/4, respectively),
	and 
	(ii) the NIG model estimated as in Section \ref{sec:estimation} that includes
	all external covariates, called NIG$_{\text{f}}$, NIG$_{\text{d}}$, or
	NIG$_{\text{f}+\text{d}}$, again depending on which variance components are 
	estimated.
	Exclusion of the external covariates as in the NIG$_{\text{f}}^-$, 
	NIG$_{\text{d}}^-$, and NIG$_{\text{f}+\text{d}}^-$ models
	amounts to direct estimation of common expected 
	prior means $\phi$ and/or $\chi$, instead of regression estimates for the
	$\phi_{jd}$ and/or $\chi_d$ as in the 
	NIG$_{\text{f}}$, NIG$_{\text{d}}$, and NIG$_{\text{f}+\text{d}}$ models.
	In the language of \cite{bernardo_shrink_2011} as introduced in 
	Section \ref{sec:introduction}, models NIG$_{\text{f}}$ and NIG$_{\text{d}}$
	may be described as local and global shrinkage rules, respectively, as 
	opposed to the global-local shrinkage models NIG$_{\text{f}+\text{d}}$ and
	NIG$_{\text{f}+\text{d}}^-$. We repeat every simulation Scenario 100 times. 
	
	SM Section \ref{sm-sec:simulations} contains an additional simulation 
	Scenario 5 that investigates the performance of the variational Bayes approach
	as compared to MCMC samples from the posterior, as well as more 
	simulation results for Scenarios 1-4 for
	the NIG model and four alternative methods: frequentist (i) 
	lasso and (ii) ridge models, and EB version of the ridge model: (iii) a
	multivariate Bayesian extension of \texttt{xtune} 
	\cite[]{ni_bayesian_2019} named mxtune, as introduced in SM Section 
	\ref{sm-sec:mxtune}.
	
	\subsection{Results}
<<simulation_gdsc_res1>>=
res <- read.table("results/simulations_gdsc_res1.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

true.phi <- mean(1/(c(1, 1, 3, 7) %*% t(unname(model.matrix(~ factor(1:4))))))
est.phi <- mean(1/res[rownames(res)=="alphaf0", 1])
est.lambdaf <- apply(res[rownames(res)=="lambdaf", c(1, 2)], 2, mean)
@	
  Figure \ref{fig:simulations_gdsc_est1} shows the estimated
	$\bm{\alpha}_{\text{feat}}$ together with its true value for NIG$_{\text{f}}$ 
	in Scenario 1 of the simulation study (fixed $\tau_d^2$). Figure 
	\ref{fig:simulations_gdsc_est1}a shows that 
	$\bm{\alpha}_{\text{feat}}$ is slightly but 
	consistently underestimated. This results in slight overestimation on the
	$\phi_{jd}$ scale as depicted in Figure \ref{fig:simulations_gdsc_est1}b. 
	Model NIG$_{\text{f}}^-$ (that excludes the external covariates) gives a 
	mean $\phi$ estimate of \Sexpr{round(est.phi, 2)},  
	slightly higher than the true mean of the $\phi_{jd}$, 
	\Sexpr{round(true.phi, 2)}. 
<<simulations_gdsc_est1, fig.cap="Simulation results for Scenario 1 ($\\tau^2_d$ fixed): estimated and true values for (a) $\\alpha_{\\text{feat}}$ and (b) prior means $\\phi_{jd}$.", out.width="100%", fig.asp=1/2>>= 
@
	
<<simulation_gdsc_res2>>=
res <- read.table("results/simulations_gdsc_res2.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp
true.chi <- mean(1/(c(1, 1, 3, 7) %*% t(unname(model.matrix(~ factor(1:4))))))
est.chi <- mean(1/res[rownames(res)=="alphad0", 1])
est.lambdad <- apply(res[rownames(res)=="lambdad", c(1, 2)], 2, mean)
@	
  Figure \ref{fig:simulations_gdsc_est2}a shows the estimated
	$\bm{\alpha}_{\text{drug}}$ together with its true value for NIG$_{\text{d}}$ 
	in Scenario 2 of the simulation study (fixed $\gamma_{jd}^2$). The intercept
	term ($\alpha_{\text{drug},0}$) is overestimated, while the deviations from 
	the intercept term are approximately correct. The overestimated intercept term
	results in underestimated $\chi_d$ (Figure \ref{fig:simulations_gdsc_est2}b).
	The mean $\chi$ estimate in the 
	NIG$_{\text{d}}^-$ model is \Sexpr{round(est.chi, 2)},  
	which underestimates the true mean \Sexpr{round(true.chi, 2)}. 
<<simulations_gdsc_est2, fig.cap="Simulation results for Scenario 2 ($\\gamma^2_{jd}$ fixed): estimated and true values for (a) $\\alpha_{\\text{feat}}$ and (b) prior means $\\phi_{jd}$.", out.width="100%", fig.asp=1/2>>= 
@

<<simulation_gdsc_res3>>=
res <- read.table("results/simulations_gdsc_res3.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp
true.chi <- mean(1/(c(1, 1, 3, 7) %*% t(unname(model.matrix(~ factor(1:4))))))
est.chi <- mean(1/res[rownames(res)=="alphad0", 1])
est.lambdad <- apply(res[rownames(res)=="lambdad", c(1, 2)], 2, mean)
true.phi <- mean(1/(c(1, 1, 3, 7) %*% t(unname(model.matrix(~ factor(1:4))))))
est.phi <- mean(1/res[rownames(res)=="alphaf0", 1])
est.lambdaf <- apply(res[rownames(res)=="lambdaf", c(1, 2)], 2, mean)

phi.ratio <- apply(apply(sapply(0:3, function(s) {
  res[rownames(res)==paste0("alphaf" ,s), 2]}) %*%
    t(cbind(1, rbind(0, diag(3)))), 2, function(m) {
      res[rownames(res)=="alphaf0", 2]/m}), 2, mean)
chi.ratio <- apply(apply(sapply(0:3, function(s) {
  res[rownames(res)==paste0("alphad" ,s), 2]}) %*%
    t(cbind(1, rbind(0, diag(3)))), 2, function(m) {
      res[rownames(res)=="alphad0", 2]/m}), 2, mean)
true.ratio <- as.numeric(1/(c(1, 1, 3, 7) %*%
                              t(unname(model.matrix(~ factor(1:4))))))
@
  In Figure \ref{fig:simulations_gdsc_est3} the 
	$\bm{\alpha}_{\text{feat}}$ and $\bm{\alpha}_{\text{drug}}$ estimated by 
	NIG$_{\text{f}+\text{d}}$
	are displayed together with their true values for
	simulation Scenario 3. $\bm{\alpha}_{\text{feat}}$ are underestimated, while 
	$\bm{\alpha}_{\text{drug}}$ are overestimated. These biases seem to 
	be consistent though. The mean ratios $\phi_1$ to $\phi_2$, $\phi_3$, 
	$\phi_4$ and $\chi_1$ to $\chi_2$, $\chi_3$, 
	$\chi_4$ are \Sexpr{round(phi.ratio, 2)[-1]} and 
	\Sexpr{round(chi.ratio, 2)[-1]}, respectively, compared to their true values
	\Sexpr{round(true.ratio, 2)[-1]}. So in a relative sense, the 
	$\bm{\alpha}_{\text{feat}}$ estimates are correct, while the 
	$\bm{\alpha}_{\text{drug}}$ are somewhat higher than expected. 
	The NIG$_{\text{f}+\text{d}}^-$ model is also consistently over- 
	and underestimating $\phi$
	and $\chi$ with mean estimates \Sexpr{round(est.phi, 2)} and 
	\Sexpr{round(est.chi, 2)}, respectively (compared to the true mean 
	\Sexpr{round(true.chi, 2)}). 
<<simulations_gdsc_est3, fig.cap="Simulation results for Scenario 3: estimated and true values for (a) $\\alpha_{\\text{feat}}$, (b) prior means $\\phi_{jd}$, (c) $\\alpha_{\\text{drug}}$, and (d) prior means $\\chi_{d}$.", out.width="100%", fig.asp=1>>= 
@
  
  Figure \ref{fig:simulations_gdsc_est4} displays the mean $\phi_{jd}$ and 
  $\chi_d$ estimates for different noise levels in simulation Scenario 4. 
  The simulation shows that with increasing noise level, the estimated 
  prior means $\phi_{jd}$ and $\chi_d$ for the four groups of external 
  covariates become more and more alike. In other words, noise in the external
  covariates impedes estimation of $\bm{\alpha}_{\text{feat}}$ and
  $\bm{\alpha}_{\text{drug}}$, as expected.
<<simulations_gdsc_est4, fig.cap="Simulation results for Scenario 3: mean estimated prior means (a) $\\phi_{jd}$, and (b) $\\chi_{d}$ for different levels of noise in the external covariates.", out.width="100%", fig.asp=1/2>>= 
@

  Estimation of only $\bm{\alpha}_{\text{feat}}$ is relatively unbiased, as
  evident from simulation Scenario 1. In contrast,
  $\bm{\alpha}_{\text{drug}}$ are underestimated (Scenarios 2 and 3).
  $\bm{\alpha}_{\text{drug}}$ is inversely related to the prior $\bm{\beta}$
  variance. Underestimation of variance components in regression is 
	a common phenomenon in variational Bayes approximations and a possible
	explanation of the estimation bias in Scenarios 2 and 3. 
	In Scenario 3, this is compensated for
	by the overestimation of the $\phi_{jd}$, such that the estimated mean 
	variance $\V(\beta_{jd})$ are stil biased, but only slightly underestimated
	(Figure \ref{fig:simulations_gdsc_var}). We note that an increase in cell 
	lines $n$ and/or the number of drugs $D$ improves estimation
	of the $\chi_d$. However, we choose to highlight the high-dimensional 
	(more parameters than samples) situation here, because it is the most common
	in practice.
<<simulations_gdsc_var, fig.cap="Simulation results for Scenario 3: mean estimated prior variances $\\V(\\beta_{jd})$ versus true values, with line of identity (dotted).", out.width="50%", fig.asp=1>>= 
@
	
	\section{GDSC data}\label{sec:gdsc}
	\subsection{Primary data}
	The GDSC project's \cite[]{yang_genomics_2013} aim is ``to improve cancer 
	treatments by discovering therapeutic biomarkers that can be used to identify 
	patients most likely to respond to anticancer drug". Part of the project is to
	screen $>1000$ human cancer cell lines for drug sensitivities. The cell lines
	have been genetically characterised and several drug sensitivity measures are
	recorded.
	The data is freely available from \cite{garnett_systematic_2012} and consist 
	of: (i) the sensitivity measures of the cell lines 
	to the drugs, (ii) annotation of the 
	screened compounds, and (iii) the cell lines' genomic profile (mutations, 
	copy numbers, methylation profiles, and gene expression). We will attempt to 
	predict drug sensitivities of the cell lines, as quantified by half maximal
	inhibitory concentration (IC50), using the
	gene expression and gene mutation data. Other choices of sensitivity measures
	than IC50 are 
	possible, but a discussion on the pros and cons of different sensitivity
	measures is beyond the aim of this paper. We have used the version of the 
	data that is presented in \cite{iorio_landscape_2016}.
	We averaged repeated measures over cell line-drug combinations and model the
	logarithm of the IC50 values. In the following, IC50 refers 
	to these log-transformed values. 
	After removing all cell lines with missing values, we end up with 388 to 1043
	IC50 estimates for 251 drugs. Differences in the number of cell lines
	between drugs occur, because not all drug and cell line combinations are 
	available. The pre-processed expression and mutation data consist of
	17737 and 300 genes, respectively.
	% 
	% A comprehensive penalized regression
	% approach such as in 
	% {\cite{mai_composite_2019} uses most of the available molecular markers 
	% instead of just gene expression (68 mutations, 426 copy numbers, 
	% and 2602 gene expression levels for 498 cell lines and 97 drugs). The 
	% current version of our software is not able to handle such problem sizes,
	% due to the repeated calculation of posterior parameters in the empirical
	% Bayes iterations. Currently, problems with number of outcomes in the hundreds
	% and number of features and samples up to a thousand are computationally 
	% feasible.

	\subsection{External data}
	Two ternary drug covariates are available: the developmental stage
	(experimental, in clinical development, or clinically approved) of the
	drugs and the action (unkown, cytotoxic, or targeted) of the drugs. 
	These drug covariates are taken 
	directly from the GDSC database's annotation file and dummy-coded
	with reference categories clinically approved drugs and cytotoxic drugs. 
	We expect that drugs that have been clinically approved are easiest to 
	predict and hence yield the largest prior $\beta_{jd}$ variances, followed by
	the drugs in clinical development, and the experimental drugs. Likewise we 
	expect the targeted drugs to yield the largest prior $\beta_{jd}$ variances, 
	followed by the cytotoxic drugs, and the unkown target drugs. Note that large
	$\beta_{jd}$ variances translate to large
	prior $\gamma_{jd}^2$ and $\tau_d^2$ means.
	
	Furthermore, we have a binary feature covariate available that indicates
	whether a gene belongs to the drug target pathway. The feature covariate
	was created by comparing the target pathways in the GDSC annotation to
	the KEGG \cite[]{kanehisa_kegg:_2000} and reactome 
	\cite[]{fabregat_reactome_2018} repositories. The reference category here is
	features that are not in the target pathway. For this external covariate,
	we expect that genes that are in the pathway of the drug
	are more predictive than genes that are not, i.e., 
	they have larger prior $\beta_{jd}$ variances than drugs that are not in 
	the pathway. 
	
	The type of molecular marker may be included as external 
	covariate, i.e., whether the feature is a gene expression or gene mutation.
	As an alternative to direct inclusion of the mutation data, we use $p$-values
	from the mutations as external covariate. These were obtained from a $t$-test 
	comparing IC50 values of mutated and unmutated genes. We expect that lower 
	mutation $p$-values result in a larger prior $\beta_{jd}$ variances.
	
	Lastly, $p$-values from an analysis of the CCLE data 
	\cite[]{li_landscape_2019}, a database similar to the GDSC, are included as
	external covariate. These $p$-values are obtained from a simple correlation 
	between the IC50 values and the gene expressions from the CCLE data. 
	The harmonic mean per gene is then used as external covariate for the GDSC
	data analysis. Again, we expect a positive relation between these external
	$p$-values and the larger prior $\beta_{jd}$ variances.
	
	\subsection{Analyses}\label{sec:analyses}
	Four analyses were conducted: 
	\begin{itemize}
	  \item{Analysis 1 includes gene expressions as predictors and $p$-values from
	    the gene mutation data as external covariate ($G=1$, $H=0$). 
	    A pre-processing step selects between 221 and 280 genes per drug for 
	    which both the expression as well as a mutation $p$-value is available.}
	  \item{Analysis 2 includes both gene expressions and mutations as predictors
	    with the feature type as external covariate, i.e., wether the feature
	    is a expression or mutation ($G=1$, $H=0$). For this analysis,
	    we pre-select 300 gene expressions with maximum 
	    variance and 295 gene mutations for which there are both mutated and
	    wildtype cell lines available.}
	  \item{Analysis 3 uses 500 gene expressions as features, selected based on
	    maximum varaince. The CCLE $p$-values are included as external 
	    covariates ($G=1$, $H=0$).}
	  \item{Analysis 4 includes the gene expressions as features and both the
	    annoted drug variables and pathway status of the genes as external 
	    covariates ($G=1$, $H=2$, before dummy coding). 
	    We pre-select 500 genes expressions based on maximum variance.}
	\end{itemize}
	
	In all analyses we estimated the same models as in the simulations (Section 
	\ref{sec:simulations} and SM Section \ref{sm-sec:simulations}):
	(i) NIG$_{\text{f}+\text{d}}^-$, (ii) NIG$_{\text{f}+\text{d}}$, 
	frequentist (iii) lasso and (iv) ridge models, and (v) mxtune 
	(see SM Section \ref{sm-sec:mxtune}). 
	In addition, in Analyses 2 and
	4, we fit a (vi) bSEM model
	\cite[]{leday_gene_2017,kpogbezan_empirical_2017}. The
	bSEM model is similar to the NIG model in that it
	draws the $\beta_{jd}$ from a conditionally normal distribution, but
	fixes $\tau_d^2=1$ and draws
	$\gamma_{jd}^2 \sim \Gamma^{-1} (\phi_{jd}, \lambda_{jd})$, instead of
	$\gamma_{jd}^2 \sim \mathcal{IG} (\phi_{jd}, \lambda_{jd})$, where
	$\Gamma^{-1} (\phi, \lambda)$ denotes the inverse gamma distribution with
	shape $\phi$ and scale $\lambda$. Additionally, the bSEM model can
	only incorporate one binary external covariate that divides the features into
	two groups, i.e., $\bm{\alpha}_{\text{feat}} = \begin{bmatrix} \alpha_0 &
	\alpha_1
	\end{bmatrix} \tr$ and $\bm{c}_{jd} = \begin{bmatrix} 1 & 0 \end{bmatrix}
	\tr$ if feature $jd$ belongs to group one and $\bm{c}_{jd} = \begin{bmatrix}
	1 & 1 \end{bmatrix} \tr$ if feature $jd$ belongs to group two. Both
	$\bm{\alpha}_{\text{feat}}$ and $\lambda_{\text{feat}}$ are
	estimated in a similar fashion as in Section \ref{sec:estimation}.
	
	In all analyses we use all cell lines to estimate the hyperparameters 
	presented in Section \ref{sec:results}. Mean prediction PMSE,
	$\text{PMSE} = D^{-1} n^{-1} \sum_{d=1}^D 
  \sum_{i=1}^n (\mathbf{y}_d - \mathbf{x}_i \tr \hat{\bm{\beta}_d})^2$, with
  $\hat{\bm{\beta}_d}$ the estimator for $\bm{\beta}_d$, and its standard error
  are estimated by 10-fold cross validation. In the NIG and bSEM models, that 
  provide full posteriors, the posterior mean 
  $\E(\bm{\beta}_d | \mathbf{y}_d)$ is used point estimate.
	
	\subsection{Results}\label{sec:results}
<<analysis_gdsc>>==
# estimates tables
fit <- read.table("results/analysis_gdsc_fit1.txt", row.names=NULL)
temp <- fit[, 1]
fit <- as.matrix(fit[, -1])
rownames(fit) <- temp

tab1 <- t(fit)[c(1, 2), c(1:3)]
rownames(tab1) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$")
colnames(tab1) <- c("feature intercept", "$p$-value", "drug intercept")

fit <- read.table("results/analysis_gdsc_fit2.txt", row.names=NULL)
temp <- fit[, 1]
fit <- as.matrix(fit[, -1])
rownames(fit) <- temp

tab2 <- t(fit)[c(1, 2), c(1:3)]
rownames(tab2) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$")
colnames(tab2) <- c("feature intercept", "mutation", "drug intercept")

fit <- read.table("results/analysis_gdsc_fit3.txt", row.names=NULL)
temp <- fit[, 1]
fit <- as.matrix(fit[, -1])
rownames(fit) <- temp

tab3 <- t(fit)[c(1, 2), c(1:3)]
rownames(tab3) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$")
colnames(tab3) <- c("feature intercept", "$p$-value", "drug intercept")

fit <- read.table("results/analysis_gdsc_fit4.txt", row.names=NULL)
temp <- fit[, 1]
fit <- as.matrix(fit[, -1])
rownames(fit) <- temp

tab4.1 <- t(fit)[c(1, 2), c(1:2)]
rownames(tab4.1) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$")
colnames(tab4.1) <- c("feature intercept", "pathway")
tab4.2 <- t(fit)[c(1, 2), c(3:7)]
rownames(tab4.2) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$")
colnames(tab4.2) <- c("drug intercept", 
                    "experimental", "development", "targeted", "unknown")

# pmse table
res <- read.table("results/analysis_gdsc_res1.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

pmse1 <- res[substr(rownames(res), 1, 5)=="pmse.", ]
pmse1 <- aggregate(pmse1, by=list(rep(1:10, each=251)), FUN="mean")[, -1]
mpmse1 <- apply(pmse1, 2, mean)
sdpmse1 <- apply(pmse1, 2, sd)
ppmse1 <- paste0(round(mpmse1, 3), " (", round(sdpmse1, 3), ")")

res <- read.table("results/analysis_gdsc_res2.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

pmse2 <- res[substr(rownames(res), 1, 5)=="pmse.", ]
pmse2 <- aggregate(pmse2, by=list(rep(1:10, each=251)), FUN="mean")[, -1]
mpmse2 <- apply(pmse2, 2, mean)
sdpmse2 <- apply(pmse2, 2, sd)
ppmse2 <- paste0(round(mpmse2, 3), " (", round(sdpmse2, 3), ")")

res <- read.table("results/analysis_gdsc_res3.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

pmse3 <- res[substr(rownames(res), 1, 5)=="pmse.", ]
pmse3 <- aggregate(pmse3, by=list(rep(1:10, each=251)), FUN="mean")[, -1]
mpmse3 <- apply(pmse3, 2, mean)
sdpmse3 <- apply(pmse3, 2, sd)
ppmse3 <- paste0(round(mpmse3, 3), " (", round(sdpmse3, 3), ")")

res <- read.table("results/analysis_gdsc_res4.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

pmse4 <- res[substr(rownames(res), 1, 5)=="pmse.", ]
pmse4 <- aggregate(pmse4, by=list(rep(1:10, each=249)), FUN="mean")[, -1]
mpmse4 <- apply(pmse4, 2, mean)
sdpmse4 <- apply(pmse4, 2, sd)
ppmse4 <- paste0(round(mpmse4, 3), " (", round(sdpmse4, 3), ")")

tab5 <- rbind(c(ppmse1[1:6], NA, ppmse1[7:length(ppmse1)], rep(NA, 3)),
              ppmse2, c(ppmse3[1:6], NA, ppmse3[7:length(ppmse3)], rep(NA, 3)),
              ppmse4)[, c(1:3, 5:7)]
tabm5 <- rbind(c(mpmse1[1:6], NA, mpmse1[7:length(mpmse1)], rep(NA, 3)),
               mpmse2, c(mpmse3[1:6], NA, mpmse3[7:length(mpmse3)], rep(NA, 3)),
               mpmse4)[, c(1:3, 5:7)]
tab5 <- t(sapply(1:nrow(tab5), function(r) {
  id <- which(tabm5[r, ]==min(tabm5[r, ], na.rm=TRUE))
  out <- tab5[r, ]
  out[id] <- paste0("\\textbf{", out[id], "}")
  out}))
rownames(tab5) <- paste0("analysis ", 1:nrow(tab5))
colnames(tab5) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$", "ridge", "xtune",
                    "mxtune", "bSEM")
@
  The non-zero $\bm{\alpha}$ estimates in Tables ? and ? show 
  that there is an effect of the
  external covariates. Models NIG$_{\text{f}+\text{d}}$, NIG$_{\text{f}}$, and
  bSEM estimate a negative effect for
  the external pathway covariate. This translates to a negative additive effect 
  on the expected prior precision of the $\bm{\beta}_d$, i.e., genes that are 
  found
  in the target pathway of a drug are more predictive of IC50 than genes that
  are not in the target pathway, according to expectation. Furthermore the 
  NIG$_{\text{f}+\text{d}}$
  model estimates positive effects for the experimental and developmental drugs,
  compared to the approved drugs. This translates to smaller expected 
  prior precisions for the regression parameters of approved drugs, i.e.,
  approved drugs are easiest to predict, followed by developmental and 
  experimental drugs, as expected. On the other hand, the NIG$_{\text{d}}$ 
  model, that only includes external
  drug covariates, estimates a negative effect for the developmental drugs.
  The NIG$_{\text{f}+\text{d}}$ model estimates the smallest prior 
  $\bm{\beta}_d$ precision for the
  targeted drugs, followed by cytotoxic drugs, while drugs with an unkown target
  are assigned the largest prior $\bm{\beta}_d$ precision, according
  to expectation. Somewhat surprisingly, the NIG$_{\text{d}}$ model
  estimates the smallest
  prior $\bm{\beta}_d$ precision for the cytotoxic drugs, compared to the 
  targeted and unkown drugs. We conjecture that, due to absence of a local
  feature-specific variance component, this model is not flexible 
  enough to correctly capture the full covariance structure of the 
  $\bm{\beta}_d$ and therefore incorreclty estimates $\bm{\alpha}$.
  To see the total effect of the external covariates on the $\bm{\beta}_d$ 
  prior, we calculated the range of the expected prior variances 
  (up to a multiplicative error variance) as 
  $\hat{E}(\gamma_{jd}^2 \cdot \tau_d^2) \in \[ ... \]$ and
  $\hat{E}(\gamma_{jd}^2 \cdot \tau_d^2) \in \[ ... \]$ for
  NIG$_{\text{f}+\text{d}}$ and bSEM, respectively. This confirms that the
  estimated prior 
  $\bm{\beta}_d$ variance components are in the same order of magnitude and that
  the external covariates have a detectable effect on the prior.

<<analysis_gdsc_fit1>>=
kableExtra::kable_styling(knitr::kable(
  tab1, align="r", digits=2,
  caption="$\\bm{\\alpha}$ estimates from analysis 1. Empty cells correspond to fixed zero parameters.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@

<<analysis_gdsc_fit2>>=
kableExtra::kable_styling(knitr::kable(
  tab2, align="r", digits=2,
  caption="$\\bm{\\alpha}$ estimates from analysis 2. Empty cells correspond to fixed zero parameters.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@

<<analysis_gdsc_fit3>>=
kableExtra::kable_styling(knitr::kable(
  tab3, align="r", digits=2,
  caption="$\\bm{\\alpha}$ estimates from analysis 3. Empty cells correspond to fixed zero parameters.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@

  \begin{table}[h!]\label{tab:analysis_gdsc_fit4}
    \begin{minipage}{1\textwidth}
<<analysis_gdsc_fit4.1>>=
kableExtra::kable_styling(knitr::kable(
  tab4.1, align="r", digits=2,
  caption="$\\bm{\\alpha}$ estimates from analysis 4. Empty cells correspond to fixed zero parameters.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@
    \end{minipage}
    \begin{minipage}{1\textwidth}
<<analysis_gdsc_fit4.2>>=
kableExtra::kable_styling(knitr::kable(
  tab4.2, align="r", digits=2,
  caption="$\\bm{\\alpha}$ estimates from analysis 4. Empty cells correspond to fixed zero parameters \\textit{(continued)}.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@
    \end{minipage}
  \end{table}

  The median prediction mean squared error (PMSE), calculated on the 
  test data is displayed in Table \ref{tab:analysis_gdsc_pmse1}. 
  Note that due to standardisation, the empty reference model has a PMSE of one.
  In terms of prediction 
  error, ridge outperforms the other models 
  (PMSE of ? ), while bSEM 
  (PMSE of ? ) performs worse than 
  the null model (PMSE of ? ). Second and third best
  performing models are NIG$_{\text{f}+\text{d}}^-$ and 
  NIG$_{\text{f}+\text{d}}$ (PMSEs of ? and
  ?, respectively), both outperforming the lasso
  with a PMSE of ?. The underperformance of the 
  NIG$_{\text{f}}$ and NIG$_{\text{d}}$ models shows the usefulness of a
  global-local shrinkage prior as opposed to only global or local shrinkage as 
  as in models NIG$_{\text{f}}$ and NIG$_{\text{d}}$.
<<analysis_gdsc_pmse1>>=  
kableExtra::kable_styling(knitr::kable(
  tab5, align="r",
  caption="Mean (standard deviation) of cross-validated PMSE for GDSC data (standard error). Best performing model in bold.",
  format="latex", booktabs=TRUE, escape=FALSE), 
  latex_options=c("HOLD_position"))
@
  
  Part of the differences in performance between
  NIG, lasso, and ridge may be explained with the different levels of 
  ``sparsity'' in the solution. Although NIG and ridge do not give exactly 
  sparse solutions (none of the $\bm{\beta}_d$ coefficients are set to zero), we
  may use the ``heavy-tailedness'' of the distribution of the estimated 
  $\bm{\beta}_d$ as a proxy for sparsity. A common measure of heavy-tailedness
  is excess kurtosis, calculated as ?, 
  ?, and ? for the 
  NIG$_{\text{f}+\text{d}}$, 
  lasso, and ridge estimates, respectively. Evidently, lasso gives the 
  ``sparsest'' solution, followed by NIG and ridge. In some sense, this
  means that the NIG uses less of the $\bm{\beta}_d$ to predict than the ridge
  model. In addition, the penalized regression methods estimate penalty 
  parameters by cross-validation. Cross-validation directly minimises the 
  (approximate) PMSE, as opposed to empirical Bayes in the NIG that maximises 
  the (approximate) marginal likelihood, a measure of model fit. The superior 
  PMSE of ridge regression is therefore not surprising. A caveat with penalized
  regression methods is that they do not give measures of parameter uncertainty.
  NIG, on the other hand, gives the full posterior of the parameters, either
  through a variational Bayes approximation or with the Gibbs sampler from
  SM Section \ref{sm-sec:gibbssampler}. The full posterior gives direct access
  to the parameter uncertainties for a better interpretable model.

  Section \ref{sm-sec:gdsc} in the SM displayes the conditional predictive 
  ordinates (CPO) for the four analyses. A visual inspection of the CPOs learns
  that no extreme outliers occur.
	\section{Discussion}\label{sec:discussion}
<<time>>=
# load("results/analysis_gdsc_fit1.Rdata")
# time2.semnig <- sum(fit2.semnig$time)
time2.semnig <- 1
@
	The preceding presents a novel model for drug sensitivity
	prediction from a set of high dimensional molecular features. The model allows
	for the inclusion of discrete and continuous external covariates on both the 
	drugs and features. Inclusion of the external information is through 
	data-driven and adaptive empirical Bayes estimation of the hyperparameters in 
	the normal inverse Gaussian prior model (\ref{eq:prior}). Estimation is 
	efficient and scales well with the number of features and samples.
	Estimation of the NIG model that includes both external drugs and feature
	covariates (NIG$_{\text{f}+\text{d}}$ in Section \ref{sec:gdsc}) on the GDSC
	data from Section
	\ref{sec:gdsc} took \Sexpr{round(time2.semnig, 2)} seconds on a 2016 
	MacBook Pro with a 2 GHz Dual-Core Intel Core i5 processor and
	8 GB of 1867 MHz LPDDR3 memory, running macOS  10.15.1.
	
	Simulations in Section \ref{sec:simulations} show that the method is 
	reasonably able to estimate the hyperparameters. Simulation Scenarios 1 and 2
	show
	that estimation of drug- and feature-specific hyperparameters is, in 
	principle, fairly accurate. However, when estimated jointly, biases may occur
	due to the interplay between the two sources of information. Nevertheless,
	the simulations show that inclusion of informative external covariates is 
	still beneficial to predictive performance. 

	The model is put into practice on the GDSC data. 
	The NIG model is competitive with convential models like lasso in 
	terms of predictive performance, but is slightly outperformed by ridge.
	We note that estimation of the penalty parameters in the latter two models
	is geared towards predictive performance, because cross-validation is used
	to train these parameters, as opposed to empirical Bayes estimation of the
	hyperparameters in the NIG model, which aims to improve model fit. In that 
	sense, the comparison metric (PMSE) is not to the benefit of the NIG model.
	We note that direct prediction error optimisation is not feasible in our
	external covariates setting.
	
	The results in Section
	\ref{sec:gdsc} show that the inclusion of the external covariates covariates 
	indeed substantially modifies the hyperparameters. Although, PMSE of the NIG
	model is not necessarily lower if external covariates are included, the
	resulting hyperparameter 
	estimates are still informative and may guide future research efforts.
	
	In addition, the comparison to bSEM, a Bayesian counterpart model that allows
	for the inclusion of one binary external feature covariate favours the 
	proposed NIG model. PMSEs are almost uniformly lower over the drugs for the 
	NIG model compared to bSEM. Consequently, the total PMSE is significantly
	lower for the NIG model. 
	
	An alternative strategy to include external covariates is the varying 
	coefficient (VC) model \cite[]{hastie_varying-coefficient_1993}. The VC model
	treats the mean of the regression coefficients as a deterministic function of 
	external covariates, as opposed to our probabilistic model for the variance
	of the regression coefficients. \cite{ni_bayesian_2019} introduces a Bayesian 
	VC model where the relation between the regression 
	coefficients and external covariates is no longer deterministic, but still
	based on the mean of the coefficients. Besides our computationally more
	feasible VB-EM estimation, we advocate for a more indirect
	model for the relation between external covariates and regression 
	coefficients, i.e., through their random variance components. This indirect 
	model assumes less structure about the relation between external covariates
	and regression coefficients than the direct VC mean model approach.
  In particular, a VC mean model describes both magnitude and direction of the 
  external covariates effects, while our variance model only describes the
  magnitude and is invariant to the direction of the effects.
  
  Another possible criticism is the treatment of $\bm{\alpha}$ as fixed
  hyperparameters instead of random. A Bayesian could argue that endowment of 
  $\bm{\alpha}$ with a hyperprior results in propagation of 
  uncertainty about $\bm{\alpha}$ and as a result improved regression parameter 
  uncertainty quantification. \cite{wiel_learning_2019} show in a similar 
  setting that EB estimation of hyperparameters does not necessarily lead to
  worse uncertainty quantification as measured by frequentist coverage of
  Bayesian credible intervals, as compared to a full Bayes treatment of the
  hyperparameters. 
	
	A possible direction of future research to attempt
	to lower PMSE of the NIG model is the introduction of a cross-validated global
	variance component in the prior of the $\bm{\beta}_d$. If we add the 
	additional constraint that the drug and feature-specific variance components
	average to one, this cross-validated variance component may be interpreted as
	the overall level of shrinkage. This overall shrinkage level is then tailored 
	to lower PMSE, while the feature- and drug-specific variance component 
	include the external information. Other directions of future research are
	applications of the NIG
	model to different data types. One possibility is to apply the model in an 
	eQTL study, 
	where gene expressions are regressed on SNPs. Several interesting external
	covariates are available, both on the genes as well as the SNPs. An example of
	an external
	covariate for the genes is genelength, where we suspect that longer genes are
	harder to predict. The distance of the SNP to the gene is an example of an
	external SNP covariate, where the expectation is that SNPs further from the 
	gene are less predictive of that gene's expression.
	
	\section*{Supplementary Material}
	Supplementary Material is available online from 
	\url{https://github.com/magnusmunch/cambridge}. 
	
	\section*{Reproducible Research}
	All results and documents may be recreated from 
	\url{https://github.com/magnusmunch/cambridge}. All timings were obtained
	on a 2016 MacBook Pro with a 2 GHz Dual-Core Intel Core i5 processor and
	8 GB of 1867 MHz LPDDR3 memory, running macOS  10.15.1.
	
	\section*{Acknowledgements}
	We thank the referees for their helpful comments and suggestions on an earlier
	version of this manuscript.
	MM visited SR and GL on a travel grant funded by the Amsterdam Public Health 
	institute's Methodology program.
	\textit{Conflict of Interest}: None declared.
	
	\bibliographystyle{author_short3} 
	\bibliography{refs}
	
	\section*{Session info}

<<r session-info, eval=TRUE, echo=TRUE, tidy=TRUE>>=
devtools::session_info()
@

\end{document}