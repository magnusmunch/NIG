% set options
<<settings, include=FALSE, echo=FALSE>>=
opts_knit$set(root.dir="..", base.dir="../figs")
opts_chunk$set(fig.align='center', fig.path="../figs/", 
               echo=FALSE, cache=FALSE, message=FALSE, fig.pos='!ht')
knit_hooks$set(document=function(x) {
  sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed=TRUE)})
@

% read figure code
<<figures, include=FALSE>>=
read_chunk("code/figures.R")
@

\documentclass[a4paper,hidelinks]{article}
\usepackage{bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,
rotating,caption,subcaption,natbib,appendix,titlesec,multicol,hyperref,verbatim,
bbm,algorithm,algpseudocode,pgfplotstable,threeparttable,booktabs,mathtools,
dsfont,parskip,xr}
\externaldocument[sm-]{supplement1}

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Em}{\mathbf{E}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\Lm}{\mathbf{L}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}

% settings
\graphicspath{{../figs/}}
\pgfplotsset{compat=1.16}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Empirical Bayes estimation of simultaneous-equation models}
\date{\today}
\author{Magnus M. M\"unch$^{1,2}$\footnote{Correspondence to: 
\href{mailto:m.munch@vumc.nl}{m.munch@vumc.nl}}, Mark A. van de Wiel$^{1,3}$, 
Sylvia Richardson$^{3}$, \\ and Gwena{\"e}l G. R. Leday$^{3}$}

\begin{document}

	\maketitle
	
	\noindent
	1. Department of Epidemiology \& Biostatistics, Amsterdam Public Health
	research institute, Amsterdam University medical centers, PO Box 7057, 1007 MB
	Amsterdam, The Netherlands \\*
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\*
	3. MRC Biostatistics Unit, Cambridge Institute of Public Health, Cambridge, 
	United Kingdom
	
	\begin{abstract}
		{...}
	\end{abstract}
	
	\noindent\textbf{Keywords}: Empirical Bayes; Simultaneous-equations model; 
	Variational Bayes; Multivariate data
	
	\section{Introduction}
	- Motivation

	- Regression information
	
	- current literature
	
	- our solution: adaptive borrowing of information through empirical Bayes, 
	guided by external regression information.
	
	\section{Model}
	\subsection{Simultaneous equations model}\label{sec:SEM}
	We have continuous measurements $y_{id}$ for observation $i=1,\dots, n$ on 
	outcome $d=1,\dots,D$. Throughout this paper we assume the $y_{id}$ to be 
	centred per outcome and let $\y_d = \begin{bmatrix} y_{1d} & \dots & y_{nd} 
	\end{bmatrix} \tr$. We predict the $y_{id}$ with features $x_{ij}$, 
	$j=1,\dots, p$, collected in $\x_i = \begin{bmatrix} x_{i1} & \dots & x_{ip} 
	\end{bmatrix} \tr$. We have characteristics of the outcomes available in the 
	form of `co-data' $\mathbf{C} = \begin{bmatrix} \mathbf{c}_1 & \dots & 
	\mathbf{c}_D \end{bmatrix} \tr$. 
	
	We model the outcomes as a linear function of the features:
	\begin{subequations}\label{eq:linearmodel}
		\begin{align}
		y_{id} & = \beta_{0d} + \x_i \tr \bbeta_d + \epsilon_{id},\\
		\text{with } & \epsilon_{id} \sim \mathcal{N}(0, \sigma_d^2),
		\end{align}
	\end{subequations}
	where the $p$-dimensional $\bbeta_d$ are the outcome-specific feature effects.
	Note that (\ref{eq:linearmodel}) gives rise to a system of 
	$D$ linear simultaneous equations.
	
	\subsection{Bayesian prior model}
	We capture the uncertainty in the parameters through a Bayesian prior model. 
	We assign a flat prior to the intercept terms $\beta_{0d}$ and integrate 
	them out. The remaining parameters are endowed with the following priors:
	\begin{subequations}\label{eq:priormodel}
	  \begin{align}
      y_{id} | \bm{\beta}_d, \sigma_d^2 & \sim \mathcal{N} 
      (\x_i \tr \bm{\beta}_d, \sigma_d^2), \\
      \beta_{jd} | \gamma_d^2, \sigma_d^2 & \sim 
      \mathcal{N} (0, \sigma_d^2 \gamma_d^2), \label{eq:betaprior} \\
      \gamma_d^2 & \sim \mathcal{IG}(\theta_d, \lambda_d), \\
      \sigma_d^2 & \sim 1/\sigma_d^3,
    \end{align}
  \end{subequations}
  where $\mathcal{IG}(\theta, \lambda)$ denotes the inverse Gaussian 
  distribution with mean $\theta$ and shape $\lambda$.
  \cite{moran_variance_2018} argue that in regression models, $\bm{\beta}_d$ 
  and $\sigma_d^2$ should be independent \textit{a priori}, i.e.
  $\beta_{jd} | \gamma_d^2, \sigma_d^2 \sim \beta_{jd} | 
  \gamma_d^2$. However, in our case
  that is debatable. The $D$ regressions need not be on the same scale, so in
  order to jointly consider the $\bm{\beta}_d$ some 
  form of calibration is required. A straightforward way 
  of doing this is to include the scale the $\bm{\beta}_d$ prior with 
  $\sigma_d^2$ as in (\ref{eq:betaprior}).
  
  The normal inverse Gaussian (NIG) prior model for $\beta_{jd}$ and 
  $\gamma^2_d$ in (\ref{eq:priormodel}) was introduced in 
  \cite{barndorff-nielsen_hyperbolic_1978} and is, since 
  \cite{barndorff-nielsen_normal_1997}, routinely applied in mathematical 
  finance \cite[]{kalemanova_normal_2007}. In Figure 
  \ref{fig:dens_igaussian_marginalbeta}, the NIG 
  prior distribution is depicted together with two common prior choices, ridge 
  and lasso. From this Figure we see that, depending on the choice
  of hyperparameters, the NIG prior can vary from heavier tails than the lasso, 
  to more Gaussian tails like the ridge.
<<dens_igaussian_marginalbeta, fig.cap="Marginal $\\beta_j$ prior for several choices of $\\gamma^2_d$ hyperpior, scaled to $\\V(\\beta_j)=1$.", out.width="60%", fig.asp=1>>=
library(GeneralizedHyperbolic)
# all scaled such that variance is one
sigma <- 1
theta <- 1/sigma^2
lambda <- c(0.1, 2)
lambda1 <- sqrt(2)
col <- c(1:4)
lty <- c(1:4)
labels <- as.expression(c(bquote("IG, "~lambda==.(lambda[1])~", "~
                                   theta==.(theta)),
                          bquote("IG, "~lambda==.(lambda[2])~", "~
                                   theta==.(theta)),
                          "Point mass (ridge)", "Exponential (lasso)"))
dprior <- function(x, lambda, theta, sigma) {
  dnig(x, 0, sigma*sqrt(lambda), sqrt(lambda/(theta*sigma)), 0)
}
dlasso <- function(x, lambda1) {
  0.5*lambda1*exp(-lambda1*abs(x))
}

curve(dprior(x, lambda[1], theta, sigma), -3, 3, ylab=expression(p(beta[j])), 
      xlab=expression(beta[j]), n=1000, 
      col=col[1], lty=lty[1])
curve(dprior(x, lambda[2], theta, sigma), add=TRUE, n=1000, 
      col=col[2], lty=lty[2])
curve(dnorm(x, 0, 1), add=TRUE, n=1000, col=col[3], lty=lty[3])
curve(dlasso(x, lambda1), add=TRUE, n=1000, col=col[4], lty=lty[4])
legend("topright", legend=labels, col=col, lty=lty, 
       title="Hyperprior")
@
  
  Our argumentation to model the $\gamma^2_d$ by an inverse Gaussian 
  distribution, as has been suggested in \cite{fabrizi_specification_2016} and 
  \cite{caron_sparse_2008}, is two-fold: (i) the NIG model is more flexible than
  the standard ridge and (ii) allows to model the mean $\theta_d$ as a 
  function of the co-data $\mathbf{c}_d$, as explained in Section 
  \ref{sec:empiricalbayes}. 
	
  A few remarks on the choice of error variance prior are justified here: 
  many authors endow error variance components with vague gamma priors. 
  \cite{gelman_prior_2006}, 
  among others, advises against this practice. The degree of `vagueness' has a 
  large influence on the posterior, while degree of `vagueness' is a difficult 
  parameter to set. This influence is especially pronounced if the likelihood is
  relatively flat, as may be reasonably expected in the large $p$, small $n$ 
  setting we are in. We therefore model the error variance with Jeffrey's prior 
  \cite[]{jeffreys_invariant_1946}. In the derivation of our Jeffrey's prior
  for the error variance, we jointly consider an unknown mean and variance.
  
  - Jeffrey's prior for variance $1/\sigma^{3/2}$ (a priori independent data 
	mean and variance) or $1/\sigma^3$ (joint prior, but only mean and variance, 
	not variance and betas).
	
	\section{Estimation}
	\subsection{Variational Bayes}\label{sec:variationalbayes}
	The posterior corresponding to the model described in (\ref{eq:linearmodel})
	and (\ref{eq:priormodel}) is not available in closed form. We therefore 
	approximate the posterior by variational Bayes, where we force the posterior 
	density $Q_d$ to factorise as: $p(\bbeta_d, \sigma_d^2, \gamma_d^2) \approx 
	Q_d(\cdot) = q(\bbeta_d) \cdot q(\sigma_d^2) \cdot q(\gamma_d^2)$.
	Under such a factorisation, the marginal variational posteriors that 
	minimise the Kullback-Leibler divergence of the true posterior to the 
	variational Bayes approximation \cite[]{neal_view_1998}, are given by:
	\begin{align*}
    q(\bm{\beta}_d) & \overset{D}{=} \mathcal{N}_p 
    (\bm{\mu}_d, \bm{\Sigma}_d), \\
    q(\gamma^2_d) & \overset{D}{=} \mathcal{GIG}
    \(-\frac{p+1}{2}, \lambda_d \theta_d^{-2}, \delta_d\), \\
    q(\sigma^2_d) & \overset{D}{=} \Gamma^{-1} \(\frac{n + p + 1}{2}, \zeta_d\).
  \end{align*}
	The variational parameters $\bm{\mu}_d$, $\bm{\Sigma}_d$, $\delta_d$, and
	$\zeta_d$ contain cyclic dependencies and are iteratively 
	updated by:
  \begin{align*}
    \bm{\Sigma}_d^{(h+1)} & = (a_d^{(h)})^{-1} 
    (\X \tr \X + b_d^{(h)} \I)^{-1}, \\
    \bm{\mu}_d^{(h+1)} & = (\X \tr \X + b_d^{(h)} \I)^{-1} \X \tr \y_d, \\
    \delta_d^{(h+1)} & = \bmu_d \tr \bmu_d + \lambda_d, \\ 
    \zeta_d^{(h+1)} & = \frac{1}{2} \bigg[\mathbf{y}_d \tr \mathbf{y}_d -
    2 \mathbf{y}_d \tr \X \bm{\mu}_d^{(h+1)} + 
    \trace ( \X \tr \X \bm{\Sigma}_d^{(h+1)}) + 
    (\bm{\mu}_d^{(h+1)}) \tr \X \tr \X \bm{\mu}_d^{(h+1)} \\
    & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + 
    b_d^{(h+1)} \trace ( \bm{\Sigma}_d^{(h+1)}) + 
    b_d^{(h+1)} (\bm{\mu}_d^{(h+1)}) \tr \bm{\mu}_d^{(h+1)}\bigg],
  \end{align*}
  until convergence. Here, we set
  \begin{align*}
    a_d^{(h)} & =\E_{Q^{(h)}}(\sigma_d^{-2})=(n + p + 1)/(2 \zeta_d^{(h)}), \\
    b_d^{(h)} & = \E_{Q^{(h)}}(\gamma_d^{-2}) = \sqrt{\frac{\lambda_d}
    {\theta_d^2 \delta_d^{(h)}}} \frac{K_{\frac{p - 1}{2}} 
    \( \sqrt{\lambda_d \delta_d^{(h)}}/\theta_d \)}
    {K_{\frac{p+1}{2}} \( \sqrt{\lambda_d \delta_d^{(h)}}/\theta_d \)} + 
    \frac{p+1}{\delta_d^{(h)}}.
  \end{align*}
  
  \subsection{Empirical Bayes}\label{sec:empiricalbayes}
  A full Bayesian model requires the specification of hyper parameters 
  $\bm{\theta}=\begin{bmatrix} \theta_1 & \cdots & \theta_D \end{bmatrix} \tr$ 
  and $\begin{bmatrix} \lambda_1 & \cdots & \lambda_D \end{bmatrix} \tr$. 
  These are abstract and hard to interpret parameters 
  for which we generally lack expert knowledge. They do, however, have a 
  significant influence on the shape of the posterior distribution. We 
  therefore propose to estimate these hyper parameters by empirical Bayes. 
  Simply put, empirical Bayes fits the prior model to the data and is, as such, 
  objective (up to model specification). 
	
	The canonical method for empirical Bayes is maximisation of the marginal 
	likelihood with respect to the hyper parameters. In 
	\cite{casella_empirical_2001} the marginal likelihood is maximised by an EM 
	algorithm:
	$$
	\bm{\theta}^{(l+1)},\bm{\lambda}^{(l+1)} = \underset{\bm{\theta},\bm{\lambda}}
	{\argmax}\E_{\cdot | \Y} 
	[\log p(\Y, \B, \bgamma^2, \bsigma^2) | \bm{\theta}^{(l)},\bm{\lambda}^{(l)}],
	$$
	where $\Y = \begin{bmatrix} \y_1 & \cdots & \y_D \end{bmatrix}$,
	$\B = \begin{bmatrix} \bbeta_1 & \cdots & \bbeta_D \end{bmatrix}$, 
	$\bgamma^2 = \begin{bmatrix} \gamma^2_1 & \cdots & \gamma^2_D 
	\end{bmatrix}\tr$, $\bsigma^2 = \begin{bmatrix} \sigma^2_1 & \cdots & 
	\sigma^2_D \end{bmatrix} \tr$,
	and the expectation is with respect to the posterior. In our case, this 
	posterior is not available in closed form, which renders the expectation 
	difficult. While \cite{casella_empirical_2001} suggests to approximate the 
	expectation by a Monte Carlo sample, we propose to use the variational Bayes 
	approximation developed in Section \ref{sec:variationalbayes}:
	\begin{align*}
	  \bm{\theta}^{(l+1)},\bm{\lambda}^{(l+1)} & = 
	  \underset{\bm{\theta},\bm{\lambda}}{\argmax}\E_{Q^{(l)}} 
	  [\log p(\Y, \B, \bgamma^2, \bsigma^2)] \\
	  & = \underset{\bm{\theta},\bm{\lambda}}{\argmax} \E_{Q^{(l)}} 
	  [\log \pi (\B | \bgamma^2, \bsigma^2)] + 
	  \E_{Q^{(l)}} [\log \pi (\bgamma^2)].
	\end{align*}
	where now the expectation is with respect to the converged variational 
	posterior $Q^{(l)}=\prod_{d=1}^D Q_d^{(l)}$. 
	
	To make use of the outcome-specific codata, we parametrise 
	$\theta_d = (\mathbf{c}_d \tr \bm{\alpha})^{-1}$ and estimate $\blambda$ and
	$\bm{\alpha}$. This allows us to include continuous co-data on the outcomes
	in an objective, data-driven manner. The estimating equations are:
  \begin{align*}
    \bm{\alpha} & = \[ \mathbf{C} \tr \diag 
    (\lambda_d e_d^{(l)}) \mathbf{C} \]^{-1} \mathbf{C} \tr \diag 
    (\lambda_d) \mathbf{1}_{D \times 1}, \\
    \lambda_d & = \[ b_d^{(l)} + e_d^{(l)} (\mathbf{c}_d \tr \bm{\alpha})^2 - 
    2\mathbf{c}_d \tr \bm{\alpha} \]^{-1},
  \end{align*}
  where $e_d^{(l)} = \E_{Q^{(l)}}(\gamma_d^2) = [b_d^{(l)} - 
  (p + 1)/\delta_d^{(l)})] \cdot \delta_d^{(l)} 
  (\theta_d^{(h)})^2/\lambda_d^{(l)}$. Solving the equations is done by 
  iteratively reweighted least squares of responses $(e_d^{(l)})^{-1}$ on 
  predictors $\mathbf{c}_d$ with weights $\lambda_d e_d^{(l)}$. 
  
  \section{Comparison of NIG model to Normal inverse Gamma}
  - check model with Stan.
  
  \section{Comparison of conjugate to non-conjugate}
  
  \section{Variational Bayes versus MCMC}
  
  \section{Inverse Gaussian Empirical Bayes versus inverse Gamma}
  
  \section{Simulations for VB}
	\subsection{Setup}
  
  
  We simulate from a correctly specified model given in \ref{eq:priormodel}. We assume random $y_{id}$, $\x_i$, and $\bm{\beta}_d$, while we fix $\mathbf{C}$, $\gamma_d^2$, $\sigma_d^2$ and $\bm{\alpha}$. We estimate the $\bm{\mu}_d$, $\bm{\Sigma}_d$, $\delta_d$, $\bm{\alpha}$ and $\lambda$, while we fix the $\sigma_d^2$ to their true values.
  
  We set $n=100$, $p=200$, and $D=10$. We consider 5 evenly sized classes of drugs, such that $\mathbf{c}_{kd} = \mathbbm{1}(k = \text{class}_d)$ and $\begin{bmatrix} \mathbf{c}_1 & \cdots & \mathbf{c}_D \end{bmatrix} \tr$. From these the $\gamma_d^2 = (\mathbf{c}_d \tr \bm{\alpha})^{-1}$ are created. Next, we simulate $\bm{\beta}_{jd} \sim \mathcal{N} (0, \sigma^2_d \gamma_d^2)$. We fix the mean signal-to noise ratio $\overline{\text{SNR}}_d$, such that the predictor data variance is $s^2 = \V(\mathbf{x}_i) = \overline{\text{SNR}}_d / (\overline{\gamma^2_d} p)$ (NOT CORRECT). The last step consists of simulating the data by $\mathbf{x}_{ij} \sim \mathcal{N}(0,s^2)$ and $y_{id} = \mathcal{N} (\mathbf{x}_i \tr \bm{\beta}_d, \sigma_d^2)$. The remaining parameters are set as follows: $\bm{\alpha} = \begin{bmatrix} 1 & \dots & 5 \end{bmatrix} \tr$, $\sigma_d^2 = 1$ for all $d$, and $\overline{\text{SNR}}_d = 10$. To account for the random data and parameter generation, we repeat the simulation 100 times. We compare the results to the inverse Gamma and non-conjugate model, and present the results in Figures \ref{fig:boxplot_igaussian_res1_prior_mean}-\ref{fig:boxplot_igaussian_res2_prior_mean}.

<<boxplot_igaussian_res1_prior_mean, fig.cap="Prior mean estimates", out.width="80%", fig.asp=2/3>>=
@

<<boxplot_igaussian_res2_prior_mean, fig.cap="Prior mean estimates", out.width="80%", fig.asp=2/3>>=
@

  \section{Application}
  
  \section{Discussion}
	
  \section*{Software}
	A (developmental) \texttt{R} package is available from 
	\url{https://github.com/magnusmunch/cambridge}. 

	\section*{Supplementary Material}
	Supplementary Material is available online from 
	\url{https://github.com/magnusmunch/cambridge}. 
	
	\section*{Reproducible Research}
	All results and documents may be recreated from 
	\url{https://github.com/magnusmunch/cambridge}.
	
	\section*{Acknowledgements}
	\textit{Conflict of Interest}: None declared.

	\bibliographystyle{author_short3.bst}
	\bibliography{refs}
	
	\section*{Session info}

<<r session-info, eval=TRUE, echo=TRUE, tidy=TRUE>>=
devtools::session_info()
@

\end{document}
