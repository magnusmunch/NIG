% set options
<<settings, include=FALSE, echo=FALSE>>=
opts_knit$set(root.dir="..", base.dir="../figs")
opts_chunk$set(fig.align='center', fig.path="../figs/", 
               echo=FALSE, cache=FALSE, message=FALSE, fig.pos='!ht')
knit_hooks$set(document=function(x) {
  sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed=TRUE)})
options(knitr.kable.NA="")
@

% read figure code
<<figures, include=FALSE>>=
read_chunk("code/figures.R")
@

\documentclass[a4paper,hidelinks]{article}
\usepackage{bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,
rotating,caption,subcaption,natbib,appendix,titlesec,multicol,hyperref,verbatim,
bbm,algorithm,algpseudocode,pgfplotstable,threeparttable,booktabs,mathtools,
dsfont,parskip,xr,tikz}
\externaldocument[sm-]{supplement}
\usetikzlibrary{bayesnet}

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Em}{\mathbf{E}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\Lm}{\mathbf{L}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}

% settings
\pgfplotsset{compat=1.16}
\graphicspath{{../figs/}}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Drug sensitivity prediction with normal inverse Gaussian shrinkage 
informed by external data}
\date{\today}
\author{Magnus M. M\"unch$^{1,2,3}$\footnote{Correspondence to: 
\href{mailto:m.munch@amsterdamumc.nl}{m.munch@amsterdamumc.nl}}, Mark A. van de 
Wiel$^{1,3}$, Sylvia Richardson$^{3}$, \\ and Gwena{\"e}l G. R. Leday$^{3}$}

\begin{document}

	\maketitle
	
	\noindent
	1. Department of Epidemiology \& Biostatistics, Amsterdam UMC, VU University, 
	PO Box 7057, 1007 MB Amsterdam, The Netherlands \\
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\
	3. MRC Biostatistics Unit, Cambridge Institute of Public Health, Cambridge,
	United Kingdom
	
	\begin{abstract}
		{In precision medicine, a common problem is drug sensitivity prediction from
		cancer tissue cell lines. These types of problems entail modelling 
		multivariate drug responses on high dimensional molecular feature sets in 
		typically $>1000$ cell lines. The dimensions of the problem require 
		specialised models and estimation methods. In addition, external information
		on both the drugs and the features is often available. We propose to 
		model the drug responses through a linear regression with shrinkage enforced
		through a normal
		inverse Gaussian prior. We let the prior depend on the external information,
		and estimate the model and external information dependence in an 
		empirical-variational Bayes framework. We demonstrate the usefullness of
		this model in both a simulated setting and in the publicly available
		Genomics of Drug Sensitivity in Cancer data.}
	\end{abstract}
	
	\noindent\textbf{Keywords}: Drug sensitivity; Empirical Bayes; 
	Genomics of Drug Sensitivity in Cancer (GDSC); Variational Bayes
	
	\noindent\textbf{Software available from}: 
	\url{https://github.com/magnusmunch/NIG}
	
	\section{Introduction}\label{sec:introduction}
	Recently, promising results in precision medicine 
	have sparked an interest in cancer drug sensitivity prediction models
	\cite[]{iorio_landscape_2016}. Typically, these models 
	predict the drug sensitivity for new patients from a set of molecular 
	features.
	Development of such models is often done in well-characterised human cancer
	tissue cell lines. The current paper presents a novel drug sensitivity
	prediction model and an application to a real drug
	sensitivity data set.
	
	Development of such models from cell lines has proven to be difficult
	(see e.g., the DREAM 7 challenge in \cite{costello_community_2014}). 
	Difficulties arise, among others, from the dimensions of the problem. 
	Typically, the data contains
	hundreds of drugs, thousands of cell lines, and thousands of molecular 
	features. An example of a large database of drug responses and molecular
	features is the Genomics of Drug Sensitivity in Cancer (GDSC) data 
	\cite[]{yang_genomics_2013}, which we will further 
	investigate in Section \ref{sec:gdsc}. Other examples of such databases 
	include
	the Cancer Cell Line Encyclopedia (CCLE) \cite[]{li_landscape_2019} and 
	the US National Cancer Institute 60 human tumour cell line anticancer drug
	screen (NCI60) \cite[]{shoemaker_NCI60_2006}. The dimensions of these data
	prohibit the estimation of standard regression models and
	typically require some form of regularisation.
	
	The GDSC database contains additional information on both the drugs and 
	molecular features, such as the target 
	pathways and developmental stages of the drugs. Additional online
	repositories may provide extra information such as the molecular weight of
	the compounds or the publication signatures of the molecular features. In some
	cases, prior knowledge on the drug efficacies may be available,
	from previous experiments. We propose to include these possibly beneficial 
	information sources in the estimation of the sensitivity prediction models in 
	a data-driven manner. More specifically, we estimate a normal 
	inverse Gaussian model, where the extent of regularisation is estimated by an 
	adaptive empirical Bayes procedure, guided by the external information.
	
	We are not the first to work on drug sensitivity prediction models. Reviews on 
	the topic are \cite{azuaje_computational_2017} and \cite{ali_machine_2019}.
	\cite{zhao_structured_2019} and \cite{mai_composite_2019} consider a
	structured penalized multivariate regression approach. 
	\cite{aben_tandem:_2016} introduce a two-stage penalized regression 
	model that includes two different types of molecular features.
	\cite{ammad-ud-din_drug_2016} and \cite{costello_community_2014} tackle the
	problem through a multiple kernel learning approach. Our solution allows for
	the adaptive incorporation of the external 
  information on drugs and features. This is done by
  pooling information, both across drugs and features. Estimation of the model
  is through computational feasible variational bayes approximations, while 
  empirical Bayes estimation of tuning parameters pools information across drugs
  and features in a data-driven manner.
  
	The rest of the paper is structured as follows. In Section 
	\ref{sec:model} we introduce our model, the estimation of which is detailed
	in Section \ref{sec:estimation}. Section \ref{sec:simulations} describes a 
	simulation study that investigates the estimation of hyperparameters by the
	proposed method. In Section \ref{sec:gdsc} we analyse the
	GDSC data, and we end with a discussion in Section \ref{sec:discussion} on the 
	pros and cons of the proposed method.
	
	\section{Model}\label{sec:model}
	\subsection{Simultaneous equations model}\label{sec:SEM}
	Let $y_{id}$ be the continuous sensitivity measures 
	for cell lines $i=1,\dots, n$, and drug $d=1, \dots, D$. 
	We predict sensitivity from molecular features 
	$x_{ij}$, $j=1,\dots, p$, collected in 
	$\x_i = \begin{bmatrix} x_{i1} & \dots & x_{ip} \end{bmatrix} \tr$. 
	We assume that both covariates and responses have been centred per drug
	and regress the drug sensitivities on the molecular features:
	\begin{equation}\label{eq:likelihood}
	  y_{id} = \x_i \tr \bbeta_d + \epsilon_{id}, 
	  \text{ with } \epsilon_{id} \sim \mathcal{N}(0, \sigma_d^2),
	\end{equation}
	where the $p$-dimensional 
	$\bbeta_d = \begin{bmatrix} \beta_{1d} & \cdots & \beta_{pd} \end{bmatrix} 
	\tr$ are the drug-specific omics feature 
	effects. Note that (\ref{eq:likelihood}) gives rise to a system of 
	D linear regression equations.
	
	The cell lines used in drug response models are often taken from different
	tissues. In addition, other clinical covariates might be available. 
	To obtain unbiased feature effects, one may wish to account for these. 
	We do so by introducing unpenalized covariates, the $\beta_{jd}$ coefficients 
	of which are endowed with a flat prior. For the sake of clarity, in the
	following, such unpenalized covariates are omitted. However, the available
	software allows for their inclusion.
	
	\subsection{Bayesian prior model}\label{sec:prior}
	We carry out inference by endowing the parameters with the following priors:
	\begin{subequations}\label{eq:prior}
		\begin{align}
		  \beta_{jd} | \gamma^2_{jd}, \tau_d^2, \sigma^2_d & \sim \mathcal{N}_{p} 
		  (0, \gamma_{jd}^2 \tau_d^2 \sigma_d^2), 
		  \label{eq:betaprior}\\
		  \gamma_{jd}^{2} & \sim \mathcal{IG}(\phi_{jd}, \lambda_{\text{feat}}), 
		  \label{eq:gammaprior}\\
		  \tau_d^{2} & \sim \mathcal{IG}(\chi_d, \lambda_{\text{drug}}), 
		  \label{eq:tauprior}\\
		  \sigma_d^{2} & \sim 1/\sigma_d^{3}, \label{eq:sigmaprior}
		\end{align}
	\end{subequations}
	where $\mathcal{IG}(\phi, \lambda)$ denotes an inverse Gaussian distribution 
	with mean $\phi$ and shape $\lambda > 0$. 
	
	In model (\ref{eq:prior}), $\gamma_{jd}^2$ in (\ref{eq:gammaprior}) denotes a 
	local variance 
	component that is supposed to capture local, feature-specific variation in the
	model parameters $\beta_{jd}$ in (\ref{eq:betaprior}), 
	while the global variance components $\tau_d^2$ in (\ref{eq:tauprior})
	capture the drug-specific, general trend in $\bm{\beta}_{d}$. Each drug 
	response is endowed with a random error variance $\sigma_d^2$, distributed
	according to (\ref{eq:sigmaprior}).
	
	Prior distributions of the form
	(\ref{eq:prior}) are often referred to as global-local shrinkage rules 
	\cite[]{bernardo_shrink_2011}, due to the multiplicative separation of the 
	prior variance into a local component $\gamma_{jd}^2$ and a global component
	$\tau_d^2$. For appropriate local shrinkage in global-local 
	shrinkage models it is important  to account for 
  different noise levels $\sigma_d^2$ by scaling the $\beta_{jd}$ variances 
  accordingly.

	The normal inverse Gaussian (NIG) prior model was introduced in 
  \cite{barndorff-nielsen_hyperbolic_1978} and since 
  \cite{barndorff-nielsen_normal_1997} it is routinely applied in mathematical 
  finance (see, e.g., \cite{kalemanova_normal_2007}). Here we extend it with an
  additional global variance component $\tau_d^2$. Supplementary Material
  (SM) Section \ref{sm-sec:prior} contains more details on the NIG prior.
  To illustrate the effect of the NIG prior on the posterior mean, we consider 
  the prior reparametrised as in 
  \cite{carvalho_handling_2009}, i.e., in terms of shrinkage weights 
  $\kappa_{jd}=1/(1 + \gamma_{jd}^2) \in (0, 1)$.
  Under the (simplified) normal means model, i.e., 
  $\X= \begin{bmatrix} \x_1 & \cdots \x_n \end{bmatrix} \tr =\I_p$, with fixed 
  $\tau_d^2=\sigma_d^2=1$, the resulting conditional
  posterior mean for the $\beta_{jd}$ is
  $\E(\beta_{jd} | y_{jd}, \kappa_{jd}) = (1 - \kappa_{jd}) y_{jd}$. 
  Thus, $\kappa_{jd}=0$ implies no shrinkage of $\beta_{jd}$ and $\kappa_{jd}=1$
  implies full shrinkage towards zero. Figure 
  \ref{fig:dens_kappa} depicts the prior on $\kappa_{jd}$ implied by 
  several choices of $\beta_{jd}$ prior.
<<dens_kappa, fig.cap="Implied prior densities $\\pi(\\kappa_{jd})$ for the (a) NIG, (b) Student's $t$, and (c) lasso priors. Different line types correspond to different hyperparameter settings. The hyperparameter settings (given in Section \\ref{sm-sec:hyperparameters} of the SM) were chosen to show some possible, distinct shapes that each of the priors can take.", out.width="100%", fig.asp=1/3>>=
@
  
  Figure \ref{fig:dens_kappa} shows that, depending on the choice
  of hyperparameters, the NIG prior can behave similarly to the Student's $t$ 
  prior (decreasing form zero, with substantial mass close to zero and 
  little mass close to one, like the
  solid lines in Figure \ref{fig:dens_kappa}a-\ref{fig:dens_kappa}b), 
  but also rather differently (dashed and dotted lines in 
  Figure \ref{fig:dens_kappa}a-\ref{fig:dens_kappa}b). Our argumentation to 
  model the $\gamma^2_{jd}$ by an inverse Gaussian 
  distribution, as has been suggested in \cite{fabrizi_specification_2016} and 
  \cite{caron_sparse_2008}, is three-fold: (i) the NIG model is more flexible 
  than the lasso prior (as seen from Figure 
  \ref{fig:dens_kappa}), (ii) the NIG prior allows to model
  the means of the $\gamma_{jd}^2$ ($\phi_{jd}$) and $\tau_{jd}^2$ 
  ($\chi_{d}$) as a function of external data 
  more conveniently than the Student's $t$ prior, as explained in Section 
  \ref{sec:empiricalbayes}, and (iii) like the horseshoe 
  \cite[]{carvalho_handling_2009}, the NIG shrinkage
  weights prior can put mass both near zero and one, a desirable property of 
  shrinkage priors \cite[]{bernardo_shrink_2011}.

  A few remarks on the choice of error variance prior are justified here: 
  many authors endow error variance components with vague gamma priors. 
  \cite{gelman_prior_2006}, 
  among others, advises against this practice. The degree of `vagueness' has a 
  large influence on the posterior, while degree of `vagueness' is a difficult 
  parameter to set. This influence is especially pronounced if the likelihood is
  relatively flat, as may be reasonably expected in the large $p$, 
  small $n$ setting. We therefore model the error variance with Jeffreys 
  objective prior 
  \cite[]{jeffreys_invariant_1946} that does not depend on any 
  subjective specification of hyperparameters. In the derivation of our 
  Jeffreys prior
  for the error variance, we jointly consider an unknown data mean and variance 
  \cite[]{kass_selection_1996}. This
  joint consideration results in the somewhat unorthodox $1/\sigma^3$ Jeffreys
  prior.
	
	\subsection{External information}\label{sec:external}
	In drug sensitivity prediction models, external information on both the drugs
	and features is often available. Here, we assume this information to be 
	available as
	external feature `covariates' $\mathbf{c}_{jdg}$, for $g=1, \dots, G$, and 
	drug `covariates' $\mathbf{z}_{dh}$, for $h=1,\dots, H$. An example of a 
	(binary) feature covariate is target pathway presence, with $c_{jdg}=0$ if 
	gene $j$ is present in the 
	target pathway of drug $d$ and $c_{jdg}=1$ if it is not. An example of a 
	(ternary) drug
	covariate is developmental phase, with levels experimental phase, 
	clinical development, and approved by a governing agency.
	
	The external covariates come in through our mean models for 
	the $\gamma_{jd}^2$ and $\tau_d^2$ hyperpriors:
	$\phi_{jd} = (\mathbf{c}_{jd} \tr \bm{\alpha}_{\text{feat}})^{-1}$ and
	$\chi_d = (\mathbf{z}_{d} \tr \bm{\alpha}_{\text{drug}})^{-1}$, with 
	$\mathbf{c}_{jd} = \begin{bmatrix} c_{jd1} & \cdots & c_{jdG} \end{bmatrix}$
	and $\mathbf{z}_{d} = \begin{bmatrix} z_{d1} & \cdots & z_{dH} \end{bmatrix}$,
	where categorical external covariates are dummy coded.
	The model now requires hyperparameters $\bm{\alpha}_{\text{feat}}$, 
	$\lambda_{\text{feat}}$, $\bm{\alpha}_{\text{drug}}$, and $
	\lambda_{\text{drug}}$, which we estimate in a data-driven manner (see Section
	\ref{sec:empiricalbayes}).
  
  A representation of our model as a Bayesian DAG is given in Figure
  \ref{fig:dag}. We note that in many settings, the set of features might be
  different for different drugs. In that case the covariates are indexed by
  the drug $d$: $\X^d$, a trivial extension of model 
  (\ref{eq:likelihood}) and (\ref{eq:prior}). This extension is included in the
  available software, but for clarity it is omitted in the following. 
  
  \begin{figure}
    \centering
    \tikz{ %
      % Y
      \node[obs] (y) {$y_{id}$}; %
      
      % X
      \node[det, above=of y, yshift=1cm] (x) {$x_{ij}$}; %
      
      % sigma2
      \node[latent, right=of y] (sigma2) {$\sigma_d^2$}; %
      
      % beta
      \node[latent, right=of x] (beta) {$\beta_{jd}$}; %
      
      % gamma2
      \node[latent, right=of beta] (gamma2) {$\gamma_{jd}^2$}; %
      \node[const, right=of gamma2, yshift=0.5cm] (phi) {$\phi_{jd}$}; %
      \node[const, right=of gamma2, yshift=-0.5cm] (lambdaf) 
      {$\lambda_{\text{feat}}$}; %
      \node[det, right=of gamma2, xshift=2cm, yshift=0.5cm] (c) {$c_{jdg}$}; %
      
      % tau2
      \node[latent, right=of sigma2] (tau2) {$\tau_d^2$}; %
      \node[const, right=of tau2, yshift=0.5cm] (chi) {$\chi_{d}$}; %
      \node[const, right=of tau2, yshift=-0.5cm] (lambdad) 
      {$\lambda_{\text{drug}}$}; %
      \node[det, right=of tau2, xshift=2cm, yshift=0.5cm] (z) {$z_{dh}$}; %
      
      % edges
      \edge {x} {y};%
      \edge {sigma2} {y};%
      \edge {beta} {y};%
      \edge {sigma2} {beta};%
      \edge {gamma2} {beta};%
      \edge {tau2} {beta};%
      \edge {phi} {gamma2};%
      \edge {lambdaf} {gamma2};%
      \edge {chi} {tau2};%
      \edge {lambdad} {tau2};%
      \edge {c} {phi}; %
      \edge {z} {chi}; %
      
      % plates
      \plate{plateg} {(c)} {$g=1,\dots,G$}
      \plate{plateh} {(z)} {$h=1,\dots,H$}
      \plate{platei} {(y) (x)}{$i=1,\dots,n$}; %
      \plate{platej} {(x) (beta) (gamma2) (phi) (lambdaf) (c) (plateg)}
      {$j=1,\dots,p$}; %
      \plate{plated} 
      {(y) (x) (sigma2) (beta) (gamma2) (tau2) (phi) (lambdaf)
      (chi) (lambdad) (c) (z) (platei) (platej) (plateg) (plateh)}
      {$d=1,\dots,D$}; %
    }
    \caption{Hierarchical representation of the drug sensitivity prediction 
    model. Grey circles 
    represent observed variables, white circles represent unobserved variables,
    tilted squares represent fixed data, and unenclosed letters are parameters 
    to be estimated. Cell lines are indexed by $i$, features
    by $j$, drugs by $d$, drug covariates by $h$, and feature covariates by 
    $g$. The $y_{id}$ are the drug sensitivities, $x_{ij}$ the 
    molecular features, 
    $c_{jdg}$ the external feature covariates, $z_{dh}$ the external drug
    covariates, $\beta_{jd}$ the regression coefficients, $\sigma_d^2$ the 
    error variances, $\tau_d^2$ and $\gamma_{jd}^2$ the drug and feature
    specific variance components, respectively, and $\phi_{jd}$, 
    $\lambda_{\text{feat}}$, $\chi_{d}$, and $\lambda_{\text{drug}}$ the 
    hyperparameters.}
    \label{fig:dag}
  \end{figure}
	
	\section{Estimation}\label{sec:estimation}
	\subsection{Variational Bayes}\label{sec:variationalbayes}
	The posterior corresponding to the model described in (\ref{eq:likelihood})
	and (\ref{eq:prior}) is not available in closed form. 
	To avoid computationally intensive markov chain Monte Carlo (MCMC) 
	algorithms, we approximate the joint posterior by variational Bayes (see 
	\cite{blei_variational_2017} for a review), where the approximate
	posterior density factorises as: $p(\bbeta_d, \bm{\gamma}^2_d, \tau_{d}^2, 
	\sigma_d^2 | \y_d) \approx 
	Q_d(\cdot) = q(\bbeta_d) \cdot q(\bm{\gamma}_{d}^2) \cdot q(\tau_{d}^2) 
	\cdot q(\sigma_d^2)$, where $\bm{\gamma}_{d}^2 = 
	\begin{bmatrix} \gamma_{1d}^2 & \cdots & \gamma_{pd}^2 \end{bmatrix} \tr$. For
  notational convenience, we slightly abuse notation and let $q(\cdot)$ denote
	different densities for different inputs.
	Under such a factorisation, the marginal variational posteriors that 
	minimise the Kullback-Leibler divergence of the true posterior to the 
	variational Bayes approximation \cite[]{neal_view_1998} are given by:
	\begin{align*}
    q(\bm{\beta}_d) & \overset{D}{=} \mathcal{N}_{p} 
    (\bm{\mu}_d, \bm{\Sigma}_d), \\
    q(\bm{\gamma}_{d}^2) & \overset{D}{=} \prod_{j=1}^p \mathcal{GIG}
    \(-1, \lambda_{\text{feat}}/\phi_{jd}^2, \delta_{jd}\), \\
    q(\tau^2_d) & \overset{D}{=} \mathcal{GIG} \(-\frac{p + 1}{2}, 
    \lambda_{\text{drug}}/\chi_d^2, \eta_d\), \\
    q(\sigma^2_d) & \overset{D}{=} \Gamma^{-1} \(\frac{n + p + 1}{2}, \zeta_d\),
  \end{align*}
	where $\mathcal{GIG} \(p, \nu, \eta\)$ denotes the generalized inverse Gaussian
	distribution with index $p \in \mathbb{R}$, and scales $\nu > 0$ and 
	$\eta > 0$ \cite[]{jorgensen_statistical_1982}. See SM Section 
	\ref{sm-sec:vbderivations} for the derivations. The variational
	parameters $\bm{\mu}_d$, $\bm{\Sigma}_d$, $\delta_{jd}$, $\eta_d$, and
	$\zeta_d$ contain cyclic dependencies and are iteratively 
	updated by:
	\begin{subequations}\label{eq:vbequations}
    \begin{align}
      \bm{\Sigma}_d^{(h+1)} & = (a_d^{(h)})^{-1} 
      \[\X \tr \X + g_{d}^{(h)} \diag(b_{jd}^{(h)})\]^{-1}, \\
      \bm{\mu}_d^{(h+1)} & = \[\X \tr \X + g_{d}^{(h)} 
      \diag(b_{jd}^{(h)})\]^{-1} 
      \X \tr \y_d, \\
      \delta_{jd}^{(h+1)} & = a_d^{(h)} g_{d}^{(h)} \[(\bmu^{(h+1)}_{jd})^2 + 
      (\bSigma^{(h + 1)}_d)_{jj}\] + \lambda_{\text{feat}}, \\ 
      \eta_d^{(h+1)} & = a_d^{(h)} \sum_{j=1}^{p} b_{jd}^{(h + 1)}
      \[ (\bmu^{(h+1)}_{jd})^2 + (\bSigma^{(h + 1)}_d)_{jj} \] + 
      \lambda_{\text{drug}}, \\
      \zeta_d^{(h+1)} & = \frac{1}{2} \bigg[\mathbf{y}_d \tr \mathbf{y}_d -
      2 \mathbf{y}_d \tr \X \bm{\mu}_d^{(h+1)} + 
      \trace ( \X \tr \X \bm{\Sigma}_d^{(h+1)}) + 
      (\bm{\mu}_d^{(h+1)}) \tr \X \tr \X \bm{\mu}_d^{(h+1)} \\
      & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + 
      g_{d}^{(h + 1)} \trace \[ \diag (b_{jd}^{(h+1)}) \bm{\Sigma}_d^{(h+1)}\] + 
      g_{d}^{(h + 1)} (\bm{\mu}_d^{(h+1)}) \tr \diag (b_{jd}^{(h+1)}) 
      \bm{\mu}_d^{(h+1)}\bigg],
    \end{align}
  \end{subequations}
  until convergence, where $\y_d = \begin{bmatrix} y_{1d} & \cdots & y_{nd}
  \end{bmatrix} \tr$. Here, we set
  \begin{align}
    a_d^{(h)} & =\E_{Q^{(h)}}(\sigma_d^{-2})=(n + p + 1)/(2 \zeta_d^{(h)}), 
    \nonumber\\
    b_{jd}^{(h)} & = \E_{Q^{(h)}}(\gamma_{jd}^{-2}) = 
    \sqrt{\frac{\lambda_{\text{feat}}}{\phi_{jd}^2 \delta_{jd}^{(h)}}}
    \frac{K_0\(\sqrt{\delta_{jd}^{(h)}\lambda_{\text{feat}}/\phi_{jd}^2}\)}
    {K_1\(\sqrt{\delta_{jd}^{(h)}\lambda_{\text{feat}}/\phi_{jd}^2}\)} + 
    \frac{2}{\delta_{jd}^{(h)}}, \label{eq:ratiomodifiedbessel} \\
    g_d^{(h)} & =\E_{Q^{(h)}}(\tau_d^{-2})=
    \sqrt{\frac{\lambda_{\text{drug}}}{\chi_{d}^2 \eta_{d}^{(h)}}}
    \frac{K_{(p-1)/2}\(\sqrt{\eta_{d}^{(h)}\lambda_{\text{drug}}/\chi_{d}^2}\)}
    {K_{(p+1)/2}\(\sqrt{\eta_{d}^{(h)}\lambda_{\text{drug}}/\chi_{d}^2}\)} + 
    \frac{p + 1}{\eta_{d}^{(h)}}.
    \nonumber
  \end{align}
  where $K_{\nu}(x)$ denotes the modified Bessel function of the second kind.
  A method for fast and numerically stable calculation of ratios of
  modified Bessel functions of the second kind, as in 
  (\ref{eq:ratiomodifiedbessel}), is given in SM Section 
  \ref{sm-sec:ratiosmodifiedbessels}.
  
  \subsection{Empirical Bayes}\label{sec:empiricalbayes}
  We parametrised the prior 
  mean of the $\gamma_{jd}^2$ as $\phi_{jd}=(\mathbf{c}_{jd} \tr 
  \bm{\alpha}_{\text{feat}})^{-1}$ and the prior mean of $\tau_d^2$ as
  $\chi_{d}=(\mathbf{z}_{d} \tr \bm{\alpha}_{\text{drug}})^{-1}$.
  This parametrisation allows us to include feature and drug covariates,
  both continuous and discrete, into the 
  model. Additionally, it reduces the number of hyperparameters from 
  $pD$ to $|\bm{\alpha}_{\text{feat}}| + |\bm{\alpha}_{\text{drug}}| + 2$. The
  Bayesian model
  then requires the specification of the hyperparameters 
  $\bm{\alpha}=\begin{bmatrix} \bm{\alpha}_{\text{feat}} \tr & 
  \bm{\alpha}_{\text{drug}} \tr 
  \end{bmatrix} \tr$ and $\bm{\lambda} = \begin{bmatrix} \lambda_{\text{feat}} &
  \lambda_{\text{drug}}
  \end{bmatrix} \tr$. These are
  abstract and hard to interpret parameters 
  for which we generally lack expert knowledge. They do, however, have a 
  significant influence on the shape of the posterior distribution. We 
  therefore propose to estimate these hyperparameters by empirical Bayes.
  In our case, this results in an objective and data-driven inclusion of the
  external feature and drug covariates.
	
	The canonical method for empirical Bayes is to maximise the marginal 
	likelihood with respect to the hyperparameters. In 
	\cite{casella_empirical_2001} the marginal likelihood is maximised by an EM 
	algorithm:
	\begin{align*}
	  \bm{\alpha}^{(l+1)},\bm{\lambda}^{(l+1)} & = \underset{\bm{\alpha},
	  \bm{\lambda} > 0}{\argmax}\E_{\cdot | \Y} 
	  [\log p(\Y, \B, \bm{\Gamma}^2, \bm{\tau}^2, \bsigma^2) | \bm{\alpha}^{(l)},
	  \bm{\lambda}^{(l)}] \\
	  & = \underset{\bm{\alpha},\bm{\lambda} > 0}{\argmax} 
	  \E_{\cdot | \Y} [\log \pi (\bm{\Gamma}^2) | \bm{\alpha}_{\text{feat}}^{(l)},
	  \bm{\lambda}_{\text{feat}}^{(l)}] + 
	  \E_{\cdot | \Y} [\log \pi (\bm{\tau}^2) | 
	  \bm{\alpha}_{\text{drug}}^{(l)}, \bm{\lambda}_{\text{drug}}^{(l)}],
	\end{align*}
	where $\Y = \begin{bmatrix} \y_1 & \cdots & \y_D \end{bmatrix}$,
	$\B = \begin{bmatrix} \bbeta_1, \dots, \bbeta_D \end{bmatrix}$, 
	$\bm{\tau}^2 = \begin{bmatrix} \tau^2_1 & \cdots & 
	\tau^2_D \end{bmatrix} \tr$,
	$\bsigma^2 = \begin{bmatrix} \sigma^2_1 & \cdots & 
	\sigma^2_D \end{bmatrix} \tr$,
	and $\bm{\Gamma}^2 = \begin{bmatrix} \bgamma^2_1 & \cdots & \bgamma^2_D 
	\end{bmatrix}$,
	and the expectation is with respect to the joint posterior. 
	In our case, this 
	posterior is not available in closed form, which renders the expectation 
	difficult. While \cite{casella_empirical_2001} suggests to approximate the 
	expectation by a Monte Carlo sample, we propose to use the variational Bayes 
	approximation developed in Section \ref{sec:variationalbayes}:
	$$
	\bm{\alpha}^{(l+1)},\bm{\lambda}^{(l+1)} = 
	\underset{\bm{\alpha},\bm{\lambda} > 0}{\argmax}\E_{Q^{(l)}} 
	[\log \pi(\bm{\Gamma}^2)| \bm{\alpha}_{\text{feat}}^{(l)}, 
	\bm{\lambda}_{\text{feat}}^{(l)}] + 
	\E_{Q^{(l)}} [\log \pi(\bm{\tau}^2)| 
	\bm{\alpha}_{\text{drug}}^{(l)}, \bm{\lambda}_{\text{drug}}^{(l)}],
	$$
	where now the expectation is with respect to the converged variational 
	posterior $Q^{(l)}=\prod_{d=1}^D Q_d^{(l)}$. Note that the prior 
	$\bgamma^2_d$ and $\tau_d^2$ independence assumption 
	results in separate optimisation problems for the 
	feature hyperparameters ($\bm{\alpha}_{\text{feat}}$ and 
	$\bm{\lambda}_{\text{feat}}$), and the drug hyperparameters 
	($\bm{\alpha}_{\text{drug}}$ and $\bm{\lambda}_{\text{drug}}$).
	If we stack the drug and feature covariates:
	\begin{align*}
	  \mathbf{C} = 
	  \begin{bmatrix} \mathbf{c}_{11} \tr \\ 
	    \vdots \\
	    \mathbf{c}_{p1} \tr \\
	    \vdots \\
	    \mathbf{c}_{1D} \tr \\
	    \vdots \\
	    \mathbf{c}_{pD} \tr
	  \end{bmatrix} \text{ and }
	  \mathbf{Z} =
	  \begin{bmatrix} \mathbf{z}_{1} \tr \\ 
	    \vdots \\
	    \mathbf{z}_D \tr
	  \end{bmatrix},
	\end{align*}
	the empirical Bayes updates are given by:
  \begin{align*}
    \bm{\alpha}^{(l+1)}_{\text{feat}} & = \[ \mathbf{C} \tr \diag 
    (e_{jd}^{(l)}) \mathbf{C} \]^{-1} \mathbf{C} \tr 
    \mathbf{1}_{pD \times 1}, \\
    \lambda^{(l+1)}_{\text{feat}} & = pD \[ \sum_{d=1}^D \sum_{j=1}^{p} 
    b_{jd}^{(l)} + (\bm{\alpha}^{(l+1)}_{\text{feat}}) \tr \mathbf{C} \tr 
    \diag (e_{jd}^{(l)}) \mathbf{C} \bm{\alpha}^{(l+1)}_{\text{feat}} - 
    2 (\bm{\alpha}^{(l+1)}_{\text{feat}}) \tr \mathbf{C} \tr 
    \mathbf{1}_{pD \times 1} \]^{-1},\\
    \bm{\alpha}^{(l+1)}_{\text{drug}} & = \[ \mathbf{Z} \tr \diag 
    (f_{d}^{(l)}) \mathbf{Z} \]^{-1} \mathbf{Z} \tr 
    \mathbf{1}_{D \times 1}, \\
    \lambda^{(l+1)}_{\text{drug}} & = D \[ \sum_{d=1}^D g_{d}^{(l)} + 
    (\bm{\alpha}^{(l+1)}_{\text{drug}}) \tr \mathbf{Z} \tr \diag (f_{d}^{(l)})
    \mathbf{Z} \bm{\alpha}^{(l+1)}_{\text{drug}} - 
    2 (\bm{\alpha}^{(l+1)}_{\text{drug}}) \tr \mathbf{Z} \tr 
    \mathbf{1}_{D \times 1} \]^{-1},
  \end{align*}
  where SM Section \ref{sm-sec:eb} shows that $\bm{\lambda}^{(l+1)} > 0$ and
  \begin{align*}
    e_{jd}^{(l)} & = \E_{Q^{(l)}}(\gamma_{jd}^2| 
    \bm{\alpha}_{\text{feat}}^{(l)}, 
    \lambda_{\text{feat}}^{(l)}) = (b_{jd}^{(l)} - 
    2/\delta_{jd}^{(l)}) \cdot \delta_{jd}^{(l)} (\phi_{jd}^{(l)})^2/
    \lambda_{\text{feat}}^{(l)}, \\
    f_{d}^{(l)} & = \E_{Q^{(l)}}(\tau_{d}^2| \bm{\alpha}_{\text{drug}}^{(l)}, 
    \lambda_{\text{drug}}^{(l)}) = (g_{d}^{(l)} - 
    (p + 1)/\eta_{d}^{(l)}) \cdot \eta_{d}^{(l)} (\chi_{d}^{(l)})^2/
    \lambda_{\text{drug}}^{(l)}.
  \end{align*}
  
  To ensure proper and unbiased shrinkage, intercepts are included in 
  $\bm{\alpha}_{\text{feat}}$ and $\bm{\alpha}_{\text{drug}}$. This is achieved 
  by appending both
  $\mathbf{C}$ and $\mathbf{Z}$ with a column of 
  ones. These intercepts are roughly interpreted as the
  expected prior precisions $\E(\gamma_{jd}^{-2})$ and $\E(\tau_d^{-2})$
  if the feature and drug covariates are all zero. 
  Likewise, an $\alpha$ corresponding to an external covariate may be
  interpreted as an additive effect of the external covariate on the prior
  expected precision. So an $\alpha=1$ translates to an increase in expected 
  prior precision of
  1 for every increase in the external covariate of 1,
	keeping all the other external covariates fixed.
  
  Variational Bayes approximations are known to underestimate posterior 
  variances \cite[]{rue_approximate_2009,consonni_mean-field_2007,
  bishop_pattern_2006,wang_inadequacy_2005}. In simulation Scenario 5 in Section 
  \ref{sm-sec:simulations} of the SM, we compare the variational posterior to 
  MCMC samples from the posterior with fixed hyperparameters
  estimates (after the procedure described in Section \ref{sec:empiricalbayes}
  has converged). In this simulation Scenario and other settings (not 
  shown), the variational approximation to the posterior is accurate. If 
  however, the
  user is reluctant to trust the variational posterior variances, samples
  from the posterior may be generated with the
  Gibbs sampler in SM Section \ref{sm-sec:gibbssampler}. Alternatively, 
  we provide an
  implementation of the proposed model in stan using the R package rstan 
  \cite[]{guo_rstan:_2018} at 
  \url{https://github.com/magnusmunch/NIG}.
	
	\section{Simulations}\label{sec:simulations}
	\subsection{Setup}
	This section investigates the empirical 
	Bayes estimation properties of the model in a simulated setting; its main aim 
	is to assess hyperparameter estimation. It is a data based simulation,
	wherein the responses are simulated from a synthetic model, but the
	features are taken from the real GDSC expression data introducted in Section 
	\ref{sec:gdsc}. The real GDSC features contain strong collinearities. Such
	strong collinearities in the design matrix impede correct parameter estimation
	with small sample sizes. 
	We therefore replace the ambition of correctly estimating the $\beta_{jd}$
	with the more modest aim of approximately correct estimation of the 
	hyperparameters.
	
	A pre-processing step selects 100 features with largest variance, while
	251 drug sensitivities for 507 cell lines (half of the total number of cell
	lines) are 
	simulated from model (\ref{eq:likelihood}) and 
	(\ref{eq:prior}). We draw the error
	variances as $\forall d: \sigma_d^2 \sim \Gamma^{-1}(3,2)$,
	such that the prior $\sigma_d^2$ mean and variance are both one. 
	We consider the following four scenarios for the simulation of the 
	drug and feature variance components:
	\begin{itemize}
	  \item{Scenario 1 fixes $\forall d: \tau_d^2=1$ and draws the 
	    $\gamma_{jd}^2$ according to model (\ref{eq:prior}). We
	    create four external dummy feature covariates that code for four 
	    approximately equally sized groups of features. We set
	    $\bm{\alpha}_{\text{feat}}$ such that the $\gamma_{jd}^2$ of the four
	    groups of features have prior means $\phi_{jd} \in \{ 1, 1/2, 1/4, 1/8 \}$
	    ($\bm{\alpha}_{\text{feat}}=\begin{bmatrix} 1 & 1 & 3 & 7 \end{bmatrix} 
	    \tr$). The prior scale parameter is set to $\lambda_{\text{feat}}=1$}.
	  \item{Scenario 2 fixes $\forall j,d: \gamma_{jd}^2=1$ and draws the 
	    $\tau_d^2$ according to model (\ref{eq:prior}), following a procedure
	    similar to the procedure for the $\gamma_{jd}^2$ in Scenario 1: we 
	    create four groups of drugs with corresponding external drug 
	    dummy variables and set $\bm{\alpha}_{\text{drug}}=\begin{bmatrix} 
	    1 & 1 & 3 & 7 \end{bmatrix} \tr$, such that we have $\chi_{d} \in 
	    \{ 1, 1/2, 1/4, 1/8 \}$. The scale is set to $\lambda_{\text{drug}}=1$.}
	  \item{Scenario 3 combines the procedures from Scenarios 1 and 2 to draw both
	    the $\gamma_{jd}^2$ and $\tau_d^2$ according to (\ref{eq:prior}).}
	  \item{Scenario 4 is equal to Scenario three, except that we add noise to
	    the external covariates. Noise is supposed to mimic a low external 
	    covariate signal and is constructed by permutation of fractions 
	    $q \in \{0.1, 0.2, 0.33, 0.5, 0.67, 0.8, 1\}$ of the 
	    rows of the external covariates.}
	\end{itemize}
	We estimate two models: (i) the NIG model that only includes an intercept in
	the external covariates, called NIG$_{\text{f}}^-$, NIG$_{\text{d}}^-$, or
	NIG$_{\text{f}+\text{d}}^-$, depending on which variance components are 
	estimated (feature, drug, or both in Scenarios 1, 2, and 3/4, respectively),
	and 
	(ii) the NIG model estimated as in Section \ref{sec:estimation} that includes
	all external covariates, called NIG$_{\text{f}}$, NIG$_{\text{d}}$, or
	NIG$_{\text{f}+\text{d}}$, again depending on which variance components are 
	estimated.
	Exclusion of the external covariates as in the NIG$_{\text{f}}^-$, 
	NIG$_{\text{d}}^-$, and NIG$_{\text{f}+\text{d}}^-$ models
	amounts to direct estimation of common expected 
	prior means $\phi$ and/or $\chi$, instead of regression estimates for the
	$\phi_{jd}$ and/or $\chi_d$ as in the 
	NIG$_{\text{f}}$, NIG$_{\text{d}}$, and NIG$_{\text{f}+\text{d}}$ models.
	In the language of \cite{bernardo_shrink_2011} as introduced in 
	Section \ref{sec:introduction}, models NIG$_{\text{f}}$ and NIG$_{\text{d}}$
	may be described as local and global shrinkage rules, respectively, as 
	opposed to the global-local shrinkage models NIG$_{\text{f}+\text{d}}$ and
	NIG$_{\text{f}+\text{d}}^-$. We repeat every simulation Scenario 100 times. 
	
	SM Section \ref{sm-sec:simulations} contains more 
	simulation results for Scenarios 1-4 for
	the NIG model and the (i) frequentist 
	lasso and (ii) ridge models. Additionally, SM Section \ref{sm-sec:simulations} 
	contains a comparison of MCMC and VB posteriors in simulation Scenario 3.
	
	\subsection{Results}
<<simulation_gdsc>>=
# simulation 1
res <- read.table("results/simulations_gdsc_res1.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

true.phi1 <- round(mean(1/(c(1, 1, 3, 7) %*% 
                             t(unname(model.matrix(~ factor(1:4)))))), 3)
est.phi1 <- mean(1/res[rownames(res)=="alphaf0", 1])
se.phi1 <- sd(1/res[rownames(res)=="alphaf0", 1])
phi1 <- paste0(round(est.phi1, 3), " (", round(se.phi1, 3), ")")
est.lambdaf1 <- apply(res[rownames(res)=="lambdaf", c(1, 2)], 2, mean)
se.lambdaf1 <- apply(res[rownames(res)=="lambdaf", c(1, 2)], 2, sd)
lambdaf1 <- paste0(round(est.lambdaf1, 3), " (", round(se.lambdaf1, 3), ")")

# simulation 2
res <- read.table("results/simulations_gdsc_res2.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

true.chi2 <- round(mean(1/(c(1, 1, 3, 7) %*% 
                             t(unname(model.matrix(~ factor(1:4)))))), 3)
est.chi2 <- mean(1/res[rownames(res)=="alphad0", 1])
se.chi2 <- sd(1/res[rownames(res)=="alphad0", 1])
chi2 <- paste0(round(est.chi2, 3), " (", round(se.chi2, 3), ")")
est.lambdad2 <- apply(res[rownames(res)=="lambdad", c(1, 2)], 2, mean)
se.lambdad2 <- apply(res[rownames(res)=="lambdad", c(1, 2)], 2, sd)
lambdad2 <- paste0(round(est.lambdad2, 3), " (", round(se.lambdad2, 3), ")")

# simulation 3
res <- read.table("results/simulations_gdsc_res3.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

true.phi3 <- round(mean(1/(c(1, 1, 3, 7) %*% 
                             t(unname(model.matrix(~ factor(1:4)))))), 3)
est.phi3 <- mean(1/res[rownames(res)=="alphaf0", 1])
se.phi3 <- sd(1/res[rownames(res)=="alphaf0", 1])
phi3 <- paste0(round(est.phi3, 3), " (", round(se.phi3, 3), ")")
est.lambdaf3 <- apply(res[rownames(res)=="lambdaf", c(1, 2)], 2, mean)
se.lambdaf3 <- apply(res[rownames(res)=="lambdaf", c(1, 2)], 2, sd)
lambdaf3 <- paste0(round(est.lambdaf3, 3), " (", round(se.lambdaf3, 3), ")")

true.chi3 <- round(mean(1/(c(1, 1, 3, 7) %*% 
                             t(unname(model.matrix(~ factor(1:4)))))), 3)
est.chi3 <- mean(1/res[rownames(res)=="alphad0", 1])
se.chi3 <- sd(1/res[rownames(res)=="alphad0", 1])
chi3 <- paste0(round(est.chi3, 3), " (", round(se.chi3, 3), ")")
est.lambdad3 <- apply(res[rownames(res)=="lambdad", c(1, 2)], 2, mean)
se.lambdad3 <- apply(res[rownames(res)=="lambdad", c(1, 2)], 2, sd)
lambdad3 <- paste0(round(est.lambdad3, 3), " (", round(se.lambdad3, 3), ")")

est.phi.ratio3 <- apply(apply(sapply(0:3, function(s) {
  res[rownames(res)==paste0("alphaf" ,s), 2]}) %*%
    t(cbind(1, rbind(0, diag(3)))), 2, function(m) {
      res[rownames(res)=="alphaf0", 2]/m}), 2, mean)
se.phi.ratio3 <- apply(apply(sapply(0:3, function(s) {
  res[rownames(res)==paste0("alphaf" ,s), 2]}) %*%
    t(cbind(1, rbind(0, diag(3)))), 2, function(m) {
      res[rownames(res)=="alphaf0", 2]/m}), 2, sd)
phi.ratio3 <- paste0(round(est.phi.ratio3, 3), " (", 
                     round(se.phi.ratio3, 3), ")")
est.chi.ratio3 <- apply(apply(sapply(0:3, function(s) {
  res[rownames(res)==paste0("alphad" ,s), 2]}) %*%
    t(cbind(1, rbind(0, diag(3)))), 2, function(m) {
      res[rownames(res)=="alphad0", 2]/m}), 2, mean)
se.chi.ratio3 <- apply(apply(sapply(0:3, function(s) {
  res[rownames(res)==paste0("alphad" ,s), 2]}) %*%
    t(cbind(1, rbind(0, diag(3)))), 2, function(m) {
      res[rownames(res)=="alphad0", 2]/m}), 2, sd)
chi.ratio3 <- paste0(round(est.chi.ratio3, 3), " (", 
                     round(se.chi.ratio3, 3), ")")
true.ratio3 <- as.numeric(1/(c(1, 1, 3, 7) %*%
                               t(unname(model.matrix(~ factor(1:4))))))

est.varb3 <- mean(1/(res[rownames(res)=="alphad0", 1]*
                       res[rownames(res)=="alphaf0", 1]))
se.varb3 <- sd(1/(res[rownames(res)=="alphad0", 1]*
                    res[rownames(res)=="alphaf0", 1]))
varb3 <- paste0(round(est.varb3, 3), " (", round(se.varb3, 3), ")")
true.varb3 <- round(mean(
  outer(1/as.numeric(c(1, 1, 3, 7) %*% t(unname(model.matrix(~ factor(1:4))))),
        1/as.numeric(c(1, 1, 3, 7) %*% 
                       t(unname(model.matrix(~ factor(1:4))))))), 3)

est.kurtb3 <- 3*mean(
  1/(res[rownames(res)=="alphaf0", 1]*res[rownames(res)=="lambdaf", 1])*
    1/(res[rownames(res)=="alphad0", 1]*res[rownames(res)=="lambdad", 1]) + 
    1/(res[rownames(res)=="alphaf0", 1]*res[rownames(res)=="lambdaf", 1]) +
    1/(res[rownames(res)=="alphad0", 1]*res[rownames(res)=="lambdad", 1])) + 3
se.kurtb3 <- 3*sd(
  1/(res[rownames(res)=="alphaf0", 1]*res[rownames(res)=="lambdaf", 1])*
    1/(res[rownames(res)=="alphad0", 1]*res[rownames(res)=="lambdad", 1]) + 
    1/(res[rownames(res)=="alphaf0", 1]*res[rownames(res)=="lambdaf", 1]) +
    1/(res[rownames(res)=="alphad0", 1]*res[rownames(res)=="lambdad", 1])) + 3
kurtb3 <- paste0(round(est.kurtb3, 3), " (", round(se.kurtb3, 3), ")")
true.kurtb3 <- 3*mean(as.numeric(outer(
  as.numeric(1/(c(1, 1, 3, 7) %*% t(cbind(1, rbind(0, diag(3)))))),
  as.numeric(1/(c(1, 1, 3, 7) %*% t(cbind(1, rbind(0, diag(3)))))), 
  FUN=function(X, Y) {X*Y + X + Y}))) + 3
@	
  Figure \ref{fig:simulations_gdsc_est1} shows the estimated
	$\bm{\alpha}_{\text{feat}}$ together with its true value for NIG$_{\text{f}}$ 
	in Scenario 1 of the simulation study (fixed $\tau_d^2$). Figure 
	\ref{fig:simulations_gdsc_est1}a shows that estimation of
	$\bm{\alpha}_{\text{feat}}$ is accurate. This results in accurate estimates on
	the $\phi_{jd}$ scale as well (Figure \ref{fig:simulations_gdsc_est1}b). 
	Model NIG$_{\text{f}}^-$ (that excludes the external covariates) gives a 
	mean $\phi$ estimate of \Sexpr{phi1} (standard deviation between parentheses),
	about equal to the true mean of the $\phi_{jd}$, 
	\Sexpr{true.phi1}. Scale $\lambda_{\text{feat}}=1$ is overestimated by
	NIG$_{\text{f}}$ at \Sexpr{lambdaf1[2]}, while NIG$_{\text{f}}^-$
	underestimates at \Sexpr{lambdaf1[1]}.
<<simulations_gdsc_est1, fig.cap="Simulation results for Scenario 1 ($\\tau^2_d$ fixed): estimated and true values for (a) $\\alpha_{\\text{feat}}$ and (b) prior means $\\phi_{jd}$.", out.width="100%", fig.asp=1/2>>= 
@

  Figure \ref{fig:simulations_gdsc_est2}a shows the accurately estimated
	$\bm{\alpha}_{\text{drug}}$ together with the true value for NIG$_{\text{d}}$ 
	in Scenario 2 of the simulation study (fixed $\gamma_{jd}^2$). Likewise, the
	estimates are accurate on the $\chi_d$ scale 
	(Figure \ref{fig:simulations_gdsc_est2}b). The mean $\chi$ estimate in the 
	NIG$_{\text{d}}^-$ model is \Sexpr{chi2},  
	which is about equal to the true mean \Sexpr{true.chi2}. Scale 
	$\lambda_{\text{drug}}=1$ is overestimated by
	NIG$_{\text{d}}$ at \Sexpr{lambdad2[2]} and underestimated by 
	NIG$_{\text{d}}^-$ at \Sexpr{lambdad2[1]}.
<<simulations_gdsc_est2, fig.cap="Simulation results for Scenario 2 ($\\gamma^2_{jd}$ fixed): estimated (boxplots) and true values (triangles) for (a) $\\alpha_{\\text{feat}}$ and (b) prior means $\\phi_{jd}$.", out.width="100%", fig.asp=1/2>>= 
@

  In Figure \ref{fig:simulations_gdsc_est3} the 
	$\bm{\alpha}_{\text{feat}}$ and $\bm{\alpha}_{\text{drug}}$ estimated by 
	NIG$_{\text{f}+\text{d}}$
	are displayed together with their true values for
	simulation Scenario 3. $\bm{\alpha}_{\text{feat}}$ are underestimated 
	(Figure \ref{fig:simulations_gdsc_est3}a), while 
	$\bm{\alpha}_{\text{drug}}$ (Figure \ref{fig:simulations_gdsc_est3}c) are 
	overestimated, resulting in overestimated
	$\phi_{jd}$ (Figure \ref{fig:simulations_gdsc_est3}b) and 
	underestimated $\chi_{d}$ (Figure \ref{fig:simulations_gdsc_est3}d), 
	respectively . The biases seem to 
	be consistent though. The mean ratios $\phi_1$ to $\phi_2$, $\phi_3$, 
	$\phi_4$ are \Sexpr{phi.ratio3[-1]}, while the mean ratios 
	$\chi_1$ to $\chi_2$, $\chi_3$, $\chi_4$ are
	\Sexpr{chi.ratio3[-1]}. In both cases the true values are
	\Sexpr{true.ratio3[-1]}, so in a relative sense, the  
	$\bm{\alpha}_{\text{feat}}$ and  
	$\bm{\alpha}_{\text{drug}}$ estimates are about correct.
	Moreoever, overestimation of the $\phi_{jd}$ is compensated for by the 
	underestimation of the $\chi_{d}$: the estimated mean prior variances 
	$\V(\beta_{jd})=\phi_{jd} \cdot \chi_{d}$ (ignoring the error variance) are 
	unbiased (Figure \ref{fig:simulations_gdsc_var}).
	The NIG$_{\text{f}+\text{d}}^-$ model is also consistently over- 
	and underestimating $\phi$
	and $\chi$ with mean estimates \Sexpr{phi3} and 
	\Sexpr{chi3}, respectively (compared to the true mean 
	\Sexpr{true.chi3}). Again, on the $\V(\beta_{jd})$ level, this bias almost 
	vanishes; the mean estimated $\V(\beta_{jd})$ (ignoring error variance) are 
	\Sexpr{varb3} while their true mean is \Sexpr{true.varb3}.
	In Scenario 3, NIG$_{\text{f}+\text{d}}$ overestimates 
	$\lambda_{\text{feat}}=1$ at \Sexpr{lambdaf3[2]} and underestimates
	$\lambda_{\text{drug}}=1$ at \Sexpr{lambdad3[2]}. 
	Similar results hold for NIG$_{\text{f}+\text{d}}^-$ with
	$\lambda_{\text{feat}}$ estimate \Sexpr{lambdaf3[1]} and 
	$\lambda_{\text{drug}}$ estimate \Sexpr{lambdad3[1]}. 
<<simulations_gdsc_est3, fig.cap="Simulation results for Scenario 3: estimated (boxplots) and true values (triangles) for (a) $\\alpha_{\\text{feat}}$, (b) prior means $\\phi_{jd}$, (c) $\\alpha_{\\text{drug}}$, and (d) prior means $\\chi_{d}$.", out.width="100%", fig.asp=1>>= 
@
<<simulations_gdsc_var, fig.cap="Simulation results for Scenario 3: mean estimated prior variances $\\V(\\beta_{jd})$ versus true values, with line of identity (dotted).", out.width="50%", fig.asp=1>>= 
@

  Figure \ref{fig:simulations_gdsc_est4} displays the mean $\phi_{jd}$ and 
  $\chi_d$ estimates for different noise levels in simulation Scenario 4. 
  The simulation shows that with increasing noise level, the estimated 
  prior means $\phi_{jd}$ and $\chi_d$ for the four groups of external 
  covariates become more and more alike. In other words, noise in the external
  covariates impedes estimation of $\bm{\alpha}_{\text{feat}}$ and
  $\bm{\alpha}_{\text{drug}}$, as expected.
<<simulations_gdsc_est4, fig.cap="Simulation results for Scenario 3: mean estimated prior means (a) $\\phi_{jd}$, and (b) $\\chi_{d}$ for different levels of noise in the external covariates.", out.width="100%", fig.asp=1/2>>= 
@

  To summarise, estimation of only $\bm{\alpha}_{\text{feat}}$ or
  $\bm{\alpha}_{\text{drug}}$ (and consequently $\phi_{jd}$ and $\chi_{d}$)
  by NIG$_{\text{f}}$ and NIG$_{\text{d}}$, respectively is relatively unbiased,
  as evident from simulation Scenarios 1 and 2 (Figures 
  \ref{fig:simulations_gdsc_est1} and \ref{fig:simulations_gdsc_est2}). 
  In contrast, simultaneous 
  estimation in Scenario 3 results in overestimated $\phi_{jd}$ and 
  underestimated $\chi_{d}$ by NIG$_{\text{f}+\text{d}}$
  (Figure \ref{fig:simulations_gdsc_est3}). We conjecture that this interplay
  of drug and feature variance components is due to near-unidentifiability.e
  r, the resultconsequences are limited, since theiances on the $\beta_{jd}$ level are unbiasleft ed 
  (Figure \ref{fig:simulations_gdsc_var}). If separately estimated, scale 
  parameters $\lambda_{\text{feat}}$ and $\lambda_{\text{drug}}$ are 
  overestimated by NIG$_{\text{f}}$ and NIG$_{\text{d}}$, respectively.
  If estimated simultaneously NIG$_{\text{f}+\text{d}}$ overestimates 
  $\lambda_{\text{feat}}$ and underestimated $\lambda_{\text{drug}}$. On the
  $\beta_{jd}$ level, $\bm{\lambda}$ influences kurtoses 
  $\mathcal{K}(\beta_{jd})$. Figure \ref{fig:simulations_gdsc_kurt} shows the
  mean estimated kurtoses versus their true values. Kurtoses seem to be
  underestimated by NIG$_{\text{f}+\text{d}}$. In contrast, 
  NIG$_{\text{f}+\text{d}}^{-1}$ overestimates the true mean of the
  $\mathcal{K}(\beta_{jd})$, \Sexpr{true.kurtb3}, at \Sexpr{kurtb3}.
  Lastly, the results from Scenario 3 in SM Section \ref{sm-sec:simulations} 
  show that the VB approximation is quite good as compared to standard MCMC.
<<simulations_gdsc_kurt, fig.cap="Simulation results for Scenario 3: mean estimated prior variances $\\V(\\beta_{jd})$ versus true values, with line of identity (dotted).", out.width="50%", fig.asp=1>>= 
@
	
	\section{GDSC data}\label{sec:gdsc}
	\subsection{Primary data}
	The GDSC project's \cite[]{yang_genomics_2013} aim is ``to improve cancer 
	treatments by discovering therapeutic biomarkers that can be used to identify 
	patients most likely to respond to anticancer drug". Part of the project is to
	screen $>1000$ human cancer cell lines for drug sensitivities. The cell lines
	have been genetically characterised and several drug sensitivity measures are
	recorded.
	The data is freely available from \cite{garnett_systematic_2012} and consist 
	of: (i) the sensitivity measures of the cell lines 
	to the drugs, (ii) annotation of the 
	screened compounds, and (iii) the cell lines' genomic profile (mutations, 
	copy numbers, methylation profiles, and gene expression). We will attempt to 
	predict drug sensitivities of the cell lines, as quantified by half maximal
	inhibitory concentration (IC50), using the
	gene expression and gene mutation data. Other choices of sensitivity measures
	than IC50 are 
	possible, but a discussion on the pros and cons of different sensitivity
	measures is beyond the aim of this paper. We have used the version of the 
	data that is presented in \cite{iorio_landscape_2016}.
	We averaged repeated measures over cell line-drug combinations and model the
	logarithm of the IC50 values. In the following, IC50 refers 
	to these log-transformed values. 
	After removing all cell lines with missing values, we end up with 388 to 1043
	IC50 estimates for 251 drugs. Differences in the number of cell lines
	between drugs occur, because not all drug and cell line combinations are 
	available. The pre-processed expression and mutation data consist of
	17737 and 300 genes, respectively.
	% 
	% A comprehensive penalized regression
	% approach such as in 
	% {\cite{mai_composite_2019} uses most of the available molecular markers 
	% instead of just gene expression (68 mutations, 426 copy numbers, 
	% and 2602 gene expression levels for 498 cell lines and 97 drugs). The 
	% current version of our software is not able to handle such problem sizes,
	% due to the repeated calculation of posterior parameters in the empirical
	% Bayes iterations. Currently, problems with number of outcomes in the hundreds
	% and number of features and samples up to a thousand are computationally 
	% feasible.

	\subsection{External data}
	Two ternary drug covariates are available: the developmental stage
	(experimental, in clinical development, or clinically approved) of the
	drugs and the action (unkown, cytotoxic, or targeted) of the drugs. 
	These drug covariates are taken 
	directly from the GDSC database's annotation file and dummy-coded
	with reference categories clinically approved drugs and cytotoxic drugs. 
	We expect that drugs that have been clinically approved are easiest to 
	predict and hence yield the largest prior $\beta_{jd}$ variances, followed by
	the drugs in clinical development, and the experimental drugs. Likewise we 
	expect the targeted drugs to yield the largest prior $\beta_{jd}$ variances, 
	followed by the cytotoxic drugs, and the unkown target drugs. Note that large
	$\beta_{jd}$ variances translate to large
	prior $\gamma_{jd}^2$ and $\tau_d^2$ means.
	
	Furthermore, we have a binary feature covariate available that indicates
	whether a gene belongs to the drug target pathway. The feature covariate
	was created by comparing the target pathways in the GDSC annotation to
	the KEGG \cite[]{kanehisa_kegg:_2000} and reactome 
	\cite[]{fabregat_reactome_2018} repositories. The reference category here is
	features that are not in the target pathway. For this external covariate,
	we expect that genes that are in the pathway of the drug
	are more predictive than genes that are not, i.e., 
	they have larger prior $\beta_{jd}$ variances than drugs that are not in 
	the pathway. 
	
	The type of molecular marker may be included as external 
	covariate, i.e., whether the feature is a gene expression or gene mutation.
	As an alternative to direct inclusion of the mutation data, we use $p$-values
	from the mutations as external covariate. These were obtained from a $t$-test 
	comparing IC50 values of mutated and unmutated genes. We expect that lower 
	mutation $p$-values result in a larger prior $\beta_{jd}$ variances.
	
	Lastly, $p$-values from an analysis of the CCLE data 
	\cite[]{li_landscape_2019}, a database similar to the GDSC, are included as
	external covariate. These $p$-values are obtained from a simple correlation 
	between the IC50 values and the gene expressions from the CCLE data. 
	The harmonic mean per gene is then used as external covariate for the GDSC
	data analysis. Again, we expect a positive relation between these external
	$p$-values and the larger prior $\beta_{jd}$ variances.
	
	\subsection{Analyses}\label{sec:analyses}
	Four analyses were conducted: 
	\begin{itemize}
	  \item{Analysis 1 includes gene expressions as predictors and $p$-values from
	    the gene mutation data as external covariate ($G=1$, $H=0$). 
	    A pre-processing step selects between 221 and 280 genes per drug for 
	    which both the expression as well as a mutation $p$-value is available.}
	  \item{Analysis 2 includes both gene expressions and mutations as predictors
	    with the feature type as external covariate, i.e., wether the feature
	    is a expression or mutation ($G=1$, $H=0$). For this analysis,
	    we pre-select 300 gene expressions with maximum 
	    variance and 295 gene mutations for which there are both mutated and
	    wildtype cell lines available.}
	  \item{Analysis 3 uses 500 gene expressions as features, selected based on
	    maximum variance. The CCLE $p$-values are included as external 
	    covariates ($G=1$, $H=0$).}
	  \item{Analysis 4 includes the gene expressions as features and both the
	    annoted drug variables and pathway status of the genes as external 
	    covariates ($G=1$, $H=2$, before dummy coding). 
	    We pre-select 500 genes expressions based on maximum variance.}
	\end{itemize}
	
	In all analyses we estimated the same models as in the simulations (Section 
	\ref{sec:simulations} and SM Section \ref{sm-sec:simulations}):
	(i) NIG$_{\text{f}+\text{d}}^-$, (ii) NIG$_{\text{f}+\text{d}}$, and
	(iii) frequentist lasso and (iv) ridge models. 
	
	In all analyses we use all cell lines to estimate the hyperparameters 
	presented in Section \ref{sec:results}. Mean prediction means squared errors
	(PMSE) and its standard error are estimated by 10-fold cross validation, where
	$\text{PMSE} = D^{-1} n^{-1} \sum_{d=1}^D 
  \sum_{i=1}^n (\mathbf{y}_d - \mathbf{x}_i \tr \hat{\bm{\beta}_d})^2$, with
  $\hat{\bm{\beta}_d}$ the estimator for $\bm{\beta}_d$. In the NIG
  model, that provides full posteriors, the posterior mean 
  $\E(\bm{\beta}_d | \mathbf{y}_d)$ is used as point estimate.
	
	\subsection{Results}\label{sec:results}
<<analysis_gdsc>>==
# estimates tables
fit1 <- read.table("results/analysis_gdsc_fit1.txt", row.names=NULL)
temp <- fit1[, 1]
fit1 <- as.matrix(fit1[, -1])
rownames(fit1) <- temp

tab1 <- t(fit1)[c(1, 2), c(1:3)]
rownames(tab1) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$")
colnames(tab1) <- c("feature intercept", "$\\log p$-value", "drug intercept")

fit2 <- read.table("results/analysis_gdsc_fit2.txt", row.names=NULL)
temp <- fit2[, 1]
fit2 <- as.matrix(fit2[, -1])
rownames(fit2) <- temp

tab2 <- t(fit2)[c(1, 2), c(1:3)]
rownames(tab2) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$")
colnames(tab2) <- c("feature intercept", "mutation", "drug intercept")

fit3 <- read.table("results/analysis_gdsc_fit3.txt", row.names=NULL)
temp <- fit3[, 1]
fit3 <- as.matrix(fit3[, -1])
rownames(fit3) <- temp

tab3 <- t(fit3)[c(1, 2), c(1:3)]
rownames(tab3) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$")
colnames(tab3) <- c("feature intercept", "$\\log p$-value", "drug intercept")

fit4 <- read.table("results/analysis_gdsc_fit4.txt", row.names=NULL)
temp <- fit4[, 1]
fit4 <- as.matrix(fit4[, -1])
rownames(fit4) <- temp

tab4.1 <- t(fit4)[c(1, 2), c(1:2)]
rownames(tab4.1) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$")
colnames(tab4.1) <- c("feature intercept", "pathway")
tab4.2 <- t(fit4)[c(1, 2), c(3:7)]
rownames(tab4.2) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$")
colnames(tab4.2) <- c("drug intercept", 
                    "experimental", "development", "targeted", "unknown")

# pmse table
res <- read.table("results/analysis_gdsc_res1.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

psel1 <- res[substr(rownames(res), 1, 5)=="psel.", ]
pmse1 <- res[substr(rownames(res), 1, 5)=="pmse.", ]
apmse1 <- aggregate(pmse1, list(rownames(pmse1)), FUN="mean")[, -1]
pmse1 <- aggregate(pmse1, by=list(rep(1:10, each=251)), FUN="mean")[, -1]
mpmse1 <- apply(pmse1, 2, mean)
sdpmse1 <- apply(pmse1, 2, sd)
ppmse1 <- paste0(round(mpmse1, 3), " (", round(sdpmse1, 3), ")")

res <- read.table("results/analysis_gdsc_res2.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

psel2 <- res[substr(rownames(res), 1, 5)=="psel.", ]
pmse2 <- res[substr(rownames(res), 1, 5)=="pmse.", ]
pmse2 <- aggregate(pmse2, by=list(rep(1:10, each=251)), FUN="mean")[, -1]
mpmse2 <- apply(pmse2, 2, mean)
sdpmse2 <- apply(pmse2, 2, sd)
ppmse2 <- paste0(round(mpmse2, 3), " (", round(sdpmse2, 3), ")")

res <- read.table("results/analysis_gdsc_res3.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

psel3 <- res[substr(rownames(res), 1, 5)=="psel.", ]
pmse3 <- res[substr(rownames(res), 1, 5)=="pmse.", ]
pmse3 <- aggregate(pmse3, by=list(rep(1:10, each=251)), FUN="mean")[, -1]
mpmse3 <- apply(pmse3, 2, mean)
sdpmse3 <- apply(pmse3, 2, sd)
ppmse3 <- paste0(round(mpmse3, 3), " (", round(sdpmse3, 3), ")")

res <- read.table("results/analysis_gdsc_res4.txt", row.names=NULL)
temp <- res[, 1]
res <- as.matrix(res[, -1])
rownames(res) <- temp

psel4 <- res[substr(rownames(res), 1, 5)=="psel.", ]
pmse4 <- res[substr(rownames(res), 1, 5)=="pmse.", ]
pmse4 <- aggregate(pmse4, by=list(rep(1:10, each=249)), FUN="mean")[, -1]
mpmse4 <- apply(pmse4, 2, mean)
sdpmse4 <- apply(pmse4, 2, sd)
ppmse4 <- paste0(round(mpmse4, 3), " (", round(sdpmse4, 3), ")")

tab5 <- rbind(ppmse1, ppmse2, ppmse3, ppmse4)
tabm5 <- rbind(mpmse1, mpmse2, mpmse3, mpmse4)
tab5.1 <- tab5[, c(1:4)]
tabm5.1 <- tabm5[, c(1:4)]
tab5.1 <- t(sapply(1:nrow(tab5.1), function(r) {
  id <- which(tabm5.1[r, ]==min(tabm5.1[r, ], na.rm=TRUE))
  out <- tab5.1[r, ]
  out[id] <- paste0("\\textbf{", out[id], "}")
  out}))
rownames(tab5.1) <- paste0("analysis ", 1:nrow(tab5.1))
colnames(tab5.1) <- c("NIG$_{\\text{f}+\\text{d}}^-$", 
                    "NIG$_{\\text{f}+\\text{d}}$", "ridge", "lasso")
tab5.1 <- t(tab5.1)

tab5.2 <- tab5[, c(4, 13:14, 10:12)]
tabm5.2 <- tabm5[, c(4, 13:14, 10:12)]
tab5.2 <- t(sapply(1:nrow(tab5.2), function(r) {
  id <- which(tabm5.2[r, ]==min(tabm5.2[r, ], na.rm=TRUE))
  out <- tab5.2[r, ]
  out[id] <- paste0("\\textbf{", out[id], "}")
  out}))
rownames(tab5.2) <- paste0("analysis ", 1:nrow(tab5.2))
colnames(tab5.2) <- c("lasso($\\sim$50)$^*$", "lasso(25)", "lasso(100)", 
                      "NIG$_{\\text{f}+\\text{d}}+$DSS($\\sim$50)$^*$",
                      "NIG$_{\\text{f}+\\text{d}}+$DSS(25)",
                      "NIG$_{\\text{f}+\\text{d}}+$DSS(100)")
tab5.2 <- t(tab5.2)
tab5.2 <- tab5.2[c(2, 1, 3, 5, 4, 6), ]

mpsel <- rbind(colMeans(psel1), colMeans(psel2), colMeans(psel3), 
               colMeans(psel4))

# prior variances
vars1 <- round(1/as.numeric(model.matrix(~ factor(c(1:3))) %*% 
                              fit4[c(3, 6, 7), 2]), 4)

# pmse ranges
ranges1 <- apply(rbind(mpmse1, mpmse2, mpmse3, mpmse4)[, 1:4], 1, range) 
ranges1 <- round(ranges1[2, ] - ranges1[1, ], 4)
psels1 <- round(mpsel[, c(4, 6, 10)], 0)
@
  The non-zero NIG$_{\text{f}+\text{d}}$ $\bm{\alpha}$ estimates in Tables 
  \ref{tab:analysis_gdsc_fit1}-\ref{tab:analysis_gdsc_fit4.2} show 
  that there is an effect of the
  external covariates. 
  \begin{itemize}
	  \item{Analysis 1 results in a positive additive effect of the mutation 
	    $log p$-values on the prior $\bm{\beta}_d$ precisions
	    (Table \ref{tab:analysis_gdsc_fit1}). More precisely, with doubling
	    $p$-value, the prior $\bm{\beta}_d$ precision increases by 
	    $\log(2)*\Sexpr{round(tab1[2, 2], 3)}$=\Sexpr{
	    round(log(2)*tab1[2, 2], 3)}.
	    This translates to more $\bm{\beta}_d$ shrinkage with increasing 
	    $p$-value, as expected.}
	  \item{Analysis 2 shows that gene mutations are more predictive 
	    than gene expressions, as observed from the negative effect of mutation
	    dummy on prior precisions (Table \ref{tab:analysis_gdsc_fit2}): 
	    mutations are shrunken less than
	    expressions.}
	  \item{Analysis 3 indicates that CCLE $\log p$-values are positively related
	    to prior precision (Table \ref{tab:analysis_gdsc_fit3}). In particular
	    if the $p$-value doubles, the prior $\bm{\beta}_d$ precision increases by 
	    $\log(2)*\Sexpr{round(tab3[2, 2], 3)}$=\Sexpr{
	    round(log(2)*tab1[2, 2], 3)}, i.e., higher CCLE $p$-values results in 
	    more shrinkage of $\bm{\beta}_d$, as expected.}
	  \item{Analysis 4 gives a negative effect for the pathway dummy (Table 
	    \ref{tab:analysis_gdsc_fit4.1}), indicating less shrinkage for genes
	    that are in the drugs' target pathway, as expected. According to 
	    expectation, experimental and 
	    developmental drugs prior precisions are shrunken more than
	    the reference category, approved drugs (Table 
	    \ref{tab:analysis_gdsc_fit4.2}). Somewhat surprisingly, targeted and 
	    unkown drugs are shrunken more than the reference, cytotoxic drugs (Table 
	    \ref{tab:analysis_gdsc_fit4.2}), although the differences on the
	    $\bm{\beta}_d$ variance scale are small:
	    $\hat{\E}(\tau_{d}^{2}) = \{ \Sexpr{vars1[1]}, \Sexpr{vars1[2]}, 
	    \Sexpr{vars1[3]} \}$ for cytotoxic, targeted, and unkown drugs, 
	    respectively (ignoring other variance components).}
	\end{itemize}
<<analysis_gdsc_fit1>>=
kableExtra::kable_styling(knitr::kable(
  tab1, align="r", digits=3,
  caption="$\\bm{\\alpha}$ estimates from analysis 1. Empty cells correspond to fixed zero parameters.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@
<<analysis_gdsc_fit2>>=
kableExtra::kable_styling(knitr::kable(
  tab2, align="r", digits=3,
  caption="$\\bm{\\alpha}$ estimates from analysis 2. Empty cells correspond to fixed zero parameters.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@
<<analysis_gdsc_fit3>>=
kableExtra::kable_styling(knitr::kable(
  tab3, align="r", digits=3,
  caption="$\\bm{\\alpha}$ estimates from analysis 3. Empty cells correspond to fixed zero parameters.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@
  \begin{table}[h!]\label{tab:analysis_gdsc_fit4}
    \begin{minipage}{1\textwidth}
<<analysis_gdsc_fit4.1>>=
kableExtra::kable_styling(knitr::kable(
  tab4.1, align="r", digits=3,
  caption="$\\bm{\\alpha}$ estimates from analysis 4. Empty cells correspond to fixed zero parameters.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@
    \end{minipage}
    \begin{minipage}{1\textwidth}
<<analysis_gdsc_fit4.2>>=
kableExtra::kable_styling(knitr::kable(
  tab4.2, align="r", digits=3,
  caption="$\\bm{\\alpha}$ estimates from analysis 4. Empty cells correspond to fixed zero parameters \\textit{(continued)}.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@
    \end{minipage}
  \end{table}

  The mean prediction mean squared errors (PMSE), calculated on the 
  test data, are diplayed in Table \ref{tab:analysis_gdsc_pmse1}.
  Note that due to standardisation, an empty reference model has a PMSE of one.
  In terms of PMSE, ridge outperforms the other models in all Analyses 1, 3, and
  4, while NIG$_{\text{f}+\text{d}}$ $\bm{\alpha}$ performs best in Analysis 2. 
  In general however, all models perform very similar. The ranges of mean PMSE 
  over all methods are
  \Sexpr{ranges1[1:3]}, and \Sexpr{ranges1[4]}, for Analyses 1-4, respectively,
  indicating that the differences in predictive performance are very small.
  Furthermore, the difference between NIG$_{\text{f}+\text{d}}$, that includes
  external covariates, and NIG$_{\text{f}+\text{d}}^-$,
  that excludes the external covariates is small; an indication that the
  external covariates are not very informative here.
<<analysis_gdsc_pmse1>>=  
kableExtra::kable_styling(knitr::kable(
  tab5.1, align="r",
  caption="Mean (standard deviation) of cross-validated PMSE for GDSC data. Best performing model (per analysis) in bold.",
  format="latex", booktabs=TRUE, escape=FALSE), 
  latex_options=c("HOLD_position"))
@
  
  Part of the differences in performance between
  NIG and ridge may be explained with the different levels of 
  sparsity in the solution. Although NIG does not automatically select features, 
  as opposed to the lasso,
  its $\bm{\beta}_d$ prior has larger kurtosis than the ridge prior (see SM
  Section \ref{sm-sec:prior}). The resulting heavy-tailedness as compared to 
  the ridge prior facilitates features selection. 
  Selection of features from a sparse prior
  may be achieved through the decoupling shrinkage and selection approach (DSS)
  introduced in \cite{hahn_decoupling_2015}. We have applied DSS in our 
  analyses, where we select either (approximately) the same number of features 
  as lasso (about 50 in all analyses), 25, or 100 features. Table 
  \ref{tab:analysis_gdsc_pmse2} compares the resulting mean PMSE values. The
  Table shows that NIG$_{\text{f}+\text{d}}$ plus DSS outperforms lasso,
  for all three numbers of selected features, in all analyses.
  
<<analysis_gdsc_pmse2>>=  
kableExtra::add_footnote(kableExtra::kable_styling(knitr::kable(
  tab5.2, align="r",
  caption="Mean (standard deviation) of cross-validated PMSE for selection methods (number of selected features between parentheses) on GDSC data. Best performing model (per analysis) in bold.",
  format="latex", booktabs=TRUE, escape=FALSE), 
  latex_options=c("HOLD_position")), 
  label="cross validated lasso selects, on average, 42-56 features in all analyses (indicated with $\\sim$50).",
  notation="symbol", threeparttable=TRUE, escape=FALSE)
@
  
  To assess model fit, Section \ref{sm-sec:gdsc} in the SM displays the 
  conditional predictive 
  ordinates (CPO) for the NIG$_{\text{f}+\text{d}}$ model for the four analyses. 
  A visual inspection of the CPOs learns that no extreme outliers occur.
  
	\section{Discussion}\label{sec:discussion}
<<time>>=
times <- round(read.table("results/analysis_gdsc_time1.txt")[, 1]/60, 0)
@
	The preceding presents a novel model for drug sensitivity
	prediction from a set of high dimensional molecular features. The model allows
	for the inclusion of discrete and continuous external covariates on both the 
	drugs and features. Inclusion of the external information is through 
	data-driven and adaptive empirical Bayes estimation of the hyperparameters in 
	the normal inverse Gaussian prior model (\ref{eq:prior}). Variational Bayes
	estimation is efficient and scales well with the number of features and 
	samples. Estimation of NIG$_{\text{f}+\text{d}}$ in the GDSC data analayses
	in Section \ref{sec:gdsc} took 
	\Sexpr{times[-1]}, and \Sexpr{times[1]} minutes on a 2016 
	MacBook Pro with 2 GHz Dual-Core Intel Core i5 processor and 8 GB of memory, 
	running macOS  10.15.1.
	
	Simulation Scenarios 1 and 2
	in Section \ref{sec:simulations} show that estimation of drug- and 
	feature-specific hyperparameters is, in principle, fairly accurate. However, 
	when estimated jointly, biases may occur
	due to the interplay between the two sources of information. Fortunately,
	these biases cancel out on the $\beta_{jd}$ level, such that the prior 
	variance estimates $\V(\beta_{jd})$ are accurate. Ultimately, predicitive
	performance benefits from the inclusion of external covariates, according
	to the results in Section \ref{sm-sec:simulations} of the SM.

	The model is put into practice on the GDSC data in Section \ref{sec:gdsc}. 
	The comparison of NIG$_{\text{f}+\text{d}}$ to 
	NIG$_{\text{f}+\text{d}}^-$ (that excludes the external covariates) shows
	that although the inclusion of external covariates substantially modifies the
	hyperparameters, predictive performance as measured by PMSE is only slightly
	better in one of four analyses. The NIG model is competitive with convential, 
	penalized methods like
	lasso and ridge, but all three methods achieve PMSE of only 0.80 in all 
	analyses, a 20\% reduction compared to the empty model. In three of the four 
	analyses, ridge slightly outperforms
	NIG, which in turn outperforms lasso. In Analysis 2, NIG slightly
	outperforms ridge and lasso. The indications of the above are two-fold: (i) 
	the GDSC data does not contain a lot of signal overall, and (ii)
	the external covariates are not very informative for the GDSC data. We note
	however, that NIG seems to have a small advantage in terms of feature 
	selection. If we follow the DSS approach in \cite{hahn_decoupling_2015} for
	feature selection, PMSE after selection is slightly better than lasso in all
	four analyses.
	
	The penalized regression methods estimate penalty 
  parameters by cross-validation. Cross-validation directly minimises the 
  (approximate) PMSE, as opposed to empirical Bayes in the NIG that maximises 
  the (approximate) marginal likelihood, a measure of model fit. To achieve
  maximal predictive accuracy, direct prediction error optimsation by 
  cross-validation is preferred. However, direct prediction error optimisation 
  by cross-validation is not 
	feasible in the external covariates setting, due to the large number of 
	hyperparameters. A caveat with penalized
  regression methods is that they do not give measures of parameter uncertainty.
  NIG, on the other hand, gives the full posterior of the parameters, either
  through a variational Bayes approximation or with the Gibbs sampler from
  SM Section \ref{sm-sec:gibbssampler}. The full posterior gives direct access
  to the parameter uncertainties for a better interpretable model. In addition, 
  given that the linear predictor is a linear combination of $\beta_{jd}$, and 
  we have access to the approximate multivariate posterior of 
  $\bm{\beta}_d$, NIG also allows to assess uncertainty of the predictions. 
	
	An alternative strategy to include external covariates is the varying 
	coefficient (VC) model \cite[]{hastie_varying-coefficient_1993}. The VC model
	treats the mean of the regression coefficients as a deterministic function of 
	external covariates, as opposed to our probabilistic model for the variance
	of the regression coefficients. \cite{ni_bayesian_2019} introduces a Bayesian 
	VC model where the relation between the regression 
	coefficients and external covariates is no longer deterministic, but still
	based on the mean of the coefficients. Besides our computationally more
	feasible VB-EM estimation, we advocate for a more indirect
	model for the relation between external covariates and regression 
	coefficients, i.e., through their random variance components. This indirect 
	model assumes less structure about the relation between external covariates
	and regression coefficients than the direct VC mean model approach.
  In particular, a VC mean model describes both magnitude and direction of the 
  external covariates effects, while our variance model only describes the
  magnitude and is invariant to the direction of the effects.
  Nonetheless, a combination of the two approaches (both mean and variance 
  modelled as functions of the external covariates) may be a fruitful future 
  research direction.
  Other methods that consider an external covariate model for the variance
  components of the regression coefficients are bSEM
  \cite[]{leday_gene_2017,kpogbezan_empirical_2017} and \texttt{xtune}
  \cite[]{zeng_incorporating_2020}. bSEM is designed to include
  only dichotomous external covariates, so is not applicable in most of the
  applications and simulations that we have considered here. \texttt{xtune}
  allows for the inclusion of multiple external feature covariates, but not on
  the drug level, so also has limited applicability in our simulations and
  applications.
  
  A possible criticism of the NIG model is the treatment of $\bm{\alpha}$ as 
  fixed hyperparameters instead of random. A Bayesian could argue that endowment
  of $\bm{\alpha}$ with a hyperprior results in propagation of 
  uncertainty about $\bm{\alpha}$ and as a result improved regression parameter 
  uncertainty quantification. \cite{van_de_wiel_learning_2019} show in a similar 
  setting that EB estimation of hyperparameters does not necessarily lead to
  worse uncertainty quantification as measured by frequentist coverage of
  Bayesian credible intervals, as compared to a full Bayes treatment of the
  hyperparameters.
	
	Possible directions of future research are applications of the NIG model to 
	different data types. Suitable applications are eQTL studies, 
	in which gene expressions are regressed on SNPs. Several interesting external
	covariates are available, both on the genes as well as the SNPs. An example of
	such an external
	covariate for the genes is genelength, where we suspect that longer genes are
	harder to predict. The distance of the SNP to the gene is an example of an
	external SNP covariate, where the expectation is that SNPs further from the 
	gene are less predictive of that gene's expression.
	
	\section*{Supplementary Material}
	Supplementary Material is available online from 
	\url{https://github.com/magnusmunch/NIG}. 
	
	\section*{Reproducible Research}
	All results and documents may be recreated from 
	\url{https://github.com/magnusmunch/NIG}.
	
	\section*{Acknowledgements}
	We thank the referees for their helpful comments and suggestions on an earlier
	version of this manuscript.
	MM visited SR and GL on a travel grant funded by the Amsterdam Public Health 
	institute's Methodology program.
	\textit{Conflict of Interest}: None declared.
	
	\bibliographystyle{author_short3} 
	\bibliography{refs}
	
	\section*{Session info}

<<r session-info, eval=TRUE, echo=TRUE, tidy=TRUE>>=
devtools::session_info()
@

\end{document}