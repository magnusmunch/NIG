\documentclass[a4paper,hidelinks]{article}
\usepackage{bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,rotating,caption,subcaption,natbib,appendix,titlesec,multicol,hyperref,verbatim,bbm,algorithm,algpseudocode,pgfplotstable,threeparttable, booktabs,mathtools,dsfont,parskip}

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Em}{\mathbf{E}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\Lm}{\mathbf{L}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}

% settings
\graphicspath{{/Users/magnusmunch/Documents/OneDrive/PhD/project_cambridge/graphs/}}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Drug efficacy prediction in cell lines}
\date{\today}
\author{Magnus M. M\"unch$^{1,2}$\footnote{Correspondence to: \href{mailto:m.munch@vumc.nl}{m.munch@vumc.nl}}, Mark A. van de Wiel$^{1,3}$, Sylvia Richardson$^{3}$, \\ and Gwena{\"e}l G. R. Leday$^{3}$}

\begin{document}
	\maketitle
	
	\noindent
	1. Department of Epidemiology \& Biostatistics, Amsterdam Public Health research institute, VU University Medical Center, PO Box 7057, 1007 MB
	Amsterdam, The Netherlands\\
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\
	3. MRC Biostatistics Unit, Cambridge Institute of Public Health, Cambridge, United Kingdom \\
	
	\begin{abstract}
		{...}
	\end{abstract}
	
	\noindent\textbf{Keywords}: 
	
	\noindent\textbf{Software available from}:
	
	\section{Introduction}
	- motivation: drug response prediction from different omics data types is promising both for precision medicine and drug target discovery and relates to actual tumor response \cite[]{iorio_landscape_2016}. 
		
	- problem: prediction of drug efficacy in cell lines is difficult (DREAM 7 challenge in \cite{costello_community_2014}).	

	- example data: introduction (GDSC, CCLE, NCI-60)
	
	- feature information: prior experts, pathways, database summaries, previous experiments, type of feature (somatic mutations, copy number alterations, DNA methylation, and gene expression). 
	
	- drugs information: molecular target, drug class, etc (ask Oslo).
	
	- current literature: review in \cite{azuaje_computational_2017}, reference Oslo, TANDEM \cite[]{aben_tandem:_2016}, DeltaNet \cite[]{noh_inferring_2016}, patient-derived tumor cell lines \cite[]{gao_high-throughput_2015}.
	
	- our solution: adaptive borrowing of information through empirical Bayes, guided by external feature and drug information.
	
	\section{Model}
	\subsection{Simultaneous equations model}\label{sec:SEM}
	- Gwen can deal with different number of features per equation, might solve missing drug-feature combination problem? Doesn't solve missing response-feature combination problem.
	
	- check literature for: a) conjugate version, b) similar models: "prior variance decomposition", "structured Bayesian regression"
	
	We have continuous efficacy measures $y_{id}$ for cell lines $i=1,\dots, n$, from tissues $t=1, \dots, T$, on drugs $d=1,\dots,D$. Throughout this paper we assume the $y_{id}$ to be centred per drug and let $\y_d = \begin{bmatrix} y_{1d} & \dots & y_{nd} \end{bmatrix} \tr$. We predict efficacy with molecular features $x_{ij}$, $j=1,\dots, p$, collected in $\x_i = \begin{bmatrix} x_{i1} & \dots & x_{ip} \end{bmatrix} \tr$. We have an \textit{a priori} partitioning of the omics features into $g=1, \dots, G$ non-overlapping groups, which might be informative for the problem at hand For convenience, we let $g(j)$ denote the group index of molecular feature $j$. Furthermore, we have characteristics of the drugs available in the form of `co-data' $\mathbf{C} = \begin{bmatrix} \mathbf{c}_1 & \dots & \mathbf{c}_D \end{bmatrix} \tr$. We code the tissue from which the cell lines were taken as a $T$ binary dummy variables 
	$$
	z_{it} = \begin{cases}
	1 & \text{if } \text{tissue}_i = t, \\
	0 & \text{otherwise},
	\end{cases}
	$$
	collected in $\mathbf{z}_i = \begin{bmatrix} z_{i1} & \dots & z_{iT} \end{bmatrix} \tr$.
	
	We assume that cell lines from the same tissue are related and model this relation through random effects $u_{td}$. We let the molecular feature effects $\beta_{jd}$ vary over the drugs and model drug efficacy as a linear combination of these two effects:
	\begin{subequations}\label{eq:linearmodel}
		\begin{align}
		y_{id} & = \beta_{0d} + \sum_{j=1}^p x_{ij} \beta_{jd} + \sum_{t=1}^T z_{it} u_{td} + \epsilon_{id} \\
		& = \beta_{0d} + \x_i \tr \bbeta_d + \mathbf{z}_i \tr \mathbf{u}_d + \epsilon_{id},\\
		\text{with } & \epsilon_{id} \sim \mathcal{N}(0, \sigma_d^2),
		\end{align}
	\end{subequations}
	where $p$-dimensional $\bbeta_d$ and $T$-dimensional $\bm{u}_d$ are the drug-specific omics feature and tissue effect vectors, respectively. Note that (\ref{eq:linearmodel}) gives rise to a system of $d=1, \dots, D$ linear simultaneous equations.
	
	\subsection{Bayesian prior model}
	We capture the uncertainty in the parameters through a Bayesian prior model. We assign a flat prior to the intercept terms $\beta_{0d}$ and integrate them out. The remaining parameters are endowed with the following priors:
	\begin{subequations}\label{eq:prior}
		\begin{align}
		\beta_{jd} & \sim \mathcal{N}_p (0, \phi^2_{g(j)} \gamma_d^2 \sigma_d^2), \\
		\mathbf{u}_{d} & \sim \mathcal{N}_T (\mathbf{0}, \bm{\Xi}_d \sigma_d^2),
		\end{align}
	\end{subequations}
	and hyper-priors:
	\begin{subequations}\label{eq:hyperprior}
		\begin{align}
			\sigma_d^{2} & \sim 1/\sigma_d^{3} \\
			\gamma_d^{2} & \sim \mathcal{IG}(\theta_d, \lambda), \\
			\bm{\Xi}_d & \sim \mathcal{W}^{-1}_T(\bm{\Omega}, \nu),
		\end{align}
	\end{subequations}
	where $\mathcal{IG}(\theta, \lambda)$ is an inverse Gaussian distribution with mean $\theta$ and shape $\lambda$, and $\mathcal{W}^{-1}_T(\bm{\Omega}, \nu)$ is an inverse Wishart distribution with $T \times T$-dimensional positive definite (PD) scale matrix $\bm{\Omega}$ and degrees of freedom $\nu$. Note that for $\nu > T + 1$, this inverse Wishart prior implies an expected prior covariance matrix $(\nu - T - 1)^{-1} \bm{\Omega}$ for the tissue effects, common to all drugs. Likewise, the molecular feature effects are related through the common hyper-prior for the $\gamma_d^2$.
	
	- Jeffrey's prior for variance $1/\sigma^{3/2}$ (a priori independent data mean and variance) or $1/\sigma^3$ (joint prior, but only mean and variance, not variance and betas).

	A few remarks on the choice of priors are justified here: many authors endow error variance components with vague gamma priors. \cite{gelman_prior_2006}, among others, advises against this practice. The degree of `vagueness' has a large influence on the posterior, while degree of `vagueness' is a difficult parameter to set. This influence is especially pronounced if the likelihood is relatively flat, as may be reasonably expected in the large $p$, small $n$ setting we are in. We therefore model the error variance with Jeffrey's prior \cite[]{jeffreys_invariant_1946}. 
	
	Furthermore, we choose to model the $\gamma^2_d$ by an inverse Gaussian distribution, as has been suggested in \cite{fabrizi_specification_2016} and \cite{caron_sparse_2008}, because it allows to model the mean $\theta_d$ as a function of the drug covariates $\mathbf{c}_d$, as explained in Section \ref{sec:empiricalbayes}. 
	
	- IG hyperprior used in finance \cite[]{barndorff-nielsen_normal_1997}, where it is called the NIG prior.
%	\begin{subequations}\label{eq:hyperprior}
%		\begin{align}
%		\sigma_d^{2} & \sim 1/\sigma_d^{3} \\
%		\log \gamma_d^{2} & \sim \mathcal{N}(\theta, \tau^2), \\
%		\bm{\Xi}_d & \sim \mathcal{W}^{-1}_T(\Vm, k),
%		\end{align}
%	\end{subequations}
%- try log-normal distribution for the $\gamma_d$'s. This is used in Example 10.4.1 in \cite{lunn_bugs_2012}, with justification same as ours: allows for modelling the variance as a function of covariates. Also used in examples 6.10 and 9.2 in \cite{abrams_bayesian_2004}.

%	\begin{subequations}\label{eq:hyperprior}
%		\begin{align}
%		\sigma_d^{-2} & \sim \Gamma(0.001, 0.001), \\
%		\gamma_d^{-2} & \sim \Gamma(k, \theta), \\
%		\bm{\Xi}_d & \sim \mathcal{W}^{-1}_T(\Vm, \tau),
%		\end{align}
%	\end{subequations}
%	where $g(j)$ denotes the group index of molecular feature $j$, $\Gamma(k, \theta)$ is a gamma distribution with shape $k$ and rate $\theta$, and $\mathcal{W}_T(\Vm, \tau)$ is a Wishart distribution with $T \times T$-dimensional scale matrix $\Vm$ and degrees of freedom $\tau$. Note that this Wishart prior implicates an expected prior covariance matrix $\tau \cdot \Vm$ for the tissue effects, common to all drugs.	
	
	\section{Estimation}
	\subsection{Variational Bayes}\label{sec:variationalbayes}
	The posterior corresponding to the model described in (\ref{eq:linearmodel}), (\ref{eq:prior}), and (\ref{eq:hyperprior}) is not available in closed form. We therefore approximate the posterior by variational Bayes, where we force the posterior density $Q$ to factorise as:
	\begin{align}\label{eq:variationalposterior}
	p(\B,\mathbf{U},\mathbf{\sigma}^2, \bgamma^2,\bm{\Xi}_1, \dots, \bm{\Xi}_D) \approx Q(\cdot) = q(\B) \cdot q(\mathbf{U}) \cdot q(\mathbf{\sigma}^2) \cdot q(\bgamma^2) \cdot q(\bm{\Xi}_1, \dots, \bm{\Xi}_D),
	\end{align}
	where $\B = \begin{bmatrix} \bbeta_1 & \cdots & \bbeta_D \end{bmatrix}$ and $\U = \begin{bmatrix} \mathbf{u}_1 & \cdots & \mathbf{u}_D \end{bmatrix}$, are of dimensions $p \times D$ and $T \times D$, respectively. Under the factorisation in (\ref{eq:variationalposterior}), the marginal variational posteriors that minimise the Kullback-Leibler divergence of the true posterior to the variational Bayes approximation \cite[]{neal_view_1998}, are given by:
	\begin{subequations}\label{eq:variationalposterior2}
		\begin{align}
		q_{\B} (\B) & \overset{D}{=} \prod_{d=1}^{D} \mathcal{N}_p (\bmu_d, \bSigma_d), \\
		q_{\U}(\U) & \overset{D}{=} \prod_{d=1}^{D} \mathcal{N}_T (\mathbf{m}_d, \Sm_d), \\
		q_{\bgamma^{2}}(\bgamma^{2}) & \overset{D}{=} \prod_{d=1}^D \mathcal{GIG} \(-\frac{p+1}{2}, \frac{\lambda}{\theta_d^2}, \delta_{d} \), \\
		q_{\bsigma^{2}}(\bsigma^{2}) & \overset{D}{=} \prod_{d=1}^D \Gamma^{-1} \(\frac{n + p + T + 1}{2}, \zeta_{d} \), \\
		q_{\bm{\Xi}_1, \dots, \bm{\Xi}_D}(\bm{\Xi}_1, \dots, \bm{\Xi}_D) & \overset{D}{\propto} \prod_{d=1}^D \mathcal{W}_T^{-1} (\bm{\Psi}_d, \nu + 1).
		\end{align}
	\end{subequations}
	Here $\mathcal{GIG}(\cdot)$ and $\Gamma^{-1}(\cdot)$ denote the generalized inverse Gaussian and inverse gamma distributions, respectively. The full derivations may be found in Appendix \ref{app:derivations}.	
	
	The parameters in (\ref{eq:variationalposterior2}) contain cyclic dependencies. We therefore iterate the following estimating equations until convergence to a local optimum:
	\begin{align*}
	\bSigma_d^{(h+1)} & = \frac{2\zeta_d^{(h)}}{n+p+T+1} \left\{\X \tr \X + a_d^{(h)} \cdot \diag (\phi_{g(j)}^{-2}) \right\}^{-1}, \\
	\bm{\mu}_d^{(h+1)} & = \left\{\X \tr \X + a_d^{(h)} \cdot \diag (\phi_{g(j)}^{-2}) \right\}^{-1} \X \tr ( \y_d - \Z \mathbf{m}_d^{(h)}), \\
	\Sm_d^{(h+1)} & = \frac{2\zeta_d^{(h)}}{n+p+T+1} \[ \Z \tr \Z + (\nu + 1) \cdot (\bm{\Psi}_d^{(h)})^{-1} \]^{-1}, \\
	\mathbf{m}_d^{(h+1)} & = \[ \Z \tr \Z + (\nu + 1) \cdot (\bm{\Psi}_d^{(h)})^{-1} \]^{-1} \Z \tr (\y_d - \X \bm{\mu}_d^{(h)}), \\
	\bm{\Psi}_d^{(h+1)} & = \frac{2\zeta_d^{(h)}}{n+p+T+1} \[ \Sm_d^{(h+1)} + \mathbf{m}_d^{(h+1)} (\mathbf{m}_d^{(h+1)}) \tr \] + \bm{\Omega}, \\
	\delta_d^{(h+1)} & =  \frac{n+p+T+1}{2\zeta_d^{(h)}} \sum_{j=1}^p \frac{(\mu_{jd}^{(h+1)})^2 + (\bSigma_d^{(h+1)})_{jj}}{\phi^2_{g(j)}} + \lambda, \\
	\zeta_d^{(h+1)} & = \frac{1}{2} \y_d \tr \y_d - \y_d \tr (\X \bmu_d^{(h+1)} + \Z \mathbf{m}_d^{(h+1)}) + (\bmu_d^{(h+1)}) \tr \X \tr \Z \mathbf{m}_d^{(h+1)} + \frac{1}{2} \trace (\X \tr \X \bSigma_d^{(h+1)}) \\
	& \,\,\,\,\,\,\,\,\,\, + \frac{1}{2} (\bmu_d^{(h+1)}) \tr \X \tr \X \bmu_d^{(h+1)} + \frac{1}{2} \trace (\Z \tr \Z \Sm_d^{(h+1)}) + \frac{1}{2} (\mathbf{m}_d^{(h+1)}) \tr \Z \tr \Z \mathbf{m}_d^{(h+1)} \\
	& \,\,\,\,\,\,\,\,\,\, + \frac{1}{2} a_d^{(h)} \sum_{j=1}^p \frac{(\mu_{jd}^{(h+1)})^2 + (\bSigma_d^{(h+1)})_{jj}}{\phi^2_{g(j)}} + \frac{1}{2} (\nu + 1) \cdot \trace \[ (\bm{\Psi}_d^{(h+1)})^{-1} \Sm_d^{(h+1)} \] \\
	& \,\,\,\,\,\,\,\,\,\, + \frac{1}{2} (\nu + 1) \cdot (\mathbf{m}_d^{(h+1)}) \tr (\bm{\Psi}_d^{(h+1)})^{-1} \mathbf{m}_d^{(h+1)},
	\end{align*}
	where
	$$
	a_d^{(h)} = \sqrt{\frac{\lambda}{\theta^2_d \delta_d^{(h)}}} \frac{K_{\frac{p - 1}{2}} \( \sqrt{\lambda \delta_d^{(h)}}/\theta_d \)}{K_{\frac{p+1}{2}} \( \sqrt{\lambda \delta_d^{(h)}}/\theta_d \)} + \frac{p+1}{\delta_d^{(h)}},
	$$
	and $K_{\alpha}(\cdot)$ is the modified Bessel function of the second kind.
	
	\subsection{Empirical Bayes}\label{sec:empiricalbayes}
	A full Bayesian model requires the specification of hyper-parameters $\bm{\eta} = \begin{bmatrix} (\bm{\phi}^2) \tr & \bm{\theta} \tr & \lambda & \text{vec}(\bm{\Omega}) \tr & \nu \end{bmatrix} \tr$. These are abstract and hard to interpret parameters for which we generally lack expert knowledge. They do, however, have a significant influence on the shape of the posterior distribution. We therefore propose to estimate the unspecified hyper parameters $\bm{\eta}$ by empirical Bayes. Simply put, empirical Bayes fits the prior model to the data and is, as such, objective (up to model specification). 
	
	The canonical method for empirical Bayes is maximisation of the marginal likelihood with respect to the hyper parameters. In \cite{casella_empirical_2001} the marginal likelihood is maximised by an EM algorithm:
	$$
	\bm{\eta}^{(l+1)} = \underset{\bm{\eta}}{\argmax}\E_{\cdot | \Y} [\log p(\Y, \B, \U, \bm{\Xi}_1, \dots, \bm{\Xi}_D, \bgamma^2, \bsigma^2) | \bm{\eta}^{(l)}],
	$$
	where the expectation is with respect to the posterior. In our case, this posterior is not available in closed form, which renders the expectation difficult. While \cite{casella_empirical_2001} suggests to approximate the expectation by a Monte Carlo sample, we propose to use the variational Bayes approximation developed in Section \ref{sec:variationalbayes}:
	\begin{align}\label{eq:variationalbayes}
	\bm{\eta}^{(l+1)} & = \underset{\bm{\eta}}{\argmax}\E_{Q^{(l)}} [\log p(\Y, \B, \U, \bm{\Xi}_1, \dots, \bm{\Xi}_D, \bgamma^2, \bsigma^2)] \nonumber \\
	& = \underset{\bm{\eta}}{\argmax} \E_{Q^{(l)}} [\log \pi (\B | \bgamma^2, \bsigma^2)] + \E_{Q^{(l)}} [\log \pi (\bgamma^2)] + \E_{Q^{(l)}} [\log \pi (\bm{\Xi}_1, \dots, \bm{\Xi}_D)].
	\end{align}
	where now the expectation is with respect to the converged variational posterior $Q^{(l)}$. Inspection of (\ref{eq:variationalbayes}) learns us that the problem may be decomposed into three separate optimisation problems.
	
	\subsubsection{Group-specific variance components}
	The first sub-problem concerns the group-specific variance components of the molecular feature effects. In order to separate the overall variance of the molecular feature effects from the group-specific deviations, we impose $\prod_{g=1}^G (\phi_g^2)^{|\mathcal{G}_g|} = 1$. As a consequence, the overall level of shrinkage is determined by the $\gamma^2_d$, while the $\phi_g^2$ effectuate differential shrinkage of the groups. Because the $\phi_g^2$ are variance components, we additionally require $\phi^2_1, \dots, \phi^2_G> 0$, such that our first optimisation sub-problem becomes:
	\begin{align*}
	(\bm{\phi}^2)^{(l+1)} & = \underset{\bm{\phi}^2}{\argmax} \E_{Q^{(l)}} [\log \pi (\B | \bgamma^2, \bsigma^2)] \\
	& = \underset{\bm{\phi}^2}{\argmax} -\frac{D}{2} \sum_{g=1}^G |\G_g| \log \phi_g^2 - \frac{1}{2} \sum_{g=1}^G b_g^{(l)} \phi_g^{-2} \\
	& \text{subject to } \prod_{g=1}^G (\phi_g^2)^{|\G_g|} = 1 \text{ and } \phi^2_1, \dots, \phi^2_G> 0.
	\end{align*}
	where 
	$$
	b_g^{(l)}=\sum_{d=1}^D a_d^{(l)} \frac{n+p+T+1}{2\zeta_d^{(l)}} \sum_{j \in \mathcal{G}_g} \[ (\mu_{jd}^{(l)})^2 + (\bSigma_d^{(l)})_{jj} \].
	$$
	This is a convex optimisation problem and easily solved numerically.
	
	\subsubsection{Drug-specific variance components}
	The second sub-problem concerns the prior for the drug-specific variance components $\gamma_d^2$. As explained in Section \ref{sec:SEM}, we may have `covariates' $\mathbf{c}_d$ on the drugs available. We conjecture that these covariates may be informative for the omics effect size and therefore model the prior mean of the $\gamma_d^2$ as a function of these covariates: $\theta_d = (\mathbf{c}_d \tr \bm{\alpha})^{-1}$. Then, the empirical Bayes estimation of $\bm{\alpha}$ effectively boils down to an inverse Gaussian regression of the $\gamma_d^{-2}$ on the $\mathbf{c}_d$ \cite[]{fries_optimal_1986,whitmore_regression_1983}. The problem may be formulated as:
	\begin{align*}\label{eq:optproblem2}
	\lambda^{(l+1)}, \bm{\alpha}^{(l+1)} & = \underset{\lambda,\bm{\alpha}}{\argmax} \E_{Q^{(l)}} [\log \pi (\bgamma^2)] \\
	%	& = \underset{\lambda,\bm{\alpha}}{\argmax} \frac{D}{2} \log \lambda - \frac{\lambda}{2} \sum_{d=1}^D \[ \E(\gamma_d^2)(\mathbf{c} _d \tr \bm{\alpha})^2 - 2 \mathbf{c}_d \tr \bm{\alpha} + \E(\gamma_d^{-2}) \] \\
	& = \underset{\lambda,\bm{\alpha}}{\argmax} \frac{D}{2} \log \lambda - \frac{\lambda}{2} \bm{\alpha} \tr \mathbf{C} \tr \diag [\E_{Q^{(l)}}(\gamma_d^2)] \mathbf{C} \bm{\alpha} + \lambda \bm{\alpha} \tr \mathbf{C} \tr \mathbf{1} - \frac{\lambda}{2} \sum_{d=1}^D \E_{Q^{(l)}}(\gamma_d^{-2}),
	\end{align*}
	where $\mathbf{C} = \begin{bmatrix} \mathbf{c}_1 & \dots & \mathbf{c}_D \end{bmatrix} \tr$. The solutions to this problem are:
	\begin{align*}
	\bm{\alpha}^{(l+1)} & = \left\{ \mathbf{C} \tr \Vm^{(l)} \mathbf{C} \right\}^{-1} \mathbf{C} \tr \mathbf{1}, \\
	\lambda^{(l+1)} & = \[ \sum_{d=1}^D a_d^{(l)} - (\bm{\alpha}^{(l+1)}) \tr \mathbf{C} \tr \mathbf{1} \]^{-1} D,
	\end{align*}
	where 
	$$
	\Vm^{(l)} = \diag \[ \sqrt{\frac{\delta_d^{(l)}(\theta_d^{(l)})^2}{\lambda^{(l)}}} \frac{K_{\frac{p - 1}{2}} \( \sqrt{\lambda^{(l)} \delta_d^{(l)}}/\theta^{(l)}_d \)}{K_{\frac{p + 1}{2}} \( \sqrt{\lambda^{(l)} \delta_d^{(l)}}/\theta^{(l)}_d \)} \].
	$$
	
	- Think about parametrisation of $\mathbf{C}$. Idea: use overall mean and deviations from it in alpha, such that the starting value can be the mean and 0 for alpha. Or: use intercept and code the rest as dummy variables.
	
	\subsubsection{Tissue effect covariance}
	The third sub-problem pertains to the tissue effect covariance prior. To avoid overfitting when freely estimating the $\frac{T(T+1)}{2}$ elements in $\bm{\Omega}$ and $\nu$, we parametrise them as $\bm{\Omega}=  \tau [(\tau - \rho) \I_T + \rho \mathbf{1}_{T \times T}]$ and $\nu = \tau + T + 1$, respectively. The justification for this parametrisation is three-fold: (a) it implies an easy to interpret prior covariance matrix with $\tau$ on its diagonal and $\rho$ on its off-diagonals, (b) it reduces the number of parameters to  estimate to just two, and (c) simplifies the positive definite constraint on  $\bm{\Omega}$. To ensure positive definiteness and a proper covariance matrix, we impose the constraints $\tau > \max\[\rho - T \rho,\rho, 0\]$ (see Appendix \ref{app:priortissuecovariance}). The resulting third sub-problem is a bit more involved than the first two:
	\begin{align*}
	\tau^{(l + 1)}, \rho^{(l+1)} & = \underset{\tau, \rho}{\argmax} \frac{D (T^2 + T)}{2} \log \tau + \frac{DT}{2} \tau \log \tau + \frac{D(T+1)}{2} \log \[ \tau - (T - 1) \rho\] \\
	& \,\,\,\,\,\,\,\,\,\, + \frac{D}{2} \tau \log \[ \tau - (T - 1) \rho\] + \frac{D(T^2 - 1)}{2} \log(\tau - \rho) +\frac{D(T - 1)}{2} \tau  \log(\tau - \rho) \\
	& \,\,\,\,\,\,\,\,\,\, - D \sum_{t=1}^T \log \Gamma \( \frac{\tau + t + 1}{2} \) - w_1^{(l)} \tau^2 + w_2^{(l)} \tau \rho + w_3^{(l)} \tau, \\
	& \text{subject to } \tau > \max\[\rho - T \rho,\rho, 0\],
	\end{align*}
	where
	\begin{align*}
	w_1^{(l)} & =  \frac{\tau^{(l)} + T + 2}{2} \sum_{d=1}^D \trace \[ (\bm{\Psi}_d^{(l)})^{-1}\] > 0,\\
	w_2^{(l)} & = \frac{\tau^{(l)} + T + 2}{2} \sum_{d=1}^D \left\{\trace \[ (\bm{\Psi}_d^{(l)})^{-1}\] - \trace \[\mathbf{1}_{T \times T} \cdot (\bm{\Psi}_d^{(l)})^{-1}\] \right\} < w_1^{(l)},\\
	w_3^{(l)} & = \frac{1}{2} \[D \sum_{t=1}^T \psi\(\frac{\tau^{(l)} + t + 2}{2}\) - \sum_{d=1}^D \log |\bm{\Psi}_d^{(l)}| \].
	\end{align*}
	
	- use mathematica to check convexity
	
	- we use the ELBO for VB convergence and parameters for EB convergence
	
	- we might be lenient for VB convergence. The extreme case would be 1 VB iteration per EB iteration (corresponds to tolerance of infinity).

%	\begin{align*}
%	\tau^{(l+1)}, \rho^{(l+1)} & = \underset{\tau, \rho}{\argmax} \E_{Q^{(l)}} [\log \pi (\bm{\Xi}_1, \dots, \bm{\Xi}_D)] \\
%	& = \underset{\tau, \rho}{\argmax} w_1^{(l)} \tau - w_2^{(l)} \tau^2 - w_3^{(l)} \tau \rho - D \log \Gamma_T \(\frac{\tau}{2}\) + \frac{DT}{2} \tau \log \tau \\
%	& \,\,\,\,\,\,\,\,\,\,+ \frac{D}{2} \tau \log \[ \tau - (1-T)\rho \] + \frac{D(T-1)}{2} \tau \log (\tau - \rho), \\
%	& \text{subject to } \tau > \max\[\rho - T \rho,\rho, 0\] \text{ and } 1> |\rho| \geq 0,
%	\end{align*}
%	where 
%	\begin{align*}
%	w_1^{(l)} & = \frac{1}{2} \[D \psi_T\(\frac{\tau^{(l)} + 1}{2}\) + \sum_{d=1}^D \log |\bm{\Psi}_d^{(l)}| \], \\
%	w_2^{(l)} & = \frac{\tau^{(l)} + 1}{2} \sum_{d=1}^D \trace \(\bm{\Psi}_d^{(l)}\) > 0,\\
%	w_3^{(l)} & = \frac{\tau^{(l)} + 1}{2} \sum_{d=1}^D \trace \[(\mathbf{1}_{T \times T} - \I_T) \cdot \bm{\Psi}_d^{(l)}\],
%	\end{align*}
%	and $\Gamma_T(x)$ and $\psi_T(x)$ are the $T$-variate gamma and digamma functions, respectively. $\log \Gamma_T \(\frac{\tau}{2}\)$ is convex on the intervals $(0,1), \dots, (T-1, \infty)$

%	- we model the prior expectation of the covariance instead of the precision. Precisions are conditional on the other variables, which doesn't make sense in the tissue context.
	
%	The second sub problem estimates the molecular feature effect hyper prior:
%	\begin{align}\label{eq:optproblem2}
%	k^{(l+1)},\theta^{(l+1)} & = \underset{k,\theta}{\argmax} \E_{Q^{(l)}} [\log \pi (\bgamma^2)] \nonumber\\
%	& = \underset{k,\theta}{\argmax} D k \log \theta - D \log \Gamma (k) + e_1^{(l)} k - e_2^{(l)} \theta \\
%	& \text{subject to } k ,\theta > 0,
%	\end{align}
%	where
%	\begin{align*}
%	e_1^{(l)} & = D\psi\(\frac{p}{2} + k^{(l)}\) - \sum_{d=1}^D \log a_d^{(l)}, \\
%	e_2^{(l)} & = \(\frac{p}{2} + k^{(l)} \) \sum_{d=1}^D (a_d^{(l)})^{-1},
%	\end{align*}
%	and $\Gamma(x)$ and $\psi(x)$ are the gamma and digamma functions, respectively. This is again a convex problem (see Appendix \ref{app:convexity}), but more cumbersome to solve numerically due to the log-gamma function. By approximating the digamma function with $\phi(x) \approx \log x - 1/(2x)$, we get the following closed-form solutions:
%	\begin{align*}
%	k^{(l+1)} & = \frac{1}{2} \( \log e_2^{(l)} - D^{-1} e_1^{(l)} - \log D \)^{-1}, \\
%	\theta^{(l+1)} & = D k^{(l+1)} (e_2^{(l)})^{-1}.
%	\end{align*}
	
	\section{Simulations}
	
	- Hamming distance to assess variable selection
	
	\section{Data application}
	- check GSK open target EBI or ask Chris Wallace about it. Database that links targets to drugs and diseases. Might be useful for drugs co-data.
	
	- check NCI-60
	
	- how to transform the data and deal with missingness (ask Oslo and read papers published on GDSC and CCLE repositories) 
	
	- what information is available on drugs (ask Oslo)
	
	- start with gene expression (one $g(j)$)
	
	\subsection{GDSC}
	
	\subsection{CCLE}
	
	\section{Discussion}
	- empirical Bayes gives extra information via estimation of the hyper-parameters, such as informativeness of the drug information (via model for the $\gamma_d$) and relatedness of the tissues (via $\bm{\Omega}$ and $\nu$)
	
	- regression on drug covariates very intuitive.
	
	- find more literature on inverse Gaussian prior for variances and modelling variances with covariates.
	
	\bibliographystyle{author_short3} 
	\bibliography{refs}

	\titleformat{\section}{\normalfont\Large\bfseries}{\appendixname~\thesection.}{1em}{}
	\begin{appendix}
		\section{Variational Bayes derivations}\label{app:derivations}
		In the following all expectations are with respect to the variational posterior $Q$.
		\begin{align*}
		\log q_{\B} (\B) & \propto \E_{\mathbf{U}, \bsigma^2} [\log \mathcal{L}(\Y | \B, \U, \bsigma^2 )] + \E_{\bgamma^2, \bsigma^2} [\log \pi (\B | \bgamma^2, \bsigma^2)] \\
		& \propto - \frac{1}{2} \sum_{d=1}^D \sum_{i=1}^n \E_{\mathbf{u}_d, \sigma_d^2} \[ \frac{(y_{id} - \x_i \tr \bbeta_d - \mathbf{z}_i \tr \mathbf{u}_d)^2}{\sigma_d^2} \] - \frac{1}{2} \sum_{d=1}^D \sum_{j=1}^p \phi_{g(j)}^{-2} \E_{\gamma^2_d, \sigma^2_d} \( \frac{\beta_{jd}^2}{\gamma_d^2 \sigma_d^2} \), \\
		q_{\B} (\B) & \overset{D}{=} \prod_{d=1}^{D} \mathcal{N}_p (\bmu_d, \bSigma_d), \\
		& \text{ with } \bSigma_d = \E(\sigma_d^{-2})^{-1} [\X \tr \X + \E(\gamma_d^{-2}) \cdot \diag (\phi_{g(j)}^{-2}) ]^{-1}, \\
		& \text{ and } \bmu_d = [\X \tr \X + \E(\gamma_d^{-2}) \cdot \diag (\phi_{g(j)}^{-2}) ]^{-1} \X \tr [\y_d - \Z \cdot \E(\mathbf{u}_d)].
		\end{align*}
		
		\begin{align*}
		\log q_{\U}(\U) & \propto \E_{\mathbf{B}, \bsigma^2} [\log \mathcal{L}(\Y | \B, \U, \bsigma^2 )] + \E_{\mathbf{\Xi}_1, \dots, \mathbf{\Xi}_D, \bsigma^2} [\log \pi (\U | \mathbf{\Xi}_1, \dots, \mathbf{\Xi}_D, \bsigma^2)] \\
		& \propto - \frac{1}{2} \sum_{d=1}^D \sum_{i=1}^n \E_{\bbeta_d, \sigma_d^2} \[ \frac{(y_{id} - \x_i \tr \bbeta_d - \mathbf{z}_i \tr \mathbf{u}_d)^2}{\sigma_d^2} \] - \frac{1}{2} \sum_{d=1}^D \E_{\bm{\Xi}_d, \sigma_d^2}\( \frac{\mathbf{u}_d \tr \bm{\Xi}_d^{-1} \mathbf{u}_d}{\sigma_d^2} \), \\
		q_{\U}(\U) & \overset{D}{=} \prod_{d=1}^{D} \mathcal{N}_T (\mathbf{m}_d, \Sm_d), \\
		& \text{ with } \Sm_d = \E(\sigma_d^{-2})^{-1} [\Z \tr \Z + \E(\bm{\Xi}_d^{-1}) ]^{-1}, \\
		& \text{ and } \mathbf{m}_d = [\Z \tr \Z + \E(\bm{\Xi}_d^{-1}) ]^{-1} \Z \tr [\y_d - \X \cdot \E(\bbeta_d)].
		\end{align*}

		\begin{align*}
		\log q_{\bgamma^{2}}(\bgamma^{2}) & \propto \E_{\B, \bsigma^2} [\log \pi (\B | \bgamma^{2}, \sigma^2)] + \log \pi (\bgamma^{2}) \\
		& \propto -\frac{p}{2} \sum_{d=1}^D \log \gamma_d^{2} - \frac{1}{2} \sum_{d=1}^D \sum_{j=1}^p \phi_{g(j)}^{-2} \E \( \frac{\beta_{jd}^2}{\sigma^{2}_d}\) \gamma_d^{-2} - \frac{3}{2} \sum_{d=1}^D \log \gamma_d^{2} \\
		& \,\,\,\,\,\,\,\,\,\, - \frac{\lambda}{2 \theta^2} \sum_{d=1}^D (\gamma_d^2 - \theta)^2 \gamma_d^{-2} \\
		& \propto \sum_{d=1}^D \left\{ \( -\frac{p + 1}{2} - 1\) \log \gamma_d^{2} - \frac{\lambda}{2\theta^2} \gamma_d^2 - \frac{1}{2}\[ \lambda + \E(\sigma_d^{-2}) \sum_{j=1}^p \frac{\E(\beta_{jd}^2)}{\phi_{g(j)}^{2}}\] \gamma_d^{-2} \right\}, \\
		q_{\bgamma^{2}}(\bgamma^{2}) & \overset{D}{=} \prod_{d=1}^D \mathcal{GIG} \(-\frac{p+1}{2}, \frac{\lambda}{\theta_d^2}, \delta_{d} \), \\
		& \text{with } \delta_{d} =\E(\sigma_d^{-2}) \sum_{j=1}^p \frac{\E(\beta_{jd})^2 + \V(\beta_{jd})}{\phi_{g(j)}^2} + \lambda.
		\end{align*}
		
%		\begin{align*}
%		\log q_{\bgamma^{-2}}(\bgamma^{-2}) & \propto \E_{\B, \bsigma^2} [\log \pi (\B | \bgamma^{-2}, \sigma^2)] + \log \pi (\bgamma^{-2}) \\
%		& \propto \frac{p}{2} \sum_{d=1}^D \log \gamma_d^{-2} - \frac{1}{2} \sum_{d=1}^D \sum_{j=1}^p \phi_{g(j)}^{-2} \E \( \frac{\beta_{jd}^2}{\sigma^{2}_d}\) \gamma_d^{-2} + (k - 1) \sum_{d=1}^D \log \gamma_d^{-2} - \theta \sum_{d=1}^D \gamma_d^{-2} \\
%		& = \sum_{d=1}^D \left\{ \( \frac{p}{2} + k - 1\) \log \gamma_d^{-2} - \[ \frac{\E(\sigma_d^{-2})}{2} \sum_{j=1}^p \frac{\E(\beta_{jd}^2)}{\phi_{g(j)}^{2}} + \theta \] \gamma_d^{-2} \right\}.
%		\end{align*}

%	\begin{align*}
%	q_{\bgamma^{-2}}(\bgamma^{-2}) & \overset{D}{=} \prod_{d=1}^D \Gamma \(\frac{p}{2} + k, a_{d} \), \\
%	& \text{ with } a_{d} =\frac{\E(\sigma_d^{-2})}{2} \sum_{j=1}^p \frac{\E(\beta_{jd})^2 + \V(\beta_{jd})}{\phi_{g(j)}^2} + \theta.
%	\end{align*} 
		
		\begin{align*}
		\log q_{\bsigma^{2}}(\bsigma^{2}) & \propto \E_{\B,\mathbf{U}} [\log \mathcal{L}(\Y | \B, \U, \bsigma^2 )] + \E_{\B, \bgamma^2} [\log \pi (\B | \bgamma^2, \bsigma^2)] \\
		& \,\,\,\,\,\,\,\,\,\, + \E_{\mathbf{U}, \bm{\Xi}_1, \dots, \bm{\Xi}_D} [\log \pi (\mathbf{U} | \bm{\Xi}_1, \dots, \bm{\Xi}_D, \bsigma^2)] + \log \pi (\bsigma^{2}) \\
		& \propto -\frac{n}{2} \sum_{d=1}^D \log \sigma_d^{2} - \frac{1}{2} \sum_{d=1}^D \sum_{i=1}^n \E [(y_{id} - \x_i \tr \bbeta_d - \mathbf{z}_i \tr \mathbf{u}_d)^2] \sigma_d^{2} - \frac{p}{2} \sum_{d=1}^D \log \sigma_d^{-2} \\
		& \,\,\,\,\,\,\,\,\,\, - \frac{1}{2} \sum_{d=1}^D \sum_{j=1}^p \E( \gamma_d^{-2}) \frac{\E ( \beta_{jd}^2)}{\phi_{g(j)}^{2}} \sigma_d^{-2} - \frac{T}{2} \sum_{d=1}^D \log \sigma_d^{2} - \frac{1}{2} \sum_{d=1}^D \E(\mathbf{u}_d \tr \bm{\Xi}_d^{-1} \mathbf{u}) \sigma_d^{-2} \\
		& \,\,\,\,\,\,\,\,\,\, - \frac{3}{2} \sum_{d=1}^D \log \sigma^2_d \\
		& = -\sum_{d=1}^{D} \(\frac{n + p + T + 3}{2} \) \log \sigma_d^{2} - \sum_{d=1}^{D} \frac{1}{2}\Bigg\{ \sum_{i=1}^n \E [(y_{id} - \x_i \tr \bbeta_d - \mathbf{z}_i \tr \mathbf{u}_d)^2] \\
		& \,\,\,\,\,\,\,\,\,\, + \E(\gamma_d^{-2})\sum_{j=1}^p \frac{\E(\beta_{jd}^2)}{\phi_{g(j)}^2} + \E(\mathbf{u}_{d} \tr \bm{\Xi}_d^{-1} \mathbf{u}_d) \Bigg\} \sigma_d^{-2}, \\
		q_{\bsigma^{2}}(\bsigma^{2}) & \overset{D}{=} \prod_{d=1}^D \Gamma^{-1} \(\frac{n + p + T + 1}{2}, \zeta_{d} \), \\
		& \text{ with } \zeta_{d} = \frac{\y_d \tr \y_d}{2} - \y_d \tr [\X \E(\bbeta_d) + \Z \E(\mathbf{u}_d)] + \E(\bbeta_d \tr) \X \tr \Z \E (\mathbf{u}_d) \\
		& \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + \frac{\trace [\X \tr \X \V(\bbeta_d)]}{2} + \frac{\E(\bbeta_d \tr) \X \tr \X \E(\bbeta_d)}{2} + \frac{\trace [\Z \tr \Z \V(\mathbf{u}_d)]}{2} \\
		& \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + \frac{\E(\mathbf{u}_d \tr) \Z \tr \Z \E(\mathbf{u}_d)}{2} + \frac{\E(\gamma_d^{-2})}{2} \sum_{j=1}^p \frac{\E(\beta_{jd})^2 + \V(\beta_{jd})}{\phi_{g(j)}^2} \\
		& \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,+ \frac{\E(\mathbf{u}_d \tr) \E(\bm{\Xi}_d^{-1}) \E(\mathbf{u}_d)}{2} + \frac{\trace [\E(\bm{\Xi}_d^{-1}) \V(\mathbf{u}_d)]}{2},
		\end{align*}
		
%		\begin{align*}
%		\log q_{\bsigma^{-2}}(\bsigma^{-2}) & \propto \E_{\B,\mathbf{U}} [\log \mathcal{L}(\Y | \B, \U, \bsigma^2 )] + \E_{\B, \bgamma^2} [\log \pi (\B | \bgamma^2, \bsigma^2)] \\
%		& \,\,\,\,\,\,\,\,\,\, + \E_{\mathbf{U}, \bm{\Xi}_1, \dots, \bm{\Xi}_D} [\log \pi (\mathbf{U} | \bm{\Xi}_1, \dots, \bm{\Xi}_D, \bsigma^2)] + \log \pi (\bsigma^{-2}) \\
%		& \propto \frac{n}{2} \sum_{d=1}^D \log \sigma_u^{-2} - \frac{1}{2} \sum_{d=1}^D \sum_{i=1}^n \E [(y_{id} - \x_i \tr \bbeta_d - \mathbf{z}_i \tr \mathbf{u}_d)^2] \sigma_d^{-2} + \frac{p}{2} \sum_{d=1}^D \log \sigma_d^{-2} \\
%		& \,\,\,\,\,\,\,\,\,\, - \frac{1}{2} \sum_{d=1}^D \sum_{j=1}^p \phi_{g(j)}^{-2} \E \( \frac{\beta_{jd}^2}{\gamma_d^{2}} \) \sigma_d^{-2} + \frac{T}{2} \sum_{d=1}^D \log \sigma_d^{-2} - \frac{1}{2} \sum_{d=1}^D \E(\mathbf{u}_d \tr \bm{\Xi}_d^{-1} \mathbf{u}) \sigma_d^{-2} \\
%		& \,\,\,\,\,\,\,\,\,\, - 0.999 \sum_{d=1}^D \log \sigma_d^{-2} - 0.001 \sum_{d=1}^D \sigma_d^{-2} \\
%		& = \sum_{d=1}^{D} \(\frac{n + p + T}{2} - 0.999 \) \log \sigma_d^{-2} - \sum_{d=1}^{D} \Bigg\{ \frac{\sum_{i=1}^n \E [(y_{id} - \x_i \tr \bbeta_d - \mathbf{z}_i \tr \mathbf{u}_d)^2]}{2} \\
%		& \,\,\,\,\,\,\,\,\,\, + \frac{\E(\gamma_d^{-2})}{2} \sum_{j=1}^p \frac{\E(\beta_{jd}^2)}{\phi_{g(j)}^2} + \frac{\E(\mathbf{u}_{d} \tr \bm{\Xi}_d^{-1} \mathbf{u}_d)}{2}  + 0.001 \Bigg\} \sigma_d^{-2}.
%		\end{align*}

%	\begin{align*}
%	q_{\bsigma^{-2}}(\bsigma^{-2}) & \overset{D}{=} \prod_{d=1}^D \Gamma \(\frac{n + p + T}{2} + 0.001, b_{d} \), \\
%	& \text{ with } b_{d} = \frac{\y_d \tr \y_d}{2} - \y_d \tr [\X \E(\bbeta_d) + \Z \E(\mathbf{u}_d)] + \E(\bbeta_d \tr) \X \tr \Z \E (\mathbf{u}_d) \\
%	& \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + \frac{\trace [\X \tr \X \V(\bbeta_d)]}{2} + \frac{\E(\bbeta_d \tr) \X \tr \X \E(\bbeta_d)}{2} + \frac{\trace [\Z \tr \Z \V(\mathbf{u}_d)]}{2} \\
%	& \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + \frac{\E(\mathbf{u}_d \tr) \Z \tr \Z \E(\mathbf{u}_d)}{2} + \frac{\E(\gamma_d^{-2})}{2} \sum_{j=1}^p \frac{\E(\beta_{jd})^2 + \V(\beta_{jd})}{\phi_{g(j)}^2} \\
%	& \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,+ \frac{\E(\mathbf{u}_d \tr) \E(\bm{\Xi}_d^{-1}) \E(\mathbf{u}_d)}{2} + \frac{\trace [\E(\bm{\Xi}_d^{-1}) \V(\mathbf{u}_d)]}{2} + 0.001.
%	\end{align*}
		
		\begin{align*}
		\log q_{\bm{\Xi}_1, \dots, \bm{\Xi}_D}(\bm{\Xi}_1, \dots, \bm{\Xi}_D) & \propto \E_{\mathbf{U}, \bm{\sigma}^2} [\log \pi (\mathbf{U} | \bm{\Xi}_1, \dots, \bm{\Xi}_D, \bsigma^2)] + \log \pi (\bm{\Xi}_1, \dots, \bm{\Xi}_D) \\
		& \propto -\frac{1}{2} \sum_{d=1}^D \log |\bm{\Xi}_d| - \frac{1}{2} \sum_{d=1}^D \E \(\frac{\mathbf{u}_d \tr \bm{\Xi}_d^{-1} \mathbf{u}_d}{\sigma_d^2} \) \\
		& \,\,\,\,\,\,\,\,\,\, - \frac{k + T + 1}{2} \sum_{d=1}^D \log |\bm{\Xi}_d| - \frac{1}{2} \sum_{d=1}^D \trace (\bm{\Omega} \bm{\Xi}_d^{-1}) \\
		& = -\frac{\tau + T + 2}{2} \sum_{d=1}^D \log |\bm{\Xi}_d| - \frac{1}{2} \sum_{d=1}^D \trace \left\{ \[\E(\sigma_d^{-2}) \E( \mathbf{u}_d \mathbf{u}_d \tr) + \bm{\Omega}\] \bm{\Xi}_d^{-1} \right\}, \\
		q_{\bm{\Xi}_1, \dots, \bm{\Xi}_D}(\bm{\Xi}_1, \dots, \bm{\Xi}_D) & \overset{D}{\propto} \prod_{d=1}^D \mathcal{W}_T^{-1} (\bm{\Psi}_d, \nu + 1),  \\ 
		& \text{ with } \bm{\Psi}_d =\E(\sigma^{-2}_d) \[\V(\mathbf{u}_d) + \E(\mathbf{u}_d) \E( \mathbf{u}_d \tr) \] + \bm{\Omega}.
		\end{align*}
		
%		\section{Convexity}\label{app:convexity}
%		For (\ref{eq:optproblem2}) to be a convex problem, we need the Hessian $\mathbf{H}$ of the objective function to be positive definite, i.e., $\forall \mathbf{z} \in \mathbb{R}^2: \mathbf{z} \tr \mathbf{H} \mathbf{z} > 0$. We have
%		\begin{align*}
%		\mathbf{z} \tr \mathbf{H} \mathbf{z} = \frac{D}{\theta^2 \psi^{(1)}(k)} \( z_1 \theta \psi^{(1)}(k) - z_2 \)^2 + z_2^2 \( k \psi^{(1)}(k)  - 1 \),
%		\end{align*}
%		with $\psi^{(1)}(k)$ the trigamma function. Since the trigamma function is positive for all $k > 0$, and we have $\theta, k > 0$, we need:
%		$$
%		k \psi^{(1)}(k)> 1.
%		$$
%		Using Lemma 1 in \cite{guo_refinements_2013} we can show that for $k > 0$:
%		$$
%		k \psi^{(1)}(k) > 1 + \frac{1}{2k},
%		$$
%		such that the objective function is convex.
		
		\section{Prior tissue covariance constraints}\label{app:priortissuecovariance}
%		The empirical Bayes objective function of the third sub-optimisation problem in \ref{sec:empiricalbayes} is:
%		\begin{align*}
%		\E_{Q^{(l)}}[\log \pi (\mathbf{\Xi}_1, \dots, \mathbf{\Xi}_D)] & \propto \frac{D}{2} \nu \log |\bm{\Omega}| - \frac{D T \log 2}{2} \nu - D \log \Gamma_T \(\frac{\nu}{2}\) + \frac{\nu}{2}\sum_{d=1}^D \E_{Q^{(l)}} (\log | \mathbf{\Xi}_d^{-1}|) \\
%		& \,\,\,\,\,\,\,\,\,\, - \frac{1}{2} \sum_{d=1}^D \trace [ \bm{\Omega} \cdot \E_{Q^{(l)}}(\mathbf{\Xi}_d^{-1})] \\
%		& = \frac{D}{2} \nu \log |\bm{\Omega}| - D \log \Gamma_T\(\frac{\nu}{2}\) - \frac{\nu^{(l)} + 1}{2} \sum_{d=1}^D \trace [ \bm{\Omega} \cdot ( \mathbf{\Psi}_d^{(l)})^{-1}] \\
%		& \,\,\,\,\,\,\,\,\,\, + \frac{\nu}{2} \sum_{d=1}^D \[ \psi_T\(\frac{\nu^{(l)} + 1}{2}\) - \log |\mathbf{\Psi}_d^{(l)}| \],
%		\end{align*}
%		which is maximised under constraints $\nu > T + 1$ and $\bm{\Omega}$ positive definite (PD), to ensure a proper inverse Wishart distribution.
		
		We parametrise the prior covariance as a matrix with $\tau$ on the diagonal and $\rho$ on its off-diagonals. To that end we set $\nu = \tau + T + 1$ and the $T \times T$-dimensional matrix
		$$
		\bm{\Omega} = \tau \begin{bmatrix}
		\tau &  & \rho \\
		& \ddots &  \\
		\rho & & \tau
		\end{bmatrix},
		$$
		such that $\E_{\pi(\mathbf{\Xi}_d)} (\mathbf{\Xi}_d) = (\nu - T - 1)^{-1} \bm{\Omega}$ is of the desired form. To ensure a non-degenerate prior distribution we require $\tau > 0$ and $\bm{\Omega}$ PD. We use $\tau > 0$ to note that a PD $\bm{\Omega}$ implies positive roots $\lambda$ of the characteristic polynomial:
		\begin{align*}
		\left| \begin{bmatrix}
		\tau &  & \rho \\
		& \ddots &  \\
		\rho & & \tau
		\end{bmatrix} -\lambda \I_T \right| = (\tau - \lambda - \rho)^{T-1}[\tau - \lambda - (1-T)\rho] = 0.
		\end{align*}
		For $T > 1$, the solutions are:
		$$
		(\lambda = \tau - \rho) \lor (\lambda = T \rho - \rho + \tau),
		$$
		which are positive if the following condition holds: $\tau > \max(\rho,\rho - T \rho)$. We combine this with $\tau > 0$, to arrive at the full proper prior constraint: $\tau > \max(0,\rho,\rho - T \rho)$. 
		
%		The resulting empirical Bayes step is now:
%		\begin{align*}
%		\tau^{(l + 1)}, \rho^{(l+1)} & = \underset{\tau, \rho}{\argmax} \frac{D (T^2 + T)}{2} \log \tau + \frac{DT}{2} \tau \log \tau + \frac{D(T+1)}{2} \log \[ \tau - (T - 1) \rho\] \\
%		& \,\,\,\,\,\,\,\,\,\, + \frac{D}{2} \tau \log \[ \tau - (T - 1) \rho\] + \frac{D(T^2 - 1)}{2} \log(\tau - \rho) +\frac{D(T - 1)}{2} \tau  \log(\tau - \rho) \\
%		& \,\,\,\,\,\,\,\,\,\, - D \sum_{t=1}^T \log \Gamma \( \frac{\tau + t + 1}{2} \) - w_1^{(l)} \tau^2 + w_2^{(l)} \tau \rho + w_3^{(l)} \tau, \\
%		& \text{subject to } \tau > \max\[\rho - T \rho,\rho, 0\],
%		\end{align*}
%		with
%		\begin{align*}
%		w_1^{(l)} & =  \frac{\tau^{(l)} + T + 2}{2} \sum_{d=1}^D \trace \[ (\bm{\Psi}_d^{(l)})^{-1}\] > 0,\\
%		w_2^{(l)} & = \frac{\tau^{(l)} + T + 2}{2} \sum_{d=1}^D \left\{\trace \[ (\bm{\Psi}_d^{(l)})^{-1}\] - \trace \[\mathbf{1}_{T \times T} \cdot (\bm{\Psi}_d^{(l)})^{-1}\] \right\} < w_1^{(l)},\\
%		w_3^{(l)} & = \frac{1}{2} \[D \sum_{t=1}^T \psi\(\frac{\tau^{(l)} + t + 2}{2}\) - \sum_{d=1}^D \log |\bm{\Psi}_d^{(l)}| \].
%		\end{align*}

		\section{Model without tissue effects}
		\subsection{Model}
		In order to investigate the inverse Gaussian prior model and its estimation, we consider the simpler model:
		\begin{align*}
		y_{id} | \bm{\beta}_d, \sigma_d^2 & \sim \mathcal{N} (\x_i \tr \bm{\beta}_d, \sigma_d^2), \\
		\beta_{jd} | \gamma_d^2, \sigma_d^2 & \sim \mathcal{N} (0, \sigma_d^2 \gamma_d^2), \\
		\gamma_d^2 & \sim \mathcal{IG}(\theta_d, \lambda), \\
		\sigma_d^2 & \sim 1/\sigma_d^3.
		\end{align*}
		So in other words, we ignore the tissue effect and fix all $\phi_g^2$ to one. 
		\subsection{Variational Bayes}
		The corresponding variational Bayes posterior is given by:
		\begin{align*}
		q(\bm{\beta}) & \overset{D}{=} \prod_{d=1}^D \mathcal{N}_p (\bm{\mu}_d, \bm{\Sigma}_d), \\
		q(\bm{\gamma}^2) & \overset{D}{=} \prod_{d=1}^D \mathcal{GIG}\(-\frac{p+1}{2}, \frac{\lambda}{\theta_d^2}, \delta_d\), \\
		q(\bm{\sigma}^2) & \overset{D}{=} \prod_{d=1}^D \Gamma^{-1} \(\frac{n + p + 1}{2}, \zeta_d\).
		\end{align*}
		We update the parameters until convergence:
		\begin{align*}
		\bm{\Sigma}_d^{(h+1)} & = \frac{2 \zeta_d^{(h)}}{n+p+1} (\X \tr \X + a_d^{(h)} \I)^{-1}, \\
		\bm{\mu}_d^{(h+1)} & = (\X \tr \X + a_d^{(h)} \I)^{-1} \X \tr \y_d, \\
		\delta_d^{(h+1)} & = \frac{n+p+1}{2 \zeta_d^{(h)}} [\trace(\bm{\Sigma}_d^{(h+1)}) + (\bm{\mu}_d^{(h+1)}) \tr \bm{\mu}_d^{(h+1)}] + \lambda, \\ \zeta_d^{(h+1)} & = \frac{a_d^{(h+1)}}{2} \[\mathbf{y}_d \tr \mathbf{y}_d -2 \mathbf{y}_d \tr \X \bm{\mu}_d^{(h+1)} + \trace [ (\X \tr \X + \I_p) \bm{\Sigma}_d^{(h+1)}] + (\bm{\mu}_d^{(h+1)}) \tr (\X \tr \X + \I_p)\bm{\mu}_d^{(h+1)}\],
		\end{align*}
		with
		$$
		a_d^{(h)} = \sqrt{\frac{\lambda}{\theta_d^2 \delta_d^{(h)}}} \frac{K_{\frac{p - 1}{2}}\(\sqrt{\lambda \delta_d^{(h)}} /\theta_d \)}{K_{\frac{p + 1}{2}}\(\sqrt{\lambda \delta_d^{(h)}} /\theta_d \)} + \frac{p+1}{\delta_d^{(h)}}.
		$$
		
		\subsection{Empirical Bayes}
		Again we model $\theta_d = (\mathbf{c}_d \tr \bm{\alpha})^{-1}$ to arrive at the following empirical Bayes updates:
		\begin{align*}
		\bm{\alpha}^{(l+1)} & = \left\{ \mathbf{C} \tr \Vm^{(l)} \mathbf{C} \right\}^{-1} \mathbf{C} \tr \mathbf{1}, \\
		\lambda^{(l+1)} & = \[ \sum_{d=1}^D a_d^{(l)} - (\bm{\alpha}^{(l+1)}) \tr \mathbf{C} \tr \mathbf{1} \]^{-1} D,
		\end{align*}
		where 
		$$
		\Vm^{(l)} = \diag \[ \sqrt{\frac{\delta_d^{(l)}(\theta_d^{(l)})^2}{\lambda^{(l)}}} \frac{K_{\frac{p - 1}{2}} \( \sqrt{\lambda^{(l)} \delta_d^{(l)}}/\theta^{(l)}_d \)}{K_{\frac{p + 1}{2}} \( \sqrt{\lambda^{(l)} \delta_d^{(l)}}/\theta^{(l)}_d \)} \].
		$$
		
		\subsection{Evidence lower bound}
		We monitor convergence through the evidence lower bound (ELBO):
		\begin{align*}
		\text{ELBO} & (Q^{(l)}) \propto \frac{1}{2} \sum_{d=1}^D \log |\bm{\Sigma}_d^{(l)}| - \frac{n + p + 1}{2} \sum_{d=1}^D \log \zeta_d^{(l)} + \sum_{d=1}^D \log K_{\frac{p+1}{2}}\(\sqrt{\lambda^{(l)} \delta_d^{(l)}}/\theta_d^{(l)}\) \\
		& - \frac{n+p+1}{4} \sum_{d=1}^D (\zeta_d^{(l)})^{-1} \[ \y_d \tr \y_d - 2\y_d \tr \X \bm{\mu}_d^{(l)} + (\bm{\mu}_d^{(l)}) \tr \X \tr \X \bm{\mu}_d^{(l)} + \trace ( \X \tr \X \bm{\Sigma}_d^{(l)})\] \\
		& + \frac{D(p+3)}{4} \log \lambda^{(l)} + \lambda^{(l)} \sum_{d=1}^D (\theta_d^{(l)})^{-1} - \frac{p+1}{2} \sum_{d=1}^D \log \theta_d^{(l)} - \frac{p+1}{4} \sum_{d=1}^D \log \delta_d^{(l)} \\
		& + \frac{1}{2} \sum_{d=1}^D \[ \delta_d^{(l)} - \lambda^{(l)} - \frac{n+p+1}{2\zeta_d^{(l)}} (\bm{\mu}_d^{(l)}) \tr \bm{\mu}_d^{(l)} - \frac{n+p+1}{2\zeta_d^{(l)}} \trace (\bm{\Sigma}_d^{(l)}) \] a_d^{(l)}. 
		\end{align*}
		
%		\begin{align*}
%		y_{id} | \bm{\beta}_d & \sim \mathcal{N} (\x_i \tr \bm{\beta}_d, \sigma_d^2), \\
%		\beta_{jd} | \gamma_d^2 & \sim \mathcal{N} (0, \sigma_d^2 \gamma_d^2), \\
%		\gamma_d^2 & \sim \mathcal{IG}(\theta_d, \lambda).
%		\end{align*}
%		So in other words, we ignore the tissue effect, fix the $\sigma_d^2$, and set all $\phi_g^2$ to one. The corresponding variational Bayes posterior is given by:
%		\begin{align*}
%		q(\bm{\beta}) & \overset{D}{=} \prod_{d=1}^D \mathcal{N}_p (\bm{\mu}_d, \bm{\Sigma}_d), \\
%		q(\bm{\gamma}^2) & \overset{D}{=} \prod_{d=1}^D \mathcal{GIG}\(-\frac{p+1}{2}, \frac{\lambda}{\theta_d^2}, \delta_d\).
%		\end{align*}
%		We update the parameters until convergence:
%		\begin{align*}
%		\bm{\Sigma}_d^{(h+1)} & = \sigma_d^2 (\X \tr \X + a_d^{(h)} \I)^{-1}, \\
%		& \text{with } a_d^{(h)} = \sqrt{\frac{\lambda}{\theta_d^2 \delta_d^{(h)}}} \frac{K_{\frac{p - 1}{2}}\(\sqrt{\lambda \delta_d^{(h)}} /\theta_d \)}{K_{\frac{p + 1}{2}}\(\sqrt{\lambda \delta_d^{(h)}} /\theta_d \)} + \frac{p+1}{\delta_d^{(h)}}, \\
%		\bm{\mu}_d^{(h+1)} & = (\X \tr \X + a_d^{(h)} \I)^{-1} \X \tr \y_d, \\
%		\delta_d^{(h+1)} & = \sigma_d^{-2} [\trace(\bm{\Sigma}_d^{(h+1)}) + (\bm{\mu}_d^{(h+1)}) \tr \bm{\mu}_d^{(h+1)}] + \lambda.
%		\end{align*}
%		Again we model $\theta_d = (\mathbf{c}_d \tr \bm{\alpha})^{-1}$ to arrive at the following empirical Bayes updates:
%		\begin{align*}
%		\bm{\alpha}^{(l+1)} & = \left\{ \mathbf{C} \tr \Vm^{(l)} \mathbf{C} \right\}^{-1} \mathbf{C} \tr \mathbf{1}, \\
%		\lambda^{(l+1)} & = \[ \sum_{d=1}^D a_d^{(l)} - (\bm{\alpha}^{(l+1)}) \tr \mathbf{C} \tr \mathbf{1} \]^{-1} D,
%		\end{align*}
%		where 
%		$$
%		\Vm^{(l)} = \diag \[ \sqrt{\frac{\delta_d^{(l)}(\theta_d^{(l)})^2}{\lambda^{(l)}}} \frac{K_{\frac{p - 1}{2}} \( \sqrt{\lambda^{(l)} \delta_d^{(l)}}/\theta^{(l)}_d \)}{K_{\frac{p + 1}{2}} \( \sqrt{\lambda^{(l)} \delta_d^{(l)}}/\theta^{(l)}_d \)} \].
%		$$
%		We monitor convergence through the evidence lower bound (ELBO):
%		\begin{align*}
%		\text{ELBO} & (Q^{(l)}) \propto \sum_{d=1}^D \sigma_d^{-2} \y_d \tr \X \bm{\mu}_d^{(l)} - \frac{1}{2} \sum_{d=1}^D \sigma_d^{-2} \trace ( \X \tr \X \bm{\Sigma}_d^{(l)}) - \frac{1}{2} \sum_{d=1}^D \sigma_d^{-2} (\bm{\mu}_d^{(l)}) \tr \X \tr \X \bm{\mu}_d^{(l)} \\
%		& + \frac{1}{2} \sum_{d=1} \log |\bm{\Sigma}_d^{(l)}| + \frac{D(p+3)}{4} \log \lambda^{(l)} + \lambda^{(l)} \sum_{d=1}^D (\theta_d^{(l)})^{-1} - \frac{p+1}{2} \sum_{d=1}^D \log \theta_d^{(l)} \\
%		& - \frac{p+1}{4} \sum_{d=1}^D \log \delta_d^{(l)} + \sum_{d=1}^D \log K_{\frac{p+1}{2}}\(\sqrt{\lambda^{(l)} \delta_d^{(l)}}/\theta_d^{(l)}\)\\
%		& + \frac{1}{2} \sum_{d=1}^D \left(\rule{0cm}{1.2cm}\right. \[ \delta_d^{(l)} - \lambda^{(l)} - \sigma_d^{-2} (\bm{\mu}_d^{(l)}) \tr \bm{\mu}_d^{(l)} - \sigma_d^{-2} \trace (\bm{\Sigma}_d^{(l)}) \]  \\
%		& \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \cdot \[\sqrt{\frac{\lambda^{(l)}}{\delta_d^{(l)} (\theta^{(l)}_d)^2}} \frac{K_{\frac{p - 1}{2}}\(\frac{\sqrt{\lambda^{(l)} \delta_d^{(l)}}}{\theta^{(l)}_d} \)}{K_{\frac{p + 1}{2}}\(\frac{\sqrt{\lambda^{(l)} \delta_d^{(l)}}}{\theta^{(l)}_d} \)} + \frac{p+1}{\delta_d^{(l)}}\] \left. \rule{0cm}{1.2cm}\right).
%		\end{align*}
		
		- Different options for convergence. We may monitor parameters, ELBO, or a combination. We may also iterate a fixed number of times, especially for the VB. 
		
		\subsection{Efficient computation}
		We need the following quantities during the EB iterations: $\zeta_d$, $a_d$, $\delta_d$, $\trace (\bm{\Sigma}_d)$, $\trace (\X \tr \X \bm{\Sigma}_d)$, $\bm{\mu}_d \tr \bm{\mu}_d$, and $\bm{\mu}_d \tr \X \tr \X \bm{\mu}_d$. In addition, we need $\log|\bm{\Sigma}|$ and $\mathbf{y}_d \tr \X \bm{\mu}_d$ to monitor the ELBO. The first three are just scalar operations. The rest is easily calculated using the SVD $\X = \mathbf{U} \mathbf{D} \mathbf{V} \tr$:
		\begin{align*}
		\trace (\bm{\Sigma}_d) & = \frac{2\zeta_d}{n + p + 1} \[\sum_{i=1}^n (v_i^2 + a_d)^{-1} + \max (p - n, 0)\cdot a_d^{-1}\], \\
		\trace (\X \tr \X \bm{\Sigma}_d) & = \frac{2\zeta_d}{n + p + 1} \sum_{i=1}^n v_i^2(v_i^2 + a_d)^{-1}, \\
		\bm{\mu}_d \tr \bm{\mu}_d & = \sum_{i=1}^n v_i^2 [(\mathbf{U} \tr \mathbf{y}_d)_i]^2/(v_i^2 + a_d)^2, \\
		\bm{\mu}_d \tr \X \tr \X \bm{\mu}_d & = \sum_{i=1}^n v_i^4 [(\mathbf{U} \tr \mathbf{y}_d)_i]^2/(v_i^2 + a_d)^2, \\
		\log|\bm{\Sigma}| & \propto p \log \zeta_d - \sum_{i=1} \log (v_i^2 + a_d) - \max(p - n, 0) \cdot \log a_d, \\
		\mathbf{y}_d \tr \X \bm{\mu}_d & = \sum_{i=1}^n v_i^2 [(\mathbf{U} \tr \mathbf{y}_d)_i]^2/(v_i^2 + a_d).
		\end{align*}
		where $v_i$ are the singular values of $\X$. The SVD and $\mathbf{U} \tr \mathbf{y}_d$ are $\mathcal{O}(pn^2)$ and $\mathcal{O}(n^2)$ once at the start of the algorithm. The rest of the calculations involve just $n$ scalar multiplications and/or additions. 
		
		\subsection{Simulations}
		We simulate from a correctly specified model. We assume random $y_{id}$, $\x_i$, and $\bm{\beta}_d$, while we fix $\mathbf{C}$, $\gamma_d^2$, $\sigma_d^2$ and $\bm{\alpha}$. We estimate the $\bm{\mu}_d$, $\bm{\Sigma}_d$, $\delta_d$, $\bm{\alpha}$ and $\lambda$, while we fix the $\sigma_d^2$ to their true values.
		
		We set $n=100$, $p=200$, and $D=10$. We consider 5 evenly sized classes of drugs, such that $\mathbf{c}_{kd} = \mathbbm{1}(k = \text{class}_d)$ and $\begin{bmatrix} \mathbf{c}_1 & \cdots & \mathbf{c}_D \end{bmatrix} \tr$. From these the $\gamma_d^2 = (\mathbf{c}_d \tr \bm{\alpha})^{-1}$ are created. Next, we simulate $\bm{\beta}_{jd} \sim \mathcal{N} (0, \sigma^2_d \gamma_d^2)$. We fix the mean signal-to noise ratio $\overline{\text{SNR}}_d$, such that the predictor data variance is $s^2 = \V(\mathbf{x}_i) = \overline{\text{SNR}}_d / (\overline{\gamma^2_d} p)$. The last step consists of simulating the data by $\mathbf{x}_{ij} \sim \mathcal{N}(0,s^2)$ and $y_{id} = \mathcal{N} (\mathbf{x}_i \tr \bm{\beta}_d, \sigma_d^2)$. The remaining parameters are set as follows: $\bm{\alpha} = \begin{bmatrix} 1 & \dots & 5 \end{bmatrix} \tr$, $\sigma_d^2 = 1$ for all $d$, and $\overline{\text{SNR}}_d = 5$.
		
		We compare the results to a model where the $\gamma_d^2$ are drawn from a class-specific  inverse Gamma distribution; a generalisation of the models in \cite{kpogbezan_empirical_2017} and \cite{leday_gene_2017}:
		\begin{align*}
		y_{id} | \bm{\beta}_d ,\sigma_d^2 & \sim \mathcal{N} (\x_i \tr \bm{\beta}_d, \sigma_d^2), \\
		\beta_{jd} | \gamma_d^2 ,\sigma_d^2 & \sim \mathcal{N} (0, \sigma_d^2 \gamma_d^2), \\
		\gamma_d^2 & \sim \Gamma^{-1} (a_{\text{class}(d)}, b_{\text{class}(d)}), \\
		\sigma_d^2 & \sim \Gamma^{-1} (0.001, 0.001),
		\end{align*}
		where $\text{class}(c)$ denotes the class of drug $d$. This allows for more flexible empirical Bayes estimation of the $a_d$ and $b_d$, but, as a consequence, increases the risk of overfitting. Additionally, it does not allow to include continuous covariates on the drugs.
		
		To account for the random data and parameter generation, we repeat the simulation 100 times and present the results in Table...
		
		\subsection{Starting values}
%		Starting value for $\sigma_d^2$ as in \cite{chipman_bart:_2010}. They consider a scaled-Chi squared distribution for the error variance with degrees of freedom 3 and scale chosen such that the $q*100$th percentile matches the error variance in the data $s^2(\mathbf{y}_d)$. As starting value for $\sigma_d^2$ we take the mode of this distribution: $\hat{\sigma}_d^2 = 3a/5$, with $a$ the solution to:
%		$$
%		\Gamma\(\frac{3}{3}\) \[1 - q - F(3a/(2s^2(\mathbf{y}_d)); 3/2, 1) \] = 0,
%		$$
%		where $F(x; 3/2, 1)$ is the CDF of a gamma function with shape $3/2$ and scale $1$.  In \cite{moran_variance_2018}, REFERENCE to Ročková and George (2018), and \cite{chipman_bart:_2010}, $q=0.9$ is suggested. We find that $q \in ( 0.9, 0.95 )$ gives good results.

		We generate starting values as follows. Consider the $\sigma_d^2$ and $\gamma_d^2$ as fixed and estimate them by MML of the regular ridge model:
		$$
		\hat{\sigma}_d^2, \hat{\gamma}_d^2= \underset{\sigma_d^2,\gamma_d^2}{\argmax} \int_{\bm{\beta}} p(\mathbf{y}_d | \bm{\beta}, \sigma_d^2) p(\bm{\beta} | \gamma_d^2, \sigma_d^2) d\bm{\beta}.
		$$
		The regular ridge model is conjugate, so MML is relatively simple. Next we estimate a common inverse Gaussian prior for the $\gamma_d^2$ by considering the estimates to be samples from the prior:
		\begin{align*}
		\hat{\theta} & = n^{-1} \sum_{d=1}^D \hat{\gamma}_d^2 / n, \\
		\hat{\lambda} & = n \left\{\sum_{d=1}^D \[ (\hat{\gamma}_d^2)^{-1} - \hat{\theta}^{-1}\]\right\}^{-1}.
		\end{align*}
		We estimate the mode of the $\hat{m}=\hat{\gamma}_d^2$ by kernel density estimation and equate it to the theoretical mode of the generalized inverse Gaussian distribution with the estimated $\hat{\theta}$ and $\hat{\lambda}$. We solve to find a common $\delta$:
		$$
		\hat{\delta} = \frac{\hat{m}^2 \hat{\lambda}}{\hat{\theta}^2} + (p + 3) \hat{m}.
		$$
		Lastly, we consider the $\hat{\sigma}^2_d$ fixed samples from the inverse Gamma distribution to estimate a common scale:
		$$
		\hat{\zeta} = \frac{D(n + p + 1)}{2 \sum_{d=1}^D[(\hat{\sigma}_d^2)^{-1}]}.
		$$
		
		\subsection{Independent error variance model parameters}
		Model:
		\begin{align*}
		y_{id} | \bm{\beta}_d, \sigma_d^2 & \sim \mathcal{N} (\x_i \tr \bm{\beta}_d, \sigma_d^2), \\
		\beta_{jd} | \gamma_d^2 & \sim \mathcal{N} (0, \gamma_d^2), \\
		\gamma_d^2 & \sim \mathcal{IG}((\mathbf{c}_d \tr \bm{\alpha})^{-1}, \lambda), \\
		\sigma_d^2 & \sim 1/\sigma_d^3.
		\end{align*}
		Variational posterior:
		\begin{align*}
		q(\bm{\beta}) & \overset{D}{=} \prod_{d=1}^D \mathcal{N}_p (\bm{\mu}_d, \bm{\Sigma}_d), \\
		q(\bm{\gamma}^2) & \overset{D}{=} \prod_{d=1}^D \mathcal{GIG}\(-\frac{p+1}{2}, \lambda (\mathbf{c}_d \tr \bm{\alpha})^2, \delta_d\), \\
		q(\bm{\sigma}^2) & \overset{D}{=} \prod_{d=1}^D \Gamma^{-1} \(\frac{n + 1}{2}, \zeta_d\).
		\end{align*}
		Parameter updates:
		\begin{align*}
		\bm{\Sigma}_d^{(h+1)} & = \(\frac{n+1}{2 \zeta_d^{(h)}}\X \tr \X + a_d^{(h)} \I_p\)^{-1}, \\
		\bm{\mu}_d^{(h+1)} & = \frac{n+1}{2 \zeta_d^{(h)}} \bm{\Sigma}_d^{(h+1)} \X \tr \y_d, \\
		\delta_d^{(h+1)} & = \trace(\bm{\Sigma}_d^{(h+1)}) + (\bm{\mu}_d^{(h+1)}) \tr \bm{\mu}_d^{(h+1)} + \lambda, \\ \zeta_d^{(h+1)} & = \frac{1}{2} \[\mathbf{y}_d \tr \mathbf{y}_d -2 \mathbf{y}_d \tr \X \bm{\mu}_d^{(h+1)} + \trace (\X \tr \X \bm{\Sigma}_d^{(h+1)}) + (\bm{\mu}_d^{(h+1)}) \tr \X \tr \X \bm{\mu}_d^{(h+1)}\],
		\end{align*}
		with $a_d^{(h)}$ and the empirical Bayes updates $\bm{\alpha}^{(l+1)}$ and $\lambda^{(l+1)}$ the same as before.
		\section{Ratios of modified Bessel functions}
		Ratios of modified Bessel functions of the second kind $K_{\alpha - 1}(x)/K_{\alpha}(x)$ are prone to under- and overflow for large $\alpha$. In our case, $\alpha$ increases linearly with $p$. Since $p$ may be large, this causes numerical issues in the calculation of various quantities. We alleviate the numerical issues through the following.
		
		We let $n_1=p/2$ and $n_2=(p-1)/2$ and use the well-known recursive relation:
		$$
		K_{\alpha}(x) = K_{\alpha - 2}(x) + \frac{2(\alpha - 1)}{2} K_{\alpha- 1}(x),
		$$
		to rewrite the ratio:
		\begin{align*}
		\frac{K_{\frac{p - 1}{2}}(x)}{K_{\frac{p + 1}{2}}(x)} = 
		\begin{cases}
		\Big( \dots \Big( \( 1 + \frac{2 \cdot 1 - 1}{x} \)^{-1} + \frac{2 \cdot 2-1}{x}\Big)^{-1} + \dots + \frac{2 \cdot n_1 - 1}{x}\Big)^{-1}, & \text{for } p \text{ even}, \\
		\Big( \dots \Big( \( \frac{K_0(x)}{K_1(x)} + \frac{2}{x} \cdot 1 \)^{-1} + \frac{2}{x} \cdot 2 \Big)^{-1} + \dots + \frac{2}{x} \cdot n_2 \Big)^{-1}, & \text{for } p \text{ odd}.
		\end{cases}
		\end{align*} 
		The ratio $K_0(x)/K_1(x)$ is well-behaved, such that the ratio may be computed without numerical issues.
		
		\section{General model}
		We have drug covariates $\mathbf{c}_d$ and omics feature covariates $\mathbf{z}_j$. The model is now:
		\begin{align*}
		y_{id} | \bm{\beta}_d, \sigma_d^2 & \sim \mathcal{N} (\x_i \tr \bm{\beta}_d, \sigma_d^2), \\
		\beta_{jd} | \tau_j^2, \gamma_d^2, \sigma_d^2 & \sim \mathcal{N} (0, \sigma_d^2 \gamma_d^2 \tau_j^2), \\
		\gamma_d^2 & \sim \mathcal{IG}\((\mathbf{c}_d \tr \bm{\alpha})^{-1}, \lambda\), \\
		\tau_j^2 & \sim \mathcal{IG}\((\mathbf{z}_j \tr \bm{\theta})^{-1}, \nu\), \\
		\sigma_d^2 & \sim 1/\sigma_d^3.
		\end{align*}
		Variational posterior:
		\begin{align*}
		q(\bm{\beta}) & \overset{D}{=} \prod_{d=1}^D \mathcal{N}_p (\bm{\mu}_d, \bm{\Sigma}_d), \\
		q(\bm{\gamma}^2) & \overset{D}{=} \prod_{d=1}^D \mathcal{GIG}\(-\frac{p+1}{2}, \lambda (\mathbf{c}_d \tr \bm{\alpha})^2, \delta_d\), \\
		q(\bm{\tau}^2) & \overset{D}{=} \prod_{j=1}^p \mathcal{GIG}\(-\frac{D+1}{2}, \nu (\mathbf{z}_j \tr \bm{\theta})^2, \eta_j\), \\
		q(\bm{\sigma}^2) & \overset{D}{=} \prod_{d=1}^D \Gamma^{-1} \(\frac{n + p + 1}{2}, \zeta_d\).
		\end{align*}
		Variational parameters:
		\begin{align*}
		\bm{\Sigma}_d^{(h+1)} & = \frac{2 \zeta_d^{(h)}}{n+p+1} \[\X \tr \X + a_d^{(h)} \cdot \diag(b_j^{(h)})\]^{-1}, \\
		\bm{\mu}_d^{(h+1)} & = \[\X \tr \X + a_d^{(h)} \cdot \diag(b_j^{(h)})\]^{-1} \X \tr \y_d, \\
		\delta_d^{(h+1)} & = \frac{n+p+1}{2 \zeta_d^{(h)}} \left\{\trace \[\diag(b_j^{(h)}) \bm{\Sigma}_d^{(h+1)} \] + (\bm{\mu}_d^{(h+1)}) \tr \diag(b_j^{(h)}) \bm{\mu}_d^{(h+1)}\right\} + \lambda, \\ 
		\eta_j^{(h+1)} & = \sum_{d=1}^D \frac{a_d^{(h)}(n+p+1)}{2 \zeta_d^{(h)}} \[ (\bmu_{jd}^{(h + 1)})^2 + (\bSigma_d^{(h + 1)})_{jj} \] + \nu, \\ 
		\zeta_d^{(h+1)} & = \frac{1}{2} \Big( \mathbf{y}_d \tr \mathbf{y}_d -2 \mathbf{y}_d \tr \X \bm{\mu}_d^{(h+1)} + a_d^{(h+1)} \trace \left\{ \[\X \tr \X + \diag(b_j^{(h + 1)})\] \bm{\Sigma}_d^{(h+1)}\right\} \\
		& \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + a_d^{(h+1)} (\bm{\mu}_d^{(h+1)}) \tr \[\X \tr \X + \diag(b_j^{(h + 1)})\]\bm{\mu}_d^{(h+1)}\Big),
		\end{align*}
		with
		\begin{align*}
		a_d^{(h)} & = \sqrt{\frac{\lambda (\mathbf{c}_d \tr \bm{\alpha})^2}{\delta_d^{(h)}}} \frac{K_{\frac{p-1}{2}} \( \sqrt{\lambda (\mathbf{c}_d \tr \bm{\alpha})^2 \delta_d^{(h)}} \)}{K_{\frac{p+1}{2}} \( \sqrt{\lambda (\mathbf{c}_d \tr \bm{\alpha})^2 \delta_d^{(h)}} \)} + \frac{p+1}{\delta_d^{(h)}} \text{ and} \\
		b_j^{(h)} & = \sqrt{\frac{\nu (\mathbf{z}_j \tr \bm{\theta})^2}{\eta_j^{(h)}}} \frac{K_{\frac{D-1}{2}} \( \sqrt{\nu (\mathbf{z}_j \tr \bm{\theta})^2 \eta_j^{(h)}} \)}{K_{\frac{D+1}{2}} \( \sqrt{\nu (\mathbf{z}_j \tr \bm{\theta})^2 \eta_j^{(h)}} \)} + \frac{D+1}{\eta_j^{(h)}}.
		\end{align*}
		Hyper-parameter updates:
		\begin{align*}
		\bm{\alpha}^{(l+1)} & = \left[ \mathbf{C} \tr \diag ( e_d^{(l)} ) \mathbf{C} \right]^{-1} \mathbf{C} \tr \mathbf{1}, \\
		\lambda^{(l+1)} & = \[ \sum_{d=1}^D a_d^{(l)} - (\bm{\alpha}^{(l+1)}) \tr \mathbf{C} \tr \mathbf{1} \]^{-1} D, \\
		\bm{\theta}^{(l+1)} & = \left[ \mathbf{Z} \tr \diag ( f_j^{(l)} ) \mathbf{Z} \right]^{-1} \mathbf{Z} \tr \mathbf{1}, \\
		\nu^{(l+1)} & = \[ \sum_{j=1}^p b_j^{(l)} - (\bm{\theta}^{(l+1)}) \tr \mathbf{Z} \tr \mathbf{1} \]^{-1} p,
		\end{align*}
		where 
		\begin{align*}
		e_d^{(l)} & = \sqrt{\frac{\delta_d^{(l)}}{\lambda^{(l)}(\mathbf{c}_d \tr \bm{\alpha}^{(l)})^2}} \frac{K_{\frac{p - 1}{2}} \( \sqrt{\lambda^{(l)} (\mathbf{c}_d \tr \bm{\alpha}^{(l)})^2 \delta_d^{(l)}} \)}{K_{\frac{p + 1}{2}} \( \sqrt{\lambda^{(l)} (\mathbf{c}_d \tr \bm{\alpha}^{(l)})^2 \delta_d^{(l)}} \)} \text{ and} \\
		f_j^{(l)} & = \sqrt{\frac{\eta_j^{(l)}}{\nu^{(l)}(\mathbf{z}_j \tr \bm{\theta}^{(l)})^2}} \frac{K_{\frac{D - 1}{2}} \( \sqrt{\nu^{(l)} (\mathbf{z}_j \tr \bm{\theta}^{(l)})^2 \eta_j^{(l)}} \)}{K_{\frac{D + 1}{2}} \( \sqrt{\nu^{(l)} (\mathbf{z}_j \tr \bm{\theta}^{(l)})^2 \eta_j^{(l)}} \)}.
		\end{align*}
	
	\end{appendix}

\end{document}

%		\section{Efficient computation}
%		Let $\X = \U \D \Vm \tr$ and $\Z=\U^* \D^* (\Vm^*) \tr$ be the singular value decomposition of $\X$ and $\Z$, respectively. Filling in all the expectations, we have the following iterative estimating equations:
%		\begin{align*}
%		\bSigma_d^{(l+1)} & = a_3 b_{3d}^{(l)} \Vm (\D^2 + a_1 b_{1d}^{(l)} \I_n)^{-1} \Vm \tr, \\
%		\Sm_d^{(l+1)} & = a_3 b_{3d}^{(l)} \Vm^* [(\D^*)^2 + a_2 b_{2d}^{(l)} \I_T]^{-1} (\Vm^*) \tr, \\
%		\bmu_d^{(l+1)} & = a_3 b_{3d}^{(l)} \Vm (\D + a_1 b_{1d}^{(l)} \D^{-1})^{-1} \U \tr (\y_d - \Z \mathbf{m}_d^{(l)}), \\
%		\mathbf{m}_d^{(l+1)} & = a_3 b_{3d}^{(l)} \Vm^* [\D^* + a_2 b_{2d}^{(l)} (\D^*)^{-1}]^{-1} (\U^*) \tr (\y_d - \X \bmu_d^{(l + 1)}), \\
%		b_{1d}^{(l+1)} & = \Bigg\{\frac{a_3 b_{3d}^{(l)}}{2} \Bigg[ (\y_d - \Z \mathbf{m}_d^{(l)}) \tr \U (\D + a_1 b_{1d}^{(l)} \D^{-1})^{-2} \U \tr (\y_d - \Z \mathbf{m}_d^{(l)}) \\
%		\,\,\,\,\,\,\,\,\,\, & + a_3 b_{3d}^{(l)} \sum_{i=1}^n (d_i^2 + a_1 b_{1d}^{(l)})^{-1} \Bigg] + \theta_1^{-1} \Bigg\}^{-1}, \\
%		b_{2d}^{(l+1)} & = \Bigg(\frac{a_3 b_{3d}^{(l)}}{2} \Bigg\{ (\y_d - \X \bmu_d^{(l+1)}) \tr \U^* [\D^* + a_2 b_{2d}^{(l)} (\D^*)^{-1}]^{-2} (\U^*) \tr (\y_d - \X \bmu_d^{(l+1)}) \\
%		\,\,\,\,\,\,\,\,\,\, & + a_3 b_{3d}^{(l)} \sum_{i=1}^n [(d^*_i)^2 + a_2 b_{2d}^{(l)}]^{-1} \Bigg\} + \theta_2^{-1} \Bigg)^{-1}, \\
%		b_{3d}^{(l+1)} & = \frac{\y_d \tr \y_d}{2}  - \y_d \tr (\X \bmu_d^{(l+1)} + \Z \mathbf{m}_d^{(l+1)}) + (\bmu_d^{(l+1)}) \tr \X \tr \Z \mathbf{m}_d^{(l+1)} + \frac{\sum_{i=1}^n \(d_i^2 - \frac{d_i^4}{d_i^2 + a_1 b_1^{(l+1)}}\)}{2 a_1 b_{1d}^{(l + 1)}} \\
%		\,\,\,\,\,\,\,\,\,\, & + \frac{(\bmu_d^{(l)}) \tr \X \tr \X \bmu_d^{(l)}}{2} + \frac{\sum_{t=1}^T \[(d^*_t)^2 - \frac{(d^*_t)^4}{(d^*_t)^2 + a_2 b_2^{(l+1)}}\]}{2 a_2 b_{2d}^{(l + 1)}} + \frac{(\mathbf{m}_d^{(l)}) \tr \Z \tr \Z \mathbf{m}_d^{(l)}}{2} \\
%		& \,\,\,\,\,\,\,\,\,\, + \frac{a_1 b_{1d}^{(l+1)} (\y_d - \Z \mathbf{m}_d^{(l)}) \tr \U (\D + a_1 b_{1d}^{(l)} \D^{-1})^{-2} \U \tr (\y_d - \Z \mathbf{m}_d^{(l)})}{2} \\
%		& \,\,\,\,\,\,\,\,\,\, + \frac{a_1 b_{1d}^{(l+1)} a_3 b_{3d}^{(l)} \sum_{i=1}^n (d_i^2 + a_1 b_{1d}^{(l)})^{-1} }{2} \\
%		& \,\,\,\,\,\,\,\,\,\, + \frac{a_2 b_{2d}^{(l+1)} (\y_d - \X \bmu_d^{(l+1)}) \tr \U^* [\D^* + a_2 b_{2d}^{(l)} (\D^*)^{-1}]^{-2} (\U^*) \tr (\y_d - \X \bmu_d^{(l+1)})}{2} \\
%		& \,\,\,\,\,\,\,\,\,\, + \frac{a_1 b_{1d}^{(l+1)} a_3 b_{3d}^{(l)} \sum_{i=1}^n [(d^*_i)^2 + a_2 b_{2d}^{(l)}]^{-1}}{2},
%		\end{align*}
%		where $(\D^2 + a_1 b_{1d}^{(l)})^{-1}$, $[(\D^*)^2 + a_2 b_{2d}^{(l)}]^{-1}$, $(\D + a_1 b_{1d}^{(l)} \D^{-1})^{-1}$, and $[\D^* + a_2 b_{2d}^{(l)} (\D^*)^{-1}]^{-1}$ are diagonal matrices with entries $(d_t^2 + a_1 b_{1d}^{(l)})^{-1}$, $i=1,\dots,n$, $[(d_t^*)^2 + a_2 b_{2d}^{(l)}]^{-1}$, $t=1,\dots,T$, $(d_i + a_1 b_{1d}^{(l)} / d_i)^{-1}$, $i=1, \dots, n$, and $(d^*_t + a_2 b_{2d}^{(l)} / d^*_t)^{-1}$, $t=1 , \dots, T$, respectively, and hence easily calculated once the singular values are known.