% set options
<<settings, include=FALSE, echo=FALSE>>=
opts_knit$set(root.dir="..", base.dir="../figs")
opts_chunk$set(fig.align='center', fig.path="../figs/", 
               echo=FALSE, cache=FALSE, message=FALSE, fig.pos='!ht')
knit_hooks$set(document=function(x) {
  sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed=TRUE)})
@

% read figure code
<<figures, include=FALSE>>=
read_chunk("code/figures.R")
@

\documentclass[a4paper,hidelinks]{article}
\usepackage{bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,
rotating,caption,subcaption,natbib,appendix,titlesec,multicol,hyperref,verbatim,
bbm,algorithm,algpseudocode,pgfplotstable,threeparttable,booktabs,mathtools,
dsfont,parskip,xr}
\externaldocument[md-]{manuscript1}

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Em}{\mathbf{E}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\Lm}{\mathbf{L}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}

% settings
\graphicspath{{../figs/}}
\pgfplotsset{compat=1.16}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Supplementary material to: Empirical Bayes estimation of the normal
inverse Gaussian prior in 
simultaneous-equation models}
\date{\today}
\author{Magnus M. M\"unch$^{1,2}$\footnote{Correspondence to: 
\href{mailto:m.munch@vumc.nl}{m.munch@vumc.nl}}, Mark A. van de Wiel$^{1,3}$, 
Sylvia Richardson$^{3}$, \\ and Gwena{\"e}l G. R. Leday$^{3}$}

\begin{document}

	\maketitle
	
	\noindent
	1. Department of Epidemiology \& Biostatistics, Amsterdam Public Health
	research institute, Amsterdam University medical centers, PO Box 7057, 1007 MB
	Amsterdam, The Netherlands \\*
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\*
	3. MRC Biostatistics Unit, Cambridge Institute of Public Health, Cambridge, 
	United Kingdom
	
	\section{Content overview}
	This document contains the Supplementary Material (SM) to the document 
	`Empirical Bayes estimation of simultaneous-equation models'. In the 
	following, this document is referred to as Main Document (MD).
	
	\section{NIG prior}\label{sec:nigprior}
	For a fixed error variance $\sigma_d^2$, we have a marginal prior variance 
	and kurtosis:
	\begin{align*}
	  \V(\beta_{jd} | \sigma_d^2) & = \sigma_d^2 \theta_d, \\
	  \mathcal{K}(\beta_{jd} | \sigma_d^2) & = \frac{3\theta_d}{\lambda_d} + 3.
	\end{align*}
  
	\section{NIGIG prior}\label{sec:nigigprior}
	Equivalently, we may set 
  $\eta_{jd}^2=\tau_j^2 \gamma_d^2/(\theta_j^z \theta_d^c)$ and write 
  MD equation (\ref{md-eq:generalmodel}) as:
  \begin{align*}
    \beta_{jd} | \eta_{jd}^2, \sigma_d^2 & \sim \mathcal{N} 
    \(0, \frac{\lambda_{jd}}{\chi_{jd}}\eta_{jd}^2\sigma_d^2\), \\
    \eta_{jd}^2 & \overset{D}{=}\pi^{-1}\sqrt{\chi_{jd}}\exp(\psi_{jd}) 
    \eta_{jd}^{-3/2} \cdot K_{0} \(\sqrt{\psi_{jd}^2 + \chi_{jd} (\eta_{jd}^2 
    + \eta_{jd}^{-2} - 2)} \),
  \end{align*}
  where $\lambda_{jd}=\lambda_j^z\lambda_d^c$, 
  $\chi_{jd}=\lambda_j^z\lambda_d^c/(\theta_j^z \theta_d^c)$, and
  $\psi_{jd}=\lambda_j^z/\theta_j^z + \lambda_d^c/ \theta_d^c$. A third way
  of writing the model is:
  \begin{subequations}\label{eq:generalprior}
    \begin{align}
      \beta_{jd} | \eta_{jd}^2, \sigma_d^2 & \sim \mathcal{N} 
      \(0, \eta_{jd}^2 \sigma_d^2\), \\
      \eta_{jd}^2 | \xi_{d}^2 & \sim \mathcal{IG} 
      (\theta_j^z \xi_d^2, \lambda_j^z \xi_d^2 ), \\
      \xi_d^2 & \sim \mathcal{GIG} (1, \lambda_d^c/(\theta_d^c)^2, \lambda_d^c).
    \end{align}
  \end{subequations}
	From (\ref{eq:generalprior}), for a fixed error variance
	$\sigma_d^2$, we find the marginal prior variance and kurtosis:
	\begin{subequations}\label{eq:varkurtnigig}
	  \begin{align}
	    \V(\beta_{jd} | \sigma_d^2) & = \sigma_d^2 \theta_d^c \theta_j^z  
      \(r_d + \frac{2\theta_d^c}{\lambda_d^c}\), \\
	    \mathcal{K}(\beta_{jd} | \sigma_d^2) & = \(\frac{3\theta_j^z}
	    {\lambda_j^z} + 3\) \frac{4 \theta_d^c (r_d + 2\theta_d^c/\lambda_d^c) +
	    1}{\lambda_d^c(r_d + 2\theta_d^c/\lambda_d^c)^2},
	  \end{align}
	\end{subequations}
	with $r_d = K_0(\lambda_d^c/\theta_d^c)/K_1(\lambda_d^c/\theta_d^c)$.
	If 
  we fix the prior expectation $\E(\gamma_d^2)=\theta_d^c$ and let the prior 
  variance 
  $\V(\gamma_d^2)$ go to zero, we have $\V(\beta_{jd} | \sigma_d^2) \rightarrow
  \sigma_d^2 \theta_j^z \theta_d^c$ and $\mathcal{K}(\beta_{jd} | \sigma_d^2)
  \rightarrow 3(\theta_j^z/\lambda_j^z + 1)$. From (\ref{eq:varkurtnigig}) we see
  that we can keep the variance constant by fixing $\theta_j^z$, $\theta_d^c$,
  and $\lambda_d^c$, while we increasing the kurtosis by decrearing 
  $\lambda_j^z$. This makes the model MD equations (\ref{md-eq:generalmodel}) a
  flexible model.
	
	\section{Student's \texorpdfstring{$t$}{t} prior}\label{sec:studentstprior}
	For a fixed error variance $\sigma_d^2$, we have a marginal prior variance 
	and kurtosis:
	\begin{align*}
	  \V(\beta_{jd} | \sigma_d^2) & = \frac{\sigma_d^2 \lambda_d}{\eta_d - 2}, \\
	  \mathcal{K}(\beta_{jd} | \sigma_d^2) & = \frac{6}{\eta_d - 4} + 3.
	\end{align*}
	
	\section{Variational Bayes derivations}\label{sec:vbderivations}
	For clarity we have indexed the variational 
	density functions with their respective parameters, which we omitted in the 
	MD. In the following all expectations are with respect to the 
	variational posterior $Q_d$. The variational posterior for $\bbeta_d$ is 
	found as follows:
	\begin{align*}
	  \log q_{\bbeta_d} (\bbeta_d) & \propto \E [\log \mathcal{L}
	  (\y_d | \bbeta_d, \sigma_d^2 )] + \E [\log \pi (\bbeta_d | \gamma_d^2, 
	  \sigma_d^2)] \\
	  & \propto - \frac{1}{2} \sum_{i=1}^n 
	  \E \[ \frac{(y_{id} - \x_i \tr \bbeta_d)^2}{\sigma_d^2} \] - 
	  \frac{1}{2} \sum_{j=1}^p 
	  \E \( \frac{\beta_{jd}^2}{\gamma_d^2 \sigma_d^2} \), \\
	  q_{\bbeta_d} (\bbeta_d) & \overset{D}{=} 
	  \mathcal{N}_p (\bmu_d, \bSigma_d), \\
	  & \text{with } \bSigma_d = \E(\sigma_d^{-2})^{-1} 
	  [\X \tr \X + \E(\gamma_d^{-2}) \cdot \I ]^{-1}, \\
	  & \text{and } \bmu_d = [\X \tr \X + \E(\gamma_d^{-2}) \cdot \I ]^{-1} 
	  \X \tr \y_d.
	\end{align*}
	The variational posterior for $\gamma_d^2$ is given by:
	\begin{align*}
	  \log q_{\gamma_d^{2}}(\gamma_d^{2}) & \propto \E 
	  [\log \pi (\bbeta_d | \gamma_d^{2}, \sigma_d^2)] + \log \pi(\gamma_d^{2}) \\
	  & \propto -\frac{p}{2} \log \gamma_d^{2} - \frac{1}{2} \sum_{j=1}^p 
	  \E \( \frac{\beta_{jd}^2}{\sigma^{2}_d}\) \gamma_d^{-2} - 
	  \frac{3}{2} \log \gamma_d^{2} - 
	  \frac{\lambda_d (\gamma_d^2 - \theta_d)^2}{2 \theta_d^2} \gamma_d^{-2} \\
	  & \propto \( -\frac{p + 1}{2} - 1\) \log \gamma_d^{2} - 
	  \frac{\lambda_d}{2\theta_d^2} \gamma_d^2 - 
	  \frac{1}{2}\[ \lambda_d + \E(\sigma_d^{-2}) \sum_{j=1}^p \E(\beta_{jd}^2)\] 
	  \gamma_d^{-2} , \\
	  q_{\gamma_d^{2}}(\gamma_d^{2}) & \overset{D}{=} \mathcal{GIG} 
	  \(-\frac{p+1}{2}, \frac{\lambda_d}{\theta_d^2}, \delta_{d} \), \\
	  & \text{with } \delta_{d} =\E(\sigma_d^{-2})
	  \left\{\E(\bbeta_{d} \tr) \E(\bbeta_{d}) + \trace[\V(\bbeta_{d})]\right\} + 
	  \lambda_d.
	\end{align*}
	Lastly, we have the variational posterior for $\sigma_d^2$:
	\begin{align*}
	  \log q_{\sigma_d^{2}}(\sigma_d^{2}) & \propto \E 
	  [\log \mathcal{L}(\y_d | \bbeta)d, \sigma_d^2 )] + 
	  \E [\log \pi (\bbeta_d | \gamma_d^2, \sigma_d^2)] + 
	  \log \pi (\sigma_d^{2}) \\
	  & \propto -\frac{n}{2} \log \sigma_d^{2} - \frac{1}{2} \sum_{i=1}^n 
	  \frac{\E [(y_{id} - \x_i \tr \bbeta_d)^2]}{\sigma_d^{2}} - 
	  \frac{p}{2} \log \sigma_d^{2} - \frac{1}{2} \sum_{j=1}^p 
	  \E \( \frac{\beta_{jd}^2)}{\gamma_d^{-2}}\) \sigma_d^{-2} - 
	  \frac{3}{2} \log \sigma^2_d \\
	  & = \(-\frac{n + p + 1}{2} - 1 \) \log \sigma_d^{2} - 
	  \frac{1}{2}\left\{ \sum_{i=1}^n \E [(y_{id} - \x_i \tr \bbeta_d)^2] + 
	  \E(\gamma_d^{-2})\sum_{j=1}^p \E(\beta_{jd}^2) \right\} \sigma_d^{-2}, \\
	  q_{\sigma_d^{2}}(\sigma_d^{2}) & \overset{D}{=} 
	  \Gamma^{-1} \(\frac{n + p + 1}{2}, \zeta_{d} \), \\
	  & \text{with } \zeta_{d} = \frac{1}{2} \bigg\{\mathbf{y}_d \tr \mathbf{y}_d -
    2 \mathbf{y}_d \tr \X \E(\bbeta_d) + 
    \trace [ \X \tr \X \V(\bbeta_d)] + 
    \E(\bbeta_d \tr)  \X \tr \X \E(\bbeta_d ) \\
    & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + 
    \E(\gamma_d^{-2}) \trace [ \V(\bbeta_d)] + 
    \E(\gamma_d^{-2}) \E(\bbeta_d \tr) \E(\bbeta_d ) \bigg\}.
	\end{align*}
	
	\section{Evidence lower bound}\label{sec:evidencelowerbound}
	The variational Bayes posterior is found through maximisation of an evidence
	lower bound (ELBO). Additionally, we may monitor convergence through this 
	ELBO. To compute the ELBO after updating the EB parameters from 
	$\theta_d^{(l)}, \lambda_d^{(l)}$ to 
	$\theta_d^{(l + 1)}, \lambda_d^{(l + 1)}$, we compute 
	$\text{ELBO}^{(l + 1)} = \sum_{d=1}^D \text{ELBO}_d^{(l + 1)}$, with:
	\begin{align*}
	  \text{ELBO}_d^{(l + 1)} = & \frac{1}{2} \log |\bm{\Sigma}_d^{(l)}| - 
	  \frac{a_d^{(l)}}{2} \[ \y_d \tr \y_d - 2\y_d \tr \X \bm{\mu}_d^{(l)} + 
	  (\bm{\mu}_d^{(l)}) \tr \X \tr \X \bm{\mu}_d^{(l)} + 
	  \trace ( \X \tr \X \bm{\Sigma}_d^{(l)})\] \\
	  & + \frac{b_d^{(l)}}{2} \[ \delta_d^{(l)} - 
	  a_d^{(l)}(\bm{\mu}_d^{(l)}) \tr \bm{\mu}_d^{(l)} - 
	  a_d^{(l)} \trace (\bm{\Sigma}_d^{(l)}) \] \\
	  & - \frac{p + n + 1}{2} \log \zeta_d^{(l)} + 
	  \frac{p + 1}{4} \log \lambda_d^{(l)} - 
	  \frac{p + 1}{2} \log \theta_d^{(l)} - \frac{p+1}{4} \log \delta_d^{(l)} \\
	  & + \log K_{\frac{p+1}{2}}\(\sqrt{\lambda_d^{(l)} 
	  \delta_d^{(l)}}/\theta_d^{(l)}\) + 
	  \frac{\lambda_d^{(l)} e_d^{(l)}}{2(\theta_d^{(l)})^2} \\
	  & + \frac{\lambda_d^{(l+1)}}{\theta_d^{(l+1)}} + 
	  \frac{1}{2}\log \lambda_d^{(l+1)} - \frac{\lambda_d^{(l+1)} 
	  e_d^{(l)}}{2 (\theta_d^{(l + 1)})^2} - 
	  \frac{\lambda_d^{(l + 1)} b_d^{(l)}}{2},
	\end{align*}
	where $a^{(l)}$, $b^{(l)}$, and $e^{(l)}$ are as in the MD. To compute the 
	ELBO after a VB update we simply let 
  $\theta_d^{(l)}, \lambda_d^{(l)}=\theta_d^{(l + 1)}, \lambda_d^{(l + 1)}$.
	 
  In practice, there are different options to assess convergence. We may monitor
  the parameters themselves, the ELBO, or a combination of the two. We may also 
  iterate for an \textit{a priori} fixed number of times. We may also choose to 
  use a different convergence criterium for the EB and VB iterations. Currently 
  we fix the number of EB iterations to 20, while we do 2 VB iterations per EB 
  iteration.
  
  \section{Efficient computation}
  The empirical Bayes updates require the following quantities: $\zeta_d$, $a_d$, $\delta_d$, $\trace (\bm{\Sigma}_d)$, $\trace (\X \tr \X \bm{\Sigma}_d)$, $\bm{\mu}_d \tr \bm{\mu}_d$, and $\bm{\mu}_d \tr \X \tr \X \bm{\mu}_d$. In addition, we need $\log|\bm{\Sigma}|$ and $\mathbf{y}_d \tr \X \bm{\mu}_d$ to monitor the ELBO. The first three quantities are obtained by scalar operations of $\mathcal{O}(n)$. Let $c_d^{(h)}=b_d^{(h)}$ in the conjugate setting and $c_d^{(h)}=b_d^{(h)}/a_d^{(h)}$ in the non-conjugate setting. Then, beforementioned quantities are easily calculated using the SVD $\X = \mathbf{U} \mathbf{D} \mathbf{V} \tr$:
  \begin{align*}
  \trace (\bm{\Sigma}_d^{(h+1)}) & = (a_d^{(h)})^{-1} \[\sum_{i=1}^n (v_i^2 + c_d^{(h)})^{-1} + \max (p - n, 0)\cdot (c_d^{(h)})^{-1}\], \\
  \trace (\X \tr \X \bm{\Sigma}_d^{(h+1)}) & = (a_d^{(h)})^{-1} \sum_{i=1}^n v_i^2(v_i^2 + c_d^{(h)})^{-1}, \\
  (\bm{\mu}_d^{(h+1)}) \tr \bm{\mu}_d^{(h+1)} & = \sum_{i=1}^n v_i^2 [(\mathbf{U} \tr \mathbf{y}_d)_i]^2/(v_i^2 + c_d^{(h)})^2, \\
  (\bm{\mu}_d^{(h+1)}) \tr \X \tr \X \bm{\mu}_d^{(h+1)} & = \sum_{i=1}^n v_i^4 [(\mathbf{U} \tr \mathbf{y}_d)_i]^2/(v_i^2 + c_d^{(h)})^2, \\
  \log|\bm{\Sigma}^{(h+1)}| & \propto -p \log a_d^{(h)} - \sum_{i=1} \log (v_i^2 + c_d^{(h)}) - \max(p - n, 0) \cdot \log c_d^{(h)}, \\
  \mathbf{y}_d \tr \X \bm{\mu}_d^{(h+1)} & = \sum_{i=1}^n v_i^2 [(\mathbf{U} \tr \mathbf{y}_d)_i]^2/(v_i^2 + c_d^{(h)}).
  \end{align*}
  where $v_i$ are the singular values of $\X$. The SVD and $\mathbf{U} \tr \mathbf{y}_d$ are $\mathcal{O}(pn^2)$ and $\mathcal{O}(n^2)$ operations, respectively, but have to be calculated only once at the start of the algorithm. The rest of the calculations involve just $n$ scalar multiplications and/or additions. 
  
  \section{Ratios of modified Bessel functions}\label{sec:ratiosmodifiedbessels}
  Ratios of modified Bessel functions of the second kind 
  $K_{\alpha - 1}(x)/K_{\alpha}(x)$ are prone to under- and overflow for large
  $\alpha$. In our case, $\alpha$ increases linearly with $p$. Since $p$ may be
  large, this causes numerical issues in the calculation of various quantities.
  We alleviate the numerical issues through the following.
  
  We let $n_1=p/2$ and $n_2=(p-1)/2$ and use the well-known recursive relation:
  $$
  K_{\alpha}(x) = K_{\alpha - 2}(x) + \frac{2(\alpha - 1)}{2} K_{\alpha- 1}(x),
  $$
  to rewrite the ratio:
  \begin{align*}
    \frac{K_{\frac{p - 1}{2}}(x)}{K_{\frac{p + 1}{2}}(x)} = 
    \begin{cases}
      \Big( \dots \Big( \( 1 + \frac{2 \cdot 1 - 1}{x} \)^{-1} + 
      \frac{2 \cdot 2-1}{x}\Big)^{-1} + \dots + 
      \frac{2 \cdot n_1 - 1}{x}\Big)^{-1}, & \text{for } p \text{ even}, \\
      \Big( \dots \Big( \( \frac{K_0(x)}{K_1(x)} + \frac{2}{x} \cdot 1 \)^{-1} +
      \frac{2}{x} \cdot 2 \Big)^{-1} + \dots + \frac{2}{x} \cdot n_2 \Big)^{-1},
      & \text{for } p \text{ odd}.
    \end{cases}
  \end{align*} 
  The ratio $K_0(x)/K_1(x)$ is well-behaved, such that the ratio may be computed
  without numerical issues.
  
  \section{Gibbs sampler}\label{sec:gibbssampler}
  MCMC samples from the posterior corresponding to model 
  (\ref{md-eq:priormodel}) may be generated for each equation independently. 
  MCMC samples may be generated by iteratively sampling the following 
  conditional distributions:
  \begin{subequations}
    \begin{align}
      \bm{\beta}_d | \gamma_d^2, \sigma_d^2, \y_d & \sim \mathcal{N}_p 
      (\bm{\mu}_d, \bm{\Sigma}_d), \label{eq:betasample} \\
      \text{with } & \bm{\Sigma}_d = \sigma_d^2 (\X \tr \X + 
      \gamma_d^{-2} \I)^{-1} \label{eq:sigmasample} \\
      \text{and } & \bm{\mu}_d = (\X \tr \X + 
      \gamma_d^{-2} \I)^{-1} \X \tr \y_d \label{eq:musample} \\
      \gamma_d^2 | \bm{\beta}_d, \sigma_d^2, \y & \sim \mathcal{GIG}
      \(-\frac{p+1}{2}, \lambda_d \theta_d^{-2}, \delta_d\), \nonumber \\
      \text{with } & \delta_d = \sigma_d^{-2} \bm{\beta}_d \tr \bm{\beta}_d + 
      \lambda_d, \nonumber \\
      \sigma_d^2 | \bm{\beta}_d, \gamma_d^2, \y & \sim \Gamma^{-1} 
      \(\frac{n + p + 1}{2}, \zeta_d\), \nonumber \\
      \text{with } & \zeta = \frac{1}{2} \[\mathbf{y} \tr \mathbf{y} -
      2 \mathbf{y} \tr \X \bm{\mu} + \trace ( \X \tr \X \bm{\Sigma}) + 
      (\bm{\mu}) \tr \X \tr \X \bm{\mu}  + 
      \gamma^{-2} \trace ( \bm{\Sigma}) + 
      \gamma^{-2} (\bm{\mu}) \tr \bm{\mu}\]. \nonumber
    \end{align}
  \end{subequations}
  In high dimensional space, the $p \times p$ matrix inversions in 
  (\ref{eq:sigmasample}) and (\ref{eq:musample}) are   
  significant computational bottlenecks. \cite{bhattacharya_fast_2016} describe 
  a method to sample from (\ref{eq:betasample}) without explicit calculation
  of this inverse, thereby offering a siginifcant speed-up compared to naive
  sampling.
  
  \section{Inverse Gamma model}\label{sec:inversegamma}
  The model introduced in MD Section \ref{md-sec:inversegammamodel} draws the 
  $\gamma_d^2$ from an inverse Gamma distribution:
	\begin{equation}\label{eq:invgammaprior}
	  \gamma_d^2 \sim \Gamma^{-1}(\eta_d/2, \lambda_d/2)=
	  \mathcal{GIG}(-\eta_d/2, 0, \lambda_d),
	\end{equation}
	with shape $\eta_d/2$ and scale $\lambda/2$. For the sake of comparability, we
	have rescaled the inverse Gamma parameters and reparametrised as a generalized
	inverse Gaussian. This allows for an easier comparison to the inverse 
	Gaussian hyperprior:
	\begin{equation}\label{eq:invgaussianprior}
	  \mathcal{IG}(\theta_d, \lambda_d) =
	  \mathcal{GIG}(-1/2, \lambda_d \theta_d^{-2}, \lambda_d).
  \end{equation}
	From (\ref{eq:invgammaprior}) and (\ref{eq:invgaussianprior}) it is obvious 
	that the two models coincide for $\theta_d \to \infty$ and $\eta_d = 1$. In
	Figure \ref{fig:dens_igaussian_igamma} the two distributions are compared for 
	a range of parameter values.
<<dens_igaussian_igamma, fig.cap="Several densities of the inverse Gaussian and inverse Gamma families", out.width="100%", fig.asp=2/3>>=
digauss <- function(x, theta, lambda, eta) {
  sqrt(lambda/(x^3*2*pi))*exp(-lambda*(x - theta)^2/(2*theta^2*x))
}

digamma <- function(x, theta, lambda, eta) {
  alpha <- eta/2
  beta <- lambda/2
  beta^alpha/gamma(alpha)*x^(-alpha - 1)*exp(-beta/x)
}

eta <- c(1, 2, 1, 1, 5)
lambda <- c(1, 1, 2, 1/2, 2)
theta <- c(1000, 1000, 1, 1, 1)

labels <- c("inv. Gaussian", "inv. Gamma")
col <- c(1, 2)

opar <- par(no.readonly=TRUE)
par(mar=opar$mar*c(1, 1.3, 1, 1))
layout(matrix(c(rep(c(1, 1, 2, 2, 3, 3), 2), rep(c(0, 4, 4, 5, 5, 0), 2)),
                 nrow=4, ncol=6, byrow=TRUE))
for(i in 1:length(eta)) {
  modes <- c(theta[i]*(sqrt(1 + 9*theta[i]^2/
                            (4*lambda[i]^2)) - 3*theta[i]/(2*lambda[i])),
             lambda[i]/(eta[i] + 2))
  ylim <- c(0, max(digauss(modes[1], theta[i], lambda[i], eta[i]),
                   digamma(modes[2], theta[i], lambda[i], eta[i])))
  curve(digauss(x, theta[i], lambda[i], eta[i]), 0.001, 5, n=1000,
        ylim=ylim, ylab="Density", "x", col=col[1], 
        main=bquote(eta==.(eta[i])*","~lambda==.(lambda[i])*","~theta==.(
          theta[i])))
  curve(digamma(x, theta[i], lambda[i], eta[i]), add=TRUE, 
        col=col[2], n=1000)
}
legend("topright", legend=labels, lty=1, col=col, seg.len=1)
par(opar)
@
	
	The variational posterior is given by:
	\begin{align*}
	  q(\bm{\beta}_d) & \overset{D}{=} \mathcal{N}_p 
	  (\bm{\mu}_d, \bm{\Sigma}_d), \\
	  q(\gamma_d^2) & \overset{D}{=} \mathcal{GIG}\(-\frac{p+\eta_d}{2}, 
	  \lambda_d \theta_d^{-2}, \delta_d\), \\
	  q(\sigma_d^2) & \overset{D}{=} \Gamma^{-1} \(\frac{n + p + 1}{2}, \zeta_d\).
  \end{align*}
	The variational parameters are the same as in the NIG model, except that now:
  $b_d^{(h)} = (p + \eta_d)/\delta_d^{(h)}$.
	
	The inclusion of continuous co-data is not so straightforward in the inverse
	Gamma hyperprior model. We therefore focus on categorical co-data and use the 
	following empirical Bayes procedure. Suppose we have a partitioning of our 
	equations into classes and we let prior variance components $\gamma_d^2$ and
	$\gamma^2_{d'}$ be drawn from the same distribution if equations $d$ and 
	$d'$ are from the same class. Or simply put: $\gamma^2_{d}, \gamma^2_{d'} 
	\sim \Gamma^{-1}(\eta_c/2, \lambda_c/2)$ if $d, d' \in \text{class}_c$. 
	Empirical bayes estimation of the $\eta_c$ and $\lambda_c$ by maximisation of
	the (approximated) marginal likelihood amounts to iteratively solving the
	estimating equations:
  \begin{subequations}
    \begin{align}
      \psi\(\frac{\eta_c}{2}\) & = \log \lambda_c - 
      \frac{\sum_{d \in \text{class}_c} e^{(l)}_d}{|\text{class}_c|} - \log 2, 
      \label{eq:estequation1}\\
      \lambda_c & = \eta_c \cdot |\text{class}_c| \cdot 
      \(\sum_{d \in \text{class}_c} b_d^{(l)}\)^{-1}, \label{eq:estequation2}
    \end{align}
  \end{subequations}
  until convergence, where $e^{(l)}_d = \E_{Q^{(l)}}(\log \gamma_d^2) = 
  \log \delta_d^{(l)} - \psi [ (p + \eta_d^{(l)})/2] - \log 2$. 
  We propose to do so by first solving (\ref{eq:estequation2}) for $\lambda_c$, 
  plugging it into (\ref{eq:estequation1}) and bounding the result using
  \cite{alzer_inequalities_1997} to obtain an interval $( \eta_c^*, 2\eta_c^* )$
  that contains the solution to (\ref{eq:estequation1}), where:
  $$
  \eta_c^* = \left\{ \frac{\sum_{d \in \text{class}_c} 
  e^{(l)}_d}{|\text{class}_c|} + \log \[ \frac{\sum_{d \in \text{class}_c} 
  b_d^{(l)}}{|\text{class}_c|} \]  \right\}^{-1}.
  $$
  With this interval it is straightforward to find $\eta^{(l+1)}_c$ by any 
  root-finding algorithm and plugging the solution into (\ref{eq:estequation2})
  to find $\lambda^{(l+1)}_c$. We may use $\eta_c^*$ as a starting value.
	
	In the inverse Gamma setting, $\text{ELBO}^{(l + 1)} = 
	\sum_{d=1}^D \text{ELBO}_d^{(l + 1)}$ after a EB update is given by:
	\begin{align*}
	  \text{ELBO}_d^{(l + 1)}  = & \frac{1}{2} \log |\bm{\Sigma}_d^{(l)}| -
	  \frac{a_d^{(l)}}{2} \[ \y_d \tr \y_d - 2\y_d \tr \X \bm{\mu}_d^{(l)} +
	  (\bm{\mu}_d^{(l)}) \tr \X \tr \X \bm{\mu}_d^{(l)} + \trace ( \X \tr \X 
	  \bm{\Sigma}_d^{(l)})\] \\
	  & + \frac{b_d^{(l)}}{2} \[ \delta_d^{(l)} - 
	  a_d^{(l)}(\bm{\mu}_d^{(l)}) \tr \bm{\mu}_d^{(l)} - 
	  a_d^{(l)} \trace (\bm{\Sigma}_d^{(l)}) \] - 
	  \frac{n + p + 1}{2} \log \zeta_d^{(l)} \\
	  & - \frac{p + \eta_d^{(l)}}{2} \log \delta_d^{(l)} + 
	  \frac{\eta_d^{(l)}}{2} e_d^{(l)} + \frac{\eta_d^{(l)}}{2} \log 2 + 
	  \log \Gamma \( \frac{p + \eta_d^{(l)}}{2} \) \\
	  & - \log \Gamma \( \frac{\eta_d^{(l + 1)}}{2} \) - 
	  \frac{\eta_d^{(l+1)}}{2} e_d^{(l)} - \frac{\eta_d^{(l+1)}}{2} \log 2 +
	  \frac{\eta_d^{(l + 1)}}{2} \log \lambda_d^{(l+1)} - 
	  \frac{\lambda_d^{(l + 1)} b_d^{(l)}}{2}.
	\end{align*}
  To compute the ELBO after a VB update we simply let 
  $\eta_d^{(l)}, \lambda_d^{(l)}=\eta_d^{(l + 1)}, \lambda_d^{(l + 1)}$.
	
	\section{Non-conjugate model}
	The non-conjugate model assumes $\beta_{jd}$ and $\sigma_d^2$ 
	independent \textit{a priori}:
  $$
  \beta_{jd} | \gamma_d^2, \sigma_d^2 \sim \beta_{jd} | 
  \gamma_d^2 \sim \mathcal{N} (0, \gamma_d^2).
  $$
  
	The variational distributions are as follows:
	\begin{align*}
	  q(\bm{\beta}_d) & \overset{D}{=} \mathcal{N}_p 
	  (\bm{\mu}_d, \bm{\Sigma}_d), \\
	  q(\gamma_d^2) & \overset{D}{=} \mathcal{GIG}
	  \(-\frac{p+1}{2}, \lambda_d \theta_d^{-2}, \delta_d\), \\
	  q(\sigma_d^2) & \overset{D}{=} \Gamma^{-1} 
	  \(\frac{n + 1}{2}, \zeta_d\),
  \end{align*}
	The variational parameters are iteratively updated by:
  \begin{align*}
  \bm{\Sigma}_d^{(h+1)} & = (a_d^{(h)})^{-1} (\X \tr \X + c_d^{(h)} \I)^{-1}, \\
  \bm{\mu}_d^{(h+1)} & = (\X \tr \X + c_d^{(h)} \I)^{-1} \X \tr \y_d, \\
  \delta_d^{(h+1)} & = b_d^{(h)} (c_d^{(h)})^{-1} \bmu_d \tr \bmu_d + \lambda_d, \\ 
  \zeta_d^{(h+1)} & = \frac{1}{2} \bigg[\mathbf{y}_d \tr \mathbf{y}_d -2 \mathbf{y}_d \tr \X \bm{\mu}_d^{(h+1)} + \trace ( \X \tr \X \bm{\Sigma}_d^{(h+1)}) + (\bm{\mu}_d^{(h+1)}) \tr \X \tr \X \bm{\mu}_d^{(h+1)} \\
  & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, + \frac{b_d^{(h+1)} - c_d^{(h+1)} a_d^{(h+1)}}{1 - a_d^{(h+1)}} \trace ( \bm{\Sigma}_d^{(h+1)}) + \frac{b_d^{(h+1)} - c_d^{(h+1)} a_d^{(h+1)}}{1 - a_d^{(h+1)}} (\bm{\mu}_d^{(h+1)}) \tr \bm{\mu}_d^{(h+1)}\bigg].
  \end{align*}
  Here, we set $a_d^{(h)}$, $b_d^{(h)}$, and $c_d^{(h)}$ as in 
  (\ref{eq:auxvar1})
  Note that in the inverse Gamma model, the computation of (\ref{eq:auxvar1})
  simplifies to $b_d^{(h)}=(p+\eta_d)/\delta_d^{(h)}$.
  
  Here, we set
  \begin{align}
  a_d^{(h)} & =\E_{Q^{(h)}}(\sigma_d^{-2})=(df+1)/(2 \zeta_d^{(h)}), \nonumber \\
  b_d^{(h)} & = \E_{Q^{(h)}}(\gamma_d^{-2}) = \sqrt{\frac{\lambda_d}{\theta_d^2 \delta_d^{(h)}}} \frac{K_{\frac{p + \eta_d -2}{2}} \( \sqrt{\lambda_d \delta_d^{(h)}}/\theta_d \)}{K_{\frac{p+\eta_d}{2}} \( \sqrt{\lambda_d \delta_d^{(h)}}/\theta_d \)} + \frac{p+\eta_d^{(h)}}{\delta_d^{(h)}}, \label{eq:auxvar1} \\
  c_d^{(h)} & = \begin{cases*}
  b_d^{(h)} & in the conjugate setting \\
  b_d^{(h)}/a_d^{(h)} & in the non-conjugate setting .
  \end{cases*} \nonumber
  \end{align}
  Note that in the inverse Gamma model, the computation of (\ref{eq:auxvar1}) simplifies to $b_d^{(h)}=(p+\eta_d)/\delta_d^{(h)}$.
  
  \section{Starting values}
  \subsection{Method 1}
  Starting value for $\sigma_d^2$ as in \cite{chipman_bart:_2010}. They consider a scaled-Chi squared distribution for the error variance with degrees of freedom 3 and scale chosen such that the $q*100$th percentile matches the error variance in the data $s^2(\mathbf{y}_d)$. As starting value for $\sigma_d^2$ we take the mode of this distribution: $\hat{\sigma}_d^2 = 3a/5$, with $a$ the solution to:
  $$
  \Gamma\(\frac{3}{3}\) \[1 - q - F(3a/(2s^2(\mathbf{y}_d)); 3/2, 1) \] = 0,
  $$
  where $F(x; 3/2, 1)$ is the CDF of a gamma function with shape $3/2$ and scale $1$. In \cite{moran_variance_2018}, REFERENCE to Rockova and George (2018), and \cite{chipman_bart:_2010}, $q=0.9$ is suggested. We find that $q \in ( 0.9, 0.95 )$ gives good results.
  
  \subsection{Method 2}
  We generate starting values as follows. Consider the $\sigma_d^2$ and $\gamma_d^2$ as fixed and estimate them by MML of the regular ridge model:
  $$
  \hat{\sigma}_d^2, \hat{\gamma}_d^2= \underset{\sigma_d^2,\gamma_d^2}{\argmax} \int_{\bm{\beta}} p(\mathbf{y}_d | \bm{\beta}, \sigma_d^2) p(\bm{\beta} | \gamma_d^2, \sigma_d^2) d\bm{\beta}.
  $$
  The regular ridge model is conjugate, so MML is relatively simple. Next we estimate a common inverse Gaussian prior for the $\gamma_d^2$ by considering the estimates to be samples from the prior:
  \begin{align*}
  \hat{\theta} & = n^{-1} \sum_{d=1}^D \hat{\gamma}_d^2 / n, \\
  \hat{\lambda} & = n \left\{\sum_{d=1}^D \[ (\hat{\gamma}_d^2)^{-1} - \hat{\theta}^{-1}\]\right\}^{-1}.
  \end{align*}
  We estimate the mode of the $\hat{m}=\hat{\gamma}_d^2$ by kernel density estimation and equate it to the theoretical mode of the generalized inverse Gaussian distribution with the estimated $\hat{\theta}$ and $\hat{\lambda}$. We solve to find a common $\delta$:
  $$
  \hat{\delta} = \frac{\hat{m}^2 \hat{\lambda}}{\hat{\theta}^2} + (p + 3) \hat{m}.
  $$
  Lastly, we consider the $\hat{\sigma}^2_d$ fixed samples from the inverse Gamma distribution to estimate a common scale:
  $$
  \hat{\zeta} = \frac{D(n + p + 1)}{2 \sum_{d=1}^D[(\hat{\sigma}_d^2)^{-1}]}.
  $$
	
  \bibliographystyle{author_short3}  
	\bibliography{refs}

\end{document}
